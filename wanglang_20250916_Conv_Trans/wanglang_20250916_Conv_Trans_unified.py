#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
wanglang_20250916_Conv_Trans_unified.py - ç»Ÿä¸€æ¨¡å‹å®ç°
ç»“åˆå·ç§¯ç¥ç»ç½‘ç»œå’ŒTransformerçš„æ··åˆæ¶æ„ï¼Œæ”¯æŒOptunaè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
import shutil
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class wanglang_20250916_Conv_TransConfig:
    """wanglang_20250916_Conv_Transé…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "wanglang_20250916_Conv_Trans"
    model_type: str = "classification"
    architecture: str = "conv_transformer"  # æ·»åŠ æ¶æ„å‚æ•°
    
    # åŠ¨æ€è¾“å…¥ç»´åº¦é…ç½®
    input_dim: Optional[int] = None  # å°†ä»æ•°æ®ä¸­è‡ªåŠ¨æ£€æµ‹
    seq_len: int = 100
    output_dim: int = 2
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 100
    weight_decay: float = 1e-4
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True
    dataloader_num_workers: int = 4
    pin_memory: bool = True
    gradient_clip_value: float = 1.0
    
    # å·ç§¯åˆ†æ”¯é…ç½®
    num_branches: int = 4
    kernel_sizes: List[List[int]] = None
    conv_out_channels: int = 32
    conv_activation: str = "relu"
    conv_batch_norm: bool = True
    conv_pooling: str = "adaptive_avg"
    
    # Transformeré…ç½®
    d_model: int = 128
    nhead: int = 8
    num_layers: int = 6
    dim_feedforward: int = 512
    transformer_dropout: float = 0.1
    transformer_activation: str = "relu"
    
    # åˆ†ç±»å™¨é…ç½®
    hidden_dim: int = 256
    classifier_activation: str = "tanh"
    classifier_dropout: float = 0.3
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    seed: int = 42
    
    # è·¯å¾„é…ç½®
    data_path: str = "./data"
    checkpoint_dir: str = "./checkpoints"
    results_dir: str = "./results"
    logs_dir: str = "./logs"
    
    def __post_init__(self):
        if self.kernel_sizes is None:
            self.kernel_sizes = [[1, 20], [3, 20], [5, 20], [7, 20]]

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    logger = logging.getLogger(__name__)

    if device_choice == "cpu":
        device = torch.device('cpu')
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
        logger.info("è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨CPU")
    elif device_choice == "cuda":
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            logger.info(f"è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨GPU - {gpu_name} ({gpu_memory:.1f}GB)")
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            cached_memory = torch.cuda.memory_reserved() / 1024**3

            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU: {gpu_name}")
            print(f"   ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f}GB æ€»é‡")
            print(f"   ğŸ“Š GPUæ•°é‡: {gpu_count}")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨: {current_memory:.2f}GB")
            print(f"   ğŸ“Š ç¼“å­˜ä½¿ç”¨: {cached_memory:.2f}GB")

            logger.info(f"è®¾å¤‡é…ç½®: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU")
            logger.info(f"GPUä¿¡æ¯: {gpu_name}, {gpu_memory:.1f}GBæ€»å†…å­˜, {gpu_count}ä¸ªGPU")
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {current_memory:.2f}GBå·²ç”¨, {cached_memory:.2f}GBç¼“å­˜")

            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            logger.info("è®¾å¤‡é…ç½®: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    if log_filename:
        log_file = log_dir / log_filename
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{prefix}_{timestamp}.log"
    
    # æ¸…é™¤ä¹‹å‰çš„handlersï¼Œé¿å…é‡å¤æ—¥å¿—
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger

def create_directories(config: wanglang_20250916_Conv_TransConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    dirs = [config.checkpoint_dir, config.results_dir, config.logs_dir]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

# =============================================================================
# æ•°æ®å¤„ç†ç±»
# =============================================================================

class wanglang_20250916_Conv_TransDataLoaderManager:
    """æ•°æ®åŠ è½½å™¨ç®¡ç†ç±»"""
    
    def __init__(self, data_config_path: str, config: wanglang_20250916_Conv_TransConfig):
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.data_split_info = self.data_config.get('data_split', {})
        self.scaler = StandardScaler()
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            if 'phase_1' in config:
                phase_1_config = config['phase_1']
                config['data_split'] = {
                    "workflow_type": "time_series",
                    "train": {
                        "start_date": phase_1_config['train_data']['start_date'],
                        "end_date": phase_1_config['train_data']['end_date'],
                        "samples": phase_1_config['train_data']['samples'],
                        "duration_years": phase_1_config['train_data']['duration_years'],
                        "file": phase_1_config['train_data']['file']
                    },
                    "validation": {
                        "start_date": phase_1_config['valid_data']['start_date'],
                        "end_date": phase_1_config['valid_data']['end_date'],
                        "samples": phase_1_config['valid_data']['samples'],
                        "duration_years": phase_1_config['valid_data']['duration_years'],
                        "file": phase_1_config['valid_data']['file']
                    },
                    "test": {
                        "start_date": phase_1_config['test_data']['start_date'],
                        "end_date": phase_1_config['test_data']['end_date'],
                        "samples": phase_1_config['test_data']['samples'],
                        "duration_years": phase_1_config['test_data']['duration_years'],
                        "file": phase_1_config['test_data']['file']
                    }
                }
                print(f"âœ… ä½¿ç”¨Phase 1é…ç½®ä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²")
            
            return config
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path}: {e}")
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_split_info
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=4,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )

    def _load_real_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ä»çœŸå®parquetæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            # ä»data_splité…ç½®ä¸­è·å–æ–‡ä»¶è·¯å¾„
            train_file = self.data_split_info.get('train', {}).get('file')
            valid_file = self.data_split_info.get('validation', {}).get('file')
            test_file = self.data_split_info.get('test', {}).get('file')
            
            if not all([train_file, valid_file, test_file]):
                raise ValueError("æ•°æ®é…ç½®ä¸­ç¼ºå°‘æ–‡ä»¶è·¯å¾„ä¿¡æ¯")
            
            # åŠ è½½æ•°æ®æ–‡ä»¶
            train_df = pd.read_parquet(train_file)
            valid_df = pd.read_parquet(valid_file)
            test_df = pd.read_parquet(test_file)
            
            print(f"ğŸ“Š åŠ è½½çœŸå®æ•°æ®:")
            print(f"  è®­ç»ƒé›†: {train_df.shape} - {train_file}")
            print(f"  éªŒè¯é›†: {valid_df.shape} - {valid_file}")
            print(f"  æµ‹è¯•é›†: {test_df.shape} - {test_file}")
            
            return train_df, valid_df, test_df
            
        except Exception as e:
            print(f"âŒ åŠ è½½çœŸå®æ•°æ®å¤±è´¥: {e}")
            raise

    def get_input_dim_from_dataframe(self, df: pd.DataFrame) -> int:
        """ä»DataFrameè·å–ç‰¹å¾ç»´åº¦"""
        # æå–ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
        feature_cols = [col for col in df.columns if '@' in col]
        if not feature_cols:
            # å¦‚æœæ²¡æœ‰@ç¬¦å·çš„åˆ—ï¼Œä½¿ç”¨é™¤äº†æœ€åä¸€åˆ—çš„æ‰€æœ‰æ•°å€¼åˆ—
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            if 'class' in numeric_cols:
                numeric_cols.remove('class')
            feature_cols = numeric_cols[:-1] if len(numeric_cols) > 1 else numeric_cols
        
        input_dim = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim}")
        return input_dim

    def load_data_loaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """åŠ è½½æ•°æ®åŠ è½½å™¨"""
        # åŠ è½½çœŸå®æ•°æ®
        train_df, valid_df, test_df = self._load_real_dataframes()
        
        # åŠ¨æ€æ£€æµ‹ç‰¹å¾ç»´åº¦
        input_dim = self.get_input_dim_from_dataframe(train_df)
        self.config.input_dim = input_dim
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        feature_cols = [col for col in train_df.columns if '@' in col]
        if not feature_cols:
            numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
            if 'class' in numeric_cols:
                numeric_cols.remove('class')
            feature_cols = numeric_cols[:-1] if len(numeric_cols) > 1 else numeric_cols
        
        target_col = 'class' if 'class' in train_df.columns else train_df.columns[-1]
        
        # å¤„ç†è®­ç»ƒé›†
        X_train = train_df[feature_cols].values.astype(np.float32)
        y_train = train_df[target_col].values.astype(np.int64)
        X_train = np.nan_to_num(X_train, nan=0.0)
        
        # å¤„ç†éªŒè¯é›†
        X_valid = valid_df[feature_cols].values.astype(np.float32)
        y_valid = valid_df[target_col].values.astype(np.int64)
        X_valid = np.nan_to_num(X_valid, nan=0.0)
        
        # å¤„ç†æµ‹è¯•é›†
        X_test = test_df[feature_cols].values.astype(np.float32)
        y_test = test_df[target_col].values.astype(np.int64)
        X_test = np.nan_to_num(X_test, nan=0.0)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train = self.scaler.fit_transform(X_train)
        X_valid = self.scaler.transform(X_valid)
        X_test = self.scaler.transform(X_test)
        
        # è½¬æ¢ä¸ºå¼ é‡
        X_train = torch.FloatTensor(X_train)
        X_valid = torch.FloatTensor(X_valid)
        X_test = torch.FloatTensor(X_test)
        y_train = torch.LongTensor(y_train)
        y_valid = torch.LongTensor(y_valid)
        y_test = torch.LongTensor(y_test)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(X_train, y_train)
        valid_dataset = TensorDataset(X_valid, y_valid)
        test_dataset = TensorDataset(X_test, y_test)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self._create_dataloader(train_dataset, shuffle=True, batch_size=batch_size)
        valid_loader = self._create_dataloader(valid_dataset, shuffle=False, batch_size=batch_size)
        test_loader = self._create_dataloader(test_dataset, shuffle=False, batch_size=batch_size)
        
        print(f"ğŸ“ˆ æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(valid_dataset)} æ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_loader, valid_loader, test_loader
    
    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç›¸å…³ä¿¡æ¯"""
        return {
            "workflow_type": self.data_split_info.get("workflow_type", "unknown"),
            "train_period": self.data_split_info.get("train", {}),
            "validation_period": self.data_split_info.get("validation", {}),
            "test_period": self.data_split_info.get("test", {}),
            "data_config": self.data_config
        }

# =============================================================================
# æ¨¡å‹æ¶æ„ç»„ä»¶
# =============================================================================

class ConvBranch(nn.Module):
    """å·ç§¯åˆ†æ”¯æ¨¡å—"""
    
    def __init__(self, 
                 input_dim: int,
                 kernel_size: List[int],
                 out_channels: int,
                 activation: str = "relu",
                 batch_norm: bool = True,
                 pooling: str = "adaptive_avg"):
        super().__init__()
        
        self.conv = nn.Conv2d(
            in_channels=1,
            out_channels=out_channels,
            kernel_size=tuple(kernel_size),
            padding=(kernel_size[0]//2, kernel_size[1]//2)
        )
        
        self.batch_norm = nn.BatchNorm2d(out_channels) if batch_norm else None
        
        if activation.lower() == "relu":
            self.activation = nn.ReLU()
        elif activation.lower() == "tanh":
            self.activation = nn.Tanh()
        elif activation.lower() == "gelu":
            self.activation = nn.GELU()
        else:
            self.activation = nn.ReLU()
        
        if pooling == "adaptive_avg":
            self.pooling = nn.AdaptiveAvgPool2d((1, 1))
        elif pooling == "adaptive_max":
            self.pooling = nn.AdaptiveMaxPool2d((1, 1))
        else:
            self.pooling = nn.AdaptiveAvgPool2d((1, 1))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv(x)
        
        if self.batch_norm is not None:
            x = self.batch_norm(x)
        
        x = self.activation(x)
        x = self.pooling(x)
        
        # å±•å¹³
        x = x.view(x.size(0), -1)
        
        return x

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—"""
    
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        
        self.register_buffer('pe', pe)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return x + self.pe[:x.size(0), :]

# =============================================================================
# æ¨¡å‹åŸºç±»å’Œå…·ä½“å®ç°
# =============================================================================

class BaseModel(nn.Module, ABC):
    """æ¨¡å‹åŸºç±»"""
    
    def __init__(self, config: wanglang_20250916_Conv_TransConfig):
        super().__init__()
        self.config = config
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024),
            "input_dim": getattr(self.config, 'input_dim', None)
        }

class wanglang_20250916_Conv_TransModel(BaseModel):
    """wanglang_20250916_Conv_Transæ··åˆæ¨¡å‹æ¶æ„"""
    
    def __init__(self, config: wanglang_20250916_Conv_TransConfig):
        super().__init__(config)
        
        # è¾“å…¥ç»´åº¦å¤„ç†
        self.input_dim = getattr(config, 'input_dim', None)
        if self.input_dim is None:
            print("âš ï¸ é…ç½®ä¸­æœªæŒ‡å®šinput_dimï¼Œå°†åœ¨æ•°æ®åŠ è½½æ—¶è‡ªåŠ¨æ£€æµ‹")
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç½‘ç»œå±‚ï¼Œç­‰å¾…input_dimç¡®å®š
        self.layers_initialized = False
        self.conv_branches = None
        self.input_projection = None
        self.pos_encoding = None
        self.transformer_encoder = None
        self.global_pool = None
        self.classifier = None
        
        # ä¿å­˜é…ç½®ç”¨äºå»¶è¿Ÿåˆå§‹åŒ–
        self.seq_len = config.seq_len
        self.output_dim = config.output_dim
        self.num_branches = config.num_branches
        self.kernel_sizes = config.kernel_sizes
        self.conv_out_channels = config.conv_out_channels
        self.conv_activation = config.conv_activation
        self.conv_batch_norm = config.conv_batch_norm
        self.conv_pooling = config.conv_pooling
        self.d_model = config.d_model
        self.nhead = config.nhead
        self.num_layers = config.num_layers
        self.dim_feedforward = config.dim_feedforward
        self.transformer_dropout = config.transformer_dropout
        self.transformer_activation = config.transformer_activation
        self.hidden_dim = config.hidden_dim
        self.classifier_activation = config.classifier_activation
        self.classifier_dropout = config.classifier_dropout
        
        # å¦‚æœå·²çŸ¥input_dimï¼Œç«‹å³åˆå§‹åŒ–
        if self.input_dim is not None:
            self._build_layers()
    
    def _build_layers(self):
        """æ„å»ºç½‘ç»œå±‚"""
        if self.input_dim is None:
            raise ValueError("input_dimå¿…é¡»åœ¨æ„å»ºç½‘ç»œå±‚ä¹‹å‰è®¾ç½®")
        
        print(f"ğŸ”§ æ„å»ºç½‘ç»œå±‚ï¼Œè¾“å…¥ç»´åº¦: {self.input_dim}")
        
        # åˆ›å»ºå·ç§¯åˆ†æ”¯
        self.conv_branches = nn.ModuleList([
            ConvBranch(
                input_dim=self.input_dim,
                kernel_size=self.kernel_sizes[i] if i < len(self.kernel_sizes) else [3, 20],
                out_channels=self.conv_out_channels,
                activation=self.conv_activation,
                batch_norm=self.conv_batch_norm,
                pooling=self.conv_pooling
            ) for i in range(self.num_branches)
        ])
        
        # è¾“å…¥æŠ•å½±å±‚
        self.input_projection = nn.Linear(self.input_dim, self.d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(self.d_model, max_len=self.seq_len)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model,
            nhead=self.nhead,
            dim_feedforward=self.dim_feedforward,
            dropout=self.transformer_dropout,
            activation=self.transformer_activation,
            batch_first=False
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers
        )
        
        # å…¨å±€æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # ç‰¹å¾èåˆç»´åº¦
        conv_feature_dim = self.num_branches * self.conv_out_channels
        total_feature_dim = conv_feature_dim + self.d_model
        
        # åˆ†ç±»å™¨
        if self.classifier_activation.lower() == 'tanh':
            activation_fn = nn.Tanh()
        else:
            activation_fn = nn.ReLU()
            
        self.classifier = nn.Sequential(
            nn.Linear(total_feature_dim, self.hidden_dim),
            activation_fn,
            nn.Dropout(self.classifier_dropout),
            nn.Linear(self.hidden_dim, self.output_dim)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
        self.layers_initialized = True
        print(f"âœ… ç½‘ç»œå±‚æ„å»ºå®Œæˆ")
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.Conv2d):
                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºç½‘ç»œ
        if not self.layers_initialized:
            if self.input_dim is None:
                self.input_dim = x.shape[-1]
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç»´åº¦: {self.input_dim}")
            self._build_layers()
        
        batch_size, input_dim = x.shape
        
        # ä¸ºåºåˆ—å¤„ç†æ·»åŠ åºåˆ—ç»´åº¦
        if len(x.shape) == 2:
            x = x.unsqueeze(1)  # [batch_size, 1, input_dim]
        
        # å·ç§¯åˆ†æ”¯å¤„ç†
        conv_features = []
        
        # ä¸ºå·ç§¯å‡†å¤‡è¾“å…¥: [batch_size, 1, seq_len, input_dim]
        conv_input = x.unsqueeze(1)  # [batch_size, 1, 1, input_dim]
        
        for branch in self.conv_branches:
            branch_output = branch(conv_input)
            conv_features.append(branch_output)
        
        # æ‹¼æ¥å·ç§¯ç‰¹å¾
        conv_output = torch.cat(conv_features, dim=1)  # [batch_size, total_conv_features]
        
        # Transformeråˆ†æ”¯å¤„ç†
        # è¾“å…¥æŠ•å½±
        trans_input = self.input_projection(x)  # [batch_size, seq_len, d_model]
        
        # è½¬æ¢ä¸ºTransformeræœŸæœ›çš„æ ¼å¼: [seq_len, batch_size, d_model]
        trans_input = trans_input.transpose(0, 1)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        trans_input = self.pos_encoding(trans_input)
        
        # Transformerç¼–ç 
        trans_output = self.transformer_encoder(trans_input)  # [seq_len, batch_size, d_model]
        
        # è½¬æ¢å› [batch_size, seq_len, d_model]
        trans_output = trans_output.transpose(0, 1)
        
        # å…¨å±€æ± åŒ–: [batch_size, d_model, seq_len] -> [batch_size, d_model, 1]
        trans_output = trans_output.transpose(1, 2)
        trans_output = self.global_pool(trans_output)
        trans_output = trans_output.squeeze(-1)  # [batch_size, d_model]
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([conv_output, trans_output], dim=1)
        
        # åˆ†ç±»
        output = self.classifier(combined_features)
        
        return output

# =============================================================================
# è®­ç»ƒå™¨ç±»
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config: wanglang_20250916_Conv_TransConfig, model: BaseModel, 
                 data_loader_manager: wanglang_20250916_Conv_TransDataLoaderManager, 
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir
        
        # è®¾å¤‡è®¾ç½®
        self.device = setup_device(getattr(config, 'device', 'auto'))
        self.rank = 0
        self.world_size = 1
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logging(config.logs_dir, "training")
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config.use_mixed_precision and torch.cuda.is_available() else None
        self.use_amp = config.use_mixed_precision and torch.cuda.is_available()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_score = 0.0
        self.train_history = []
        
        # Checkpointè·Ÿè¸ªå™¨
        self.checkpoint_tracker = {
            'global_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'early_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'mid_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'late_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None}
        }
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(config.seed)
        
        # åˆ›å»ºç›®å½•
        create_directories(config)
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡å¹¶è®°å½•GPUä¿¡æ¯
        self.model.to(self.device)
        self._log_device_info()
        
        # è®°å½•æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if self.use_amp:
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_info = self.data_loader_manager.get_data_info()
        self.logger.info(f"æ•°æ®åˆ†å‰²ä¿¡æ¯: {data_info['workflow_type']}")
        self.logger.info(f"è®­ç»ƒæœŸé—´: {data_info['train_period']}")
        self.logger.info(f"éªŒè¯æœŸé—´: {data_info['validation_period']}")
        self.logger.info(f"æµ‹è¯•æœŸé—´: {data_info['test_period']}")
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡å’ŒGPUä½¿ç”¨ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            
            self.logger.info(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name}")
            self.logger.info(f"ğŸ“Š GPUæ€»å†…å­˜: {gpu_memory:.1f}GB")
            self.logger.info(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {current_memory:.2f}GB")
            self.logger.info(f"ğŸ“Š å¯ç”¨å†…å­˜: {gpu_memory - current_memory:.2f}GB")
        else:
            self.logger.info("ğŸ–¥ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            
        # è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f}MB")
    
    def setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', patience=5, factor=0.5
        )
        
        self.criterion = nn.CrossEntropyLoss()
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                self.scaler.scale(loss).backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.optimizer.step()
            
            total_loss += loss.item()
            
            # è®¡ç®—é¢„æµ‹å’Œæ¦‚ç‡
            probs = torch.softmax(output, dim=1)
            preds = torch.argmax(output, dim=1)
            all_preds.extend(preds.detach().cpu().numpy())
            all_targets.extend(target.detach().cpu().numpy())
            all_probs.extend(probs.detach().cpu().numpy())
        
        metrics = {"loss": total_loss / len(train_loader)}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        
        # è®¡ç®—AUC
        try:
            if len(np.unique(all_targets)) == 2:
                all_probs_array = np.array(all_probs)
                if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
        except Exception:
            metrics["auc"] = 0.0
        
        return metrics
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                if self.use_amp:
                    with autocast():
                        output = self.model(data)
                        loss = self.criterion(output, target)
                else:
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                total_loss += loss.item()
                
                probs = torch.softmax(output, dim=1)
                preds = torch.argmax(output, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.detach().cpu().numpy())
                all_targets.extend(target.detach().cpu().numpy())
        
        metrics = {"loss": total_loss / len(val_loader)}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
        
        # è®¡ç®—AUC
        try:
            if len(np.unique(all_targets)) == 2:
                all_probs_array = np.array(all_probs)
                if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
                    # æ·»åŠ éªŒè¯é¢„æµ‹æ•°æ®ç”¨äºOptunaä¼˜åŒ–
                    metrics["y_true_val"] = all_targets
                    metrics["y_prob_val"] = all_probs_array[:, 1].tolist()
        except Exception:
            metrics["auc"] = 0.0
        
        return metrics
    
    def save_checkpoint_if_best(self, epoch, val_loss):
        """ä¿å­˜æœ€ä½³checkpoint"""
        # æ›´æ–°å…¨å±€æœ€ä½³
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        # æ›´æ–°åŒºé—´æœ€ä½³
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        # åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³ä¸ºinterval_best
        if epoch == 29:
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')
    
    def _save_checkpoint(self, epoch, val_loss, checkpoint_type):
        """ä¿å­˜checkpoint"""
        # åˆ é™¤æ—§checkpoint
        old_path = self.checkpoint_tracker[checkpoint_type]['path']
        if old_path and os.path.exists(old_path):
            os.remove(old_path)
        
        # ä¿å­˜æ–°checkpoint
        checkpoint_name = f"checkpoint_{checkpoint_type}_epoch{epoch:03d}_val{val_loss:.4f}.pt"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
            'checkpoint_type': checkpoint_type
        }, checkpoint_path)
        
        self.checkpoint_tracker[checkpoint_type] = {
            'epoch': epoch,
            'val_loss': val_loss,
            'path': checkpoint_path
        }
        self.logger.info(f"ğŸ’¾ ä¿å­˜{checkpoint_type} checkpoint: epoch {epoch}, val_loss {val_loss:.4f}")
    
    def _save_interval_best(self, checkpoint_type, epoch_range):
        """åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³checkpointä¸ºinterval_best"""
        source_path = self.checkpoint_tracker[checkpoint_type]['path']
        
        if source_path and os.path.exists(source_path):
            interval_name = f"interval_best_{epoch_range}.pt"
            interval_path = os.path.join(self.checkpoint_dir, interval_name)
            
            shutil.copy2(source_path, interval_path)
            
            epoch = self.checkpoint_tracker[checkpoint_type]['epoch']
            val_loss = self.checkpoint_tracker[checkpoint_type]['val_loss']
            
            self.logger.info(f"ğŸ“¦ å¤åˆ¶åŒºé—´æœ€ä½³ [{epoch_range}]: epoch {epoch}, val_loss {val_loss:.4f}")
            self.logger.info(f"   æºæ–‡ä»¶: {os.path.basename(source_path)}")
            self.logger.info(f"   ç›®æ ‡æ–‡ä»¶: {interval_name}")
        else:
            self.logger.warning(f"âš ï¸ åŒºé—´ [{epoch_range}] æ²¡æœ‰æ‰¾åˆ°æœ€ä½³checkpoint")
    
    def train(self, batch_size: int = None, no_save_model: bool = False) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        # è·å–æ•°æ®åŠ è½½å™¨
        batch_size = batch_size or self.config.batch_size
        train_loader, val_loader, test_loader = self.data_loader_manager.load_data_loaders(
            batch_size=batch_size)
        
        # ç¡®ä¿æ¨¡å‹å·²å®Œå…¨åˆå§‹åŒ–ï¼ˆé€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼‰
        if not self.model.layers_initialized:
            sample_batch = next(iter(train_loader))
            sample_data = sample_batch[0][:1]  # å–ä¸€ä¸ªæ ·æœ¬ï¼Œå…ˆä¸ç§»åŠ¨åˆ°GPU
            with torch.no_grad():
                _ = self.model(sample_data)  # è§¦å‘æ¨¡å‹åˆå§‹åŒ–
            # ç¡®ä¿æ¨¡å‹å‚æ•°åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.model.to(self.device)
            print(f"ğŸ”§ æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        self.setup_training_components()
        
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {self.model.get_model_info()}")
        
        start_time = time.time()
        epochs = self.config.epochs
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)

            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # è®°å½•å†å²
            epoch_info = {
                "epoch": epoch + 1,
                "train_loss": train_metrics["loss"],
                "val_loss": val_metrics["loss"],
                "lr": self.optimizer.param_groups[0]['lr'],
                "time": time.time() - epoch_start,
                "train_acc": train_metrics.get("accuracy", 0),
                "val_acc": val_metrics.get("accuracy", 0),
                "val_f1": val_metrics.get("f1", 0),
                "train_auc": train_metrics.get("auc", 0),
                "val_auc": val_metrics.get("auc", 0)
            }
            
            current_score = val_metrics.get("accuracy", 0)
            self.train_history.append(epoch_info)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(current_score)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_score > self.best_val_score:
                self.best_val_score = current_score
                if not no_save_model:
                    best_path = Path(self.config.checkpoint_dir) / "best_model.pth"
                    torch.save({
                        'epoch': epoch + 1,
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'best_val_score': self.best_val_score,
                        'config': asdict(self.config)
                    }, best_path)
            
            # ä¿å­˜checkpoint
            if not no_save_model:
                self.save_checkpoint_if_best(epoch, val_metrics["loss"])
            
            # GPUå†…å­˜ç›‘æ§
            if self.device.type == 'cuda' and (epoch + 1) % 10 == 0:
                current_memory = torch.cuda.memory_allocated() / 1024**3
                max_memory = torch.cuda.max_memory_allocated() / 1024**3
                cached_memory = torch.cuda.memory_reserved() / 1024**3
                self.logger.info(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨ (Epoch {epoch+1}): å½“å‰ {current_memory:.2f}GB, å³°å€¼ {max_memory:.2f}GB, ç¼“å­˜ {cached_memory:.2f}GB")
            
            # æ—¥å¿—è¾“å‡º
            log_msg = (
                f"Epoch {epoch+1}/{epochs} - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}, "
                f"Val Acc: {val_metrics.get('accuracy', 0):.4f}"
            )

            if 'auc' in val_metrics and val_metrics['auc'] > 0:
                log_msg += f", Val AUC: {val_metrics['auc']:.4f}"

            log_msg += f", Time: {epoch_info['time']:.2f}s"
            self.logger.info(log_msg)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        return {
            "best_val_score": self.best_val_score,
            "total_epochs": len(self.train_history),
            "total_time": total_time,
            "train_history": self.train_history
        }
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = Path(self.config.results_dir) / "training_history.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        self.logger.info(f"è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

# =============================================================================
# æ¨ç†å™¨ç±»
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, config: wanglang_20250916_Conv_TransConfig, model: BaseModel, 
                 data_loader_manager: wanglang_20250916_Conv_TransDataLoaderManager):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.device = setup_device()
        self.logger = setup_logging(config.logs_dir, "inference")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)
        self.model.eval()
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        return checkpoint
    
    def predict_batch(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """æ‰¹é‡æ¨ç†"""
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in data_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
        
        return np.array(all_preds), np.array(all_probs)
    
    def predict_single(self, data: np.ndarray) -> Tuple[Union[int, float], np.ndarray]:
        """å•æ ·æœ¬æ¨ç†"""
        data_tensor = torch.FloatTensor(data).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            output = self.model(data_tensor)
            probs = torch.softmax(output, dim=1)
            pred = torch.argmax(output, dim=1).item()
            return pred, probs.cpu().numpy()[0]
    
    def evaluate(self) -> Dict[str, float]:
        """æ¨¡å‹è¯„ä¼°"""
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        metrics["precision"] = precision_score(all_targets, all_preds, average='weighted')
        metrics["recall"] = recall_score(all_targets, all_preds, average='weighted')
        metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
        
        # AUCæŒ‡æ ‡è®¡ç®—
        if len(np.unique(all_targets)) == 2:
            all_probs_positive = np.array(all_probs)[:, 1]
            metrics["auc"] = roc_auc_score(all_targets, all_probs_positive)
        elif len(np.unique(all_targets)) > 2:
            try:
                metrics["auc"] = roc_auc_score(all_targets, all_probs, multi_class='ovr', average='weighted')
            except ValueError:
                pass
        
        return metrics
    
    def generate_predictions(self, start_date: str = None, end_date: str = None, 
                           output_path: str = None, output_format: str = "parquet") -> pd.DataFrame:
        """ç”Ÿæˆé¢„æµ‹æ•°æ®æ–‡ä»¶ç”¨äºå›æµ‹"""
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in test_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
        
        # åˆ›å»ºé¢„æµ‹DataFrame
        predictions = np.array(all_preds)
        probabilities = np.array(all_probs)
        
        # ç”Ÿæˆæ—¥æœŸåºåˆ—ï¼ˆå¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼‰
        if start_date and end_date:
            date_range = pd.date_range(start=start_date, end=end_date, periods=len(predictions))
        else:
            # ä½¿ç”¨é»˜è®¤æ—¥æœŸåºåˆ—
            date_range = pd.date_range(start='2023-01-01', periods=len(predictions), freq='D')
        
        predictions_df = pd.DataFrame({
            'date': date_range,
            'prediction': predictions,
            'confidence': probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities[:, 0],
            'probability_class_0': probabilities[:, 0],
            'probability_class_1': probabilities[:, 1] if probabilities.shape[1] > 1 else 1 - probabilities[:, 0],
            'model_name': self.config.model_name,
            'timestamp': datetime.now().isoformat()
        })
        
        # ä¿å­˜é¢„æµ‹æ–‡ä»¶
        if output_path:
            if output_format == "parquet":
                predictions_df.to_parquet(output_path, index=False)
            else:
                predictions_df.to_csv(output_path, index=False)
            self.logger.info(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")
        
        return predictions_df
    
    def save_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, 
                        output_path: str):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        results = {
            "predictions": predictions.tolist(),
            "probabilities": probabilities.tolist(),
            "config": asdict(self.config),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: wanglang_20250916_Conv_TransConfig, model: BaseModel):
        self.config = config
        self.model = model
    
    def generate_model_md(self, output_path: str = "MODEL.md"):
        """ç”ŸæˆMODEL.mdæ–‡æ¡£"""
        model_info = self.model.get_model_info()
        
        content = f"""# {self.config.model_name} Model

## æ¨¡å‹æ¦‚è¿°

{self.config.model_name} æ˜¯ä¸€ä¸ªç»“åˆå·ç§¯ç¥ç»ç½‘ç»œå’ŒTransformerçš„æ··åˆæ¶æ„æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºæ—¶é—´åºåˆ—åˆ†ç±»ä»»åŠ¡ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- **ä»»åŠ¡ç±»å‹**: {self.config.model_type}
- **æ¨¡å‹å‚æ•°**: {model_info['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°**: {model_info['trainable_parameters']:,}
- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.2f} MB
- **è¾“å…¥ç»´åº¦**: {model_info.get('input_dim', 'Dynamic')}

## æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **å·ç§¯åˆ†æ”¯**: {self.config.num_branches}ä¸ªå¹¶è¡Œå·ç§¯åˆ†æ”¯ï¼Œä½¿ç”¨ä¸åŒçš„å·ç§¯æ ¸å¤§å°
2. **Transformerç¼–ç å™¨**: {self.config.num_layers}å±‚Transformerç¼–ç å™¨
3. **ç‰¹å¾èåˆ**: å·ç§¯ç‰¹å¾å’ŒTransformerç‰¹å¾çš„èåˆ
4. **åˆ†ç±»å™¨**: å…¨è¿æ¥åˆ†ç±»å™¨

### ç½‘ç»œç»“æ„

```
è¾“å…¥ -> [å·ç§¯åˆ†æ”¯1, å·ç§¯åˆ†æ”¯2, ..., å·ç§¯åˆ†æ”¯N] -> å·ç§¯ç‰¹å¾
     -> Transformerç¼–ç å™¨ -> Transformerç‰¹å¾
     -> ç‰¹å¾èåˆ -> åˆ†ç±»å™¨ -> è¾“å‡º
```

## æŠ€æœ¯åŸç†

### å·ç§¯åˆ†æ”¯
- ä½¿ç”¨å¤šä¸ªä¸åŒå°ºå¯¸çš„å·ç§¯æ ¸æ•è·ä¸åŒæ—¶é—´å°ºåº¦çš„ç‰¹å¾
- æ¯ä¸ªåˆ†æ”¯åŒ…å«å·ç§¯å±‚ã€æ‰¹å½’ä¸€åŒ–ã€æ¿€æ´»å‡½æ•°å’Œè‡ªé€‚åº”æ± åŒ–

### Transformerç¼–ç å™¨
- ä½¿ç”¨ä½ç½®ç¼–ç å¤„ç†åºåˆ—ä¿¡æ¯
- å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»

### ç‰¹å¾èåˆ
- å°†å·ç§¯ç‰¹å¾å’ŒTransformerç‰¹å¾æ‹¼æ¥
- é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œæœ€ç»ˆåˆ†ç±»

## é…ç½®å‚æ•°

### è®­ç»ƒé…ç½®
- å­¦ä¹ ç‡: {self.config.learning_rate}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- è®­ç»ƒè½®æ•°: {self.config.epochs}
- æƒé‡è¡°å‡: {self.config.weight_decay}

### æ¨¡å‹é…ç½®
- å·ç§¯åˆ†æ”¯æ•°: {self.config.num_branches}
- Transformerç»´åº¦: {self.config.d_model}
- æ³¨æ„åŠ›å¤´æ•°: {self.config.nhead}
- Transformerå±‚æ•°: {self.config.num_layers}

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```bash
python wanglang_20250916_Conv_Trans_unified.py train --config config.yaml --data-config data.yaml
```

### æ¨¡å‹æ¨ç†

```bash
python wanglang_20250916_Conv_Trans_unified.py inference --checkpoint best_model.pth
```

### ç”Ÿæˆé¢„æµ‹æ–‡ä»¶

```bash
python wanglang_20250916_Conv_Trans_unified.py inference --inference-mode test --start-date 2023-01-01 --end-date 2023-12-31
```

## æ€§èƒ½ç‰¹æ€§

- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
- è‡ªåŠ¨GPUæ£€æµ‹å’Œä½¿ç”¨
- åŠ¨æ€è¾“å…¥ç»´åº¦é€‚é…
- å¤šcheckpointä¿å­˜ç­–ç•¥
- Optunaè¶…å‚æ•°ä¼˜åŒ–æ”¯æŒ

## æ›´æ–°æ—¥å¿—

- åˆå§‹ç‰ˆæœ¬: {datetime.now().strftime('%Y-%m-%d')}

"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¨¡å‹æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")

# =============================================================================
# ä¸»è¦æ¥å£å‡½æ•°
# =============================================================================

def apply_optuna_config(config: wanglang_20250916_Conv_TransConfig, optuna_config_path: str) -> None:
    """åº”ç”¨Optunaé…ç½®æ–‡ä»¶ä¸­çš„è¶…å‚æ•°"""
    if not optuna_config_path or not os.path.exists(optuna_config_path):
        return
    
    try:
        with open(optuna_config_path, 'r', encoding='utf-8') as f:
            optuna_config = json.load(f)
        
        # è·å–é…ç½®ç±»çš„æœ‰æ•ˆå­—æ®µ
        from dataclasses import fields
        valid_fields = {field.name for field in fields(wanglang_20250916_Conv_TransConfig)}
        
        # åº”ç”¨è¶…å‚æ•°è¦†ç›–ï¼Œåªåº”ç”¨æœ‰æ•ˆçš„å‚æ•°
        for key, value in optuna_config.items():
            if key in valid_fields and hasattr(config, key):
                setattr(config, key, value)
                print(f"ğŸ”§ Optunaè¦†ç›–å‚æ•°: {key} = {value}")
            elif key not in valid_fields:
                print(f"âš ï¸ å¿½ç•¥ä¸æ”¯æŒçš„Optunaå‚æ•°: {key}")
    except Exception as e:
        print(f"âš ï¸ åŠ è½½Optunaé…ç½®å¤±è´¥: {e}")

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               optuna_config_path: str = None, device: str = "auto",
               epochs_override: int = None, no_save_model: bool = False, seed: int = 42,
               checkpoint_dir: str = "checkpoints"):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§
    set_all_seeds(seed)
    
    # è®¾ç½®å›ºå®šçš„æ—¥å¿—è·¯å¾„
    log_dir = "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/training/logs/wanglang_20250916_Conv_Trans_3658411800"
    logger = setup_logging(log_dir=log_dir, log_filename="log.txt")
    
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    logger.info(f"ğŸ’¾ Checkpointä¿å­˜ç›®å½•: {checkpoint_dir}")
    
    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        # åˆ›å»ºé»˜è®¤é…ç½®
        config_dict = {}
    
    # å¤„ç†architectureéƒ¨åˆ†
    if config_dict and 'architecture' in config_dict:
        arch = config_dict['architecture']
        # å°†architectureä¸­çš„å‚æ•°æ˜ å°„åˆ°é…ç½®ç±»
        architecture_mapping = {
            # å·ç§¯åˆ†æ”¯é…ç½®
            'conv_branches': {
                'num_branches': 'num_branches',
                'kernel_sizes': 'kernel_sizes', 
                'out_channels': 'conv_out_channels',
                'activation': 'conv_activation',
                'batch_norm': 'conv_batch_norm',
                'pooling': 'conv_pooling'
            },
            # Transformeré…ç½®
            'transformer': {
                'd_model': 'd_model',
                'nhead': 'nhead',
                'num_layers': 'num_layers',
                'dim_feedforward': 'dim_feedforward',
                'dropout': 'transformer_dropout',
                'activation': 'transformer_activation'
            },
            # åˆ†ç±»å™¨é…ç½®
            'classifier': {
                'hidden_dim': 'hidden_dim',
                'activation': 'classifier_activation',
                'dropout': 'classifier_dropout'
            }
        }
        
        # æå–architectureå‚æ•°åˆ°é¡¶å±‚
        for section_name, section_mapping in architecture_mapping.items():
            if section_name in arch:
                section_config = arch[section_name]
                for arch_key, config_key in section_mapping.items():
                    if arch_key in section_config:
                        config_dict[config_key] = section_config[arch_key]
        
        # å¤„ç†å…¶ä»–ç›´æ¥æ˜ å°„çš„å‚æ•°
        direct_mappings = {
            'input_dim': 'input_dim',
            'output_dim': 'output_dim',
            'model_name': 'model_name',
            'model_type': 'model_type'
        }
        
        for arch_key, config_key in direct_mappings.items():
            if arch_key in arch:
                config_dict[config_key] = arch[arch_key]
        
        # ç§»é™¤architectureéƒ¨åˆ†
        del config_dict['architecture']
    
    # è¿‡æ»¤é…ç½®å­—å…¸ï¼Œåªä¿ç•™é…ç½®ç±»æ”¯æŒçš„å‚æ•°
    if config_dict:
        # è·å–é…ç½®ç±»çš„æœ‰æ•ˆå­—æ®µ
        from dataclasses import fields
        valid_fields = {field.name for field in fields(wanglang_20250916_Conv_TransConfig)}
        
        # è¿‡æ»¤æ‰ä¸æ”¯æŒçš„å‚æ•°
        filtered_config = {k: v for k, v in config_dict.items() if k in valid_fields}
        
        # è®°å½•è¢«è¿‡æ»¤æ‰çš„å‚æ•°
        filtered_out = {k: v for k, v in config_dict.items() if k not in valid_fields}
        if filtered_out:
            logger.info(f"âš ï¸ è¿‡æ»¤æ‰ä¸æ”¯æŒçš„é…ç½®å‚æ•°: {list(filtered_out.keys())}")
        
        config = wanglang_20250916_Conv_TransConfig(**filtered_config)
    else:
        config = wanglang_20250916_Conv_TransConfig()
    
    # åº”ç”¨Optunaé…ç½®
    if optuna_config_path:
        apply_optuna_config(config, optuna_config_path)
    
    # è¦†ç›–é…ç½®
    if device != "auto":
        config.device = device
    if epochs_override:
        config.epochs = epochs_override
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = wanglang_20250916_Conv_TransDataLoaderManager(data_config_path, config)
    
    # åˆ›å»ºæ¨¡å‹
    model = wanglang_20250916_Conv_TransModel(config)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UnifiedTrainer(config, model, data_loader_manager, checkpoint_dir=checkpoint_dir)
    
    # æ‰§è¡Œè®­ç»ƒ
    results = trainer.train(no_save_model=no_save_model)
    
    # è¾“å‡ºJSONæ ¼å¼çš„è®­ç»ƒç»“æœ
    final_results = {
        "final_results": {
            "best_val_score": results['best_val_score'],
            "total_epochs": results['total_epochs'],
            "total_time": results['total_time'],
            "final_train_acc": results['train_history'][-1].get('train_acc', 0),
            "final_val_acc": results['train_history'][-1].get('val_acc', 0),
            "train_auc": results['train_history'][-1].get('train_auc', 0),
            "val_auc": results['train_history'][-1].get('val_auc', 0),
            "train_losses": [epoch['train_loss'] for epoch in results['train_history']],
            "val_losses": [epoch['val_loss'] for epoch in results['train_history']],
            "train_accuracies": [epoch.get('train_acc', 0) for epoch in results['train_history']],
            "val_accuracies": [epoch.get('val_acc', 0) for epoch in results['train_history']]
        }
    }
    print(json.dumps(final_results, indent=2))

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "best_model.pth", mode: str = "eval",
                  start_date: str = None, end_date: str = None, 
                  output_path: str = None, output_format: str = "parquet"):
    """ä¸»æ¨ç†å‡½æ•°"""
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        config_dict = {}
    
    # å¤„ç†architectureéƒ¨åˆ†
    if config_dict and 'architecture' in config_dict:
        arch = config_dict['architecture']
        # å°†architectureä¸­çš„å‚æ•°æ˜ å°„åˆ°é…ç½®ç±»
        architecture_mapping = {
            # å·ç§¯åˆ†æ”¯é…ç½®
            'conv_branches': {
                'num_branches': 'num_branches',
                'kernel_sizes': 'kernel_sizes', 
                'out_channels': 'conv_out_channels',
                'activation': 'conv_activation',
                'batch_norm': 'conv_batch_norm',
                'pooling': 'conv_pooling'
            },
            # Transformeré…ç½®
            'transformer': {
                'd_model': 'd_model',
                'nhead': 'nhead',
                'num_layers': 'num_layers',
                'dim_feedforward': 'dim_feedforward',
                'dropout': 'transformer_dropout',
                'activation': 'transformer_activation'
            },
            # åˆ†ç±»å™¨é…ç½®
            'classifier': {
                'hidden_dim': 'hidden_dim',
                'activation': 'classifier_activation',
                'dropout': 'classifier_dropout'
            }
        }
        
        # æå–architectureå‚æ•°åˆ°é¡¶å±‚
        for section_name, section_mapping in architecture_mapping.items():
            if section_name in arch:
                section_config = arch[section_name]
                for arch_key, config_key in section_mapping.items():
                    if arch_key in section_config:
                        config_dict[config_key] = section_config[arch_key]
        
        # å¤„ç†å…¶ä»–ç›´æ¥æ˜ å°„çš„å‚æ•°
        direct_mappings = {
            'input_dim': 'input_dim',
            'output_dim': 'output_dim',
            'model_name': 'model_name',
            'model_type': 'model_type'
        }
        
        for arch_key, config_key in direct_mappings.items():
            if arch_key in arch:
                config_dict[config_key] = arch[arch_key]
        
        # ç§»é™¤architectureéƒ¨åˆ†
        del config_dict['architecture']
    
    # è¿‡æ»¤é…ç½®å­—å…¸ï¼Œåªä¿ç•™é…ç½®ç±»æ”¯æŒçš„å‚æ•°
    if config_dict:
        from dataclasses import fields
        valid_fields = {field.name for field in fields(wanglang_20250916_Conv_TransConfig)}
        filtered_config = {k: v for k, v in config_dict.items() if k in valid_fields}
        config = wanglang_20250916_Conv_TransConfig(**filtered_config)
    else:
        config = wanglang_20250916_Conv_TransConfig()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = wanglang_20250916_Conv_TransDataLoaderManager(data_config_path, config)
    
    # åˆ›å»ºæ¨¡å‹
    model = wanglang_20250916_Conv_TransModel(config)
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = UnifiedInferencer(config, model, data_loader_manager)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if os.path.exists(checkpoint_path):
        checkpoint = inferencer.load_checkpoint(checkpoint_path)
    
    if mode == "eval":
        # ä¼ ç»Ÿæ¨¡å‹è¯„ä¼°
        metrics = inferencer.evaluate()
        print("è¯„ä¼°ç»“æœ:")
        for key, value in metrics.items():
            print(f"  {key}: {value:.4f}")
    elif mode == "test":
        # ç”Ÿæˆé¢„æµ‹æ•°æ®æ–‡ä»¶
        predictions_df = inferencer.generate_predictions(
            start_date=start_date,
            end_date=end_date,
            output_path=output_path,
            output_format=output_format
        )
        print(f"ç”Ÿæˆé¢„æµ‹æ–‡ä»¶: {len(predictions_df)} æ¡è®°å½•")

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ä¸»æ–‡æ¡£ç”Ÿæˆå‡½æ•°"""
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        config_dict = {}
    
    # å¤„ç†architectureéƒ¨åˆ†
    if config_dict and 'architecture' in config_dict:
        arch = config_dict['architecture']
        # å°†architectureä¸­çš„å‚æ•°æ˜ å°„åˆ°é…ç½®ç±»
        architecture_mapping = {
            # å·ç§¯åˆ†æ”¯é…ç½®
            'conv_branches': {
                'num_branches': 'num_branches',
                'kernel_sizes': 'kernel_sizes', 
                'out_channels': 'conv_out_channels',
                'activation': 'conv_activation',
                'batch_norm': 'conv_batch_norm',
                'pooling': 'conv_pooling'
            },
            # Transformeré…ç½®
            'transformer': {
                'd_model': 'd_model',
                'nhead': 'nhead',
                'num_layers': 'num_layers',
                'dim_feedforward': 'dim_feedforward',
                'dropout': 'transformer_dropout',
                'activation': 'transformer_activation'
            },
            # åˆ†ç±»å™¨é…ç½®
            'classifier': {
                'hidden_dim': 'hidden_dim',
                'activation': 'classifier_activation',
                'dropout': 'classifier_dropout'
            }
        }
        
        # æå–architectureå‚æ•°åˆ°é¡¶å±‚
        for section_name, section_mapping in architecture_mapping.items():
            if section_name in arch:
                section_config = arch[section_name]
                for arch_key, config_key in section_mapping.items():
                    if arch_key in section_config:
                        config_dict[config_key] = section_config[arch_key]
        
        # å¤„ç†å…¶ä»–ç›´æ¥æ˜ å°„çš„å‚æ•°
        direct_mappings = {
            'input_dim': 'input_dim',
            'output_dim': 'output_dim',
            'model_name': 'model_name',
            'model_type': 'model_type'
        }
        
        for arch_key, config_key in direct_mappings.items():
            if arch_key in arch:
                config_dict[config_key] = arch[arch_key]
        
        # ç§»é™¤architectureéƒ¨åˆ†
        del config_dict['architecture']
    
    # è¿‡æ»¤é…ç½®å­—å…¸ï¼Œåªä¿ç•™é…ç½®ç±»æ”¯æŒçš„å‚æ•°
    if config_dict:
        from dataclasses import fields
        valid_fields = {field.name for field in fields(wanglang_20250916_Conv_TransConfig)}
        filtered_config = {k: v for k, v in config_dict.items() if k in valid_fields}
        config = wanglang_20250916_Conv_TransConfig(**filtered_config)
    else:
        config = wanglang_20250916_Conv_TransConfig()
    
    # åˆ›å»ºæ¨¡å‹
    model = wanglang_20250916_Conv_TransModel(config)
    
    # ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config, model)
    doc_generator.generate_model_md("MODEL.md")

# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="wanglang_20250916_Conv_Transç»Ÿä¸€æ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·")
    parser.add_argument("mode", choices=["train", "inference", "docs"], help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="config.yaml", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-config", default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default="best_model.pth", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--data", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # Optunaä¼˜åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--optuna-config", help="Optunaè¯•éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--no-save-model", action="store_true", help="ä¸ä¿å­˜æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°æ€§ï¼‰")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    
    # æ¨ç†ç›¸å…³å‚æ•°
    parser.add_argument("--inference-mode", choices=["test", "eval"], default="eval", 
                       help="æ¨ç†æ¨¡å¼ï¼štest(ç”Ÿæˆé¢„æµ‹æ–‡ä»¶) æˆ– eval(è¯„ä¼°)")
    parser.add_argument("--start-date", help="æ¨ç†å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end-date", help="æ¨ç†ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--format", choices=["parquet", "csv"], default="parquet", 
                       help="è¾“å‡ºæ ¼å¼")
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    
    if args.mode == "train":
        main_train(
            config_path=args.config,
            data_config_path=args.data_config,
            optuna_config_path=args.optuna_config,
            device=args.device,
            epochs_override=args.epochs,
            no_save_model=args.no_save_model,
            seed=args.seed,
            checkpoint_dir=args.checkpoint_dir
        )
    elif args.mode == "inference":
        main_inference(
            config_path=args.config,
            data_config_path=args.data_config,
            checkpoint_path=args.checkpoint,
            mode=args.inference_mode,
            start_date=args.start_date,
            end_date=args.end_date,
            output_path=args.output,
            output_format=args.format
        )
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data_config)