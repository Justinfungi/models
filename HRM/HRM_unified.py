#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HRM_unified.py - HRM (Hierarchical Reasoning Model) ç»Ÿä¸€æ¨¡å‹å®ç°
æ•´åˆè®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½çš„ç»Ÿä¸€æ¨¡æ¿ï¼Œæ”¯æŒOptunaè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import glob
import random
import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class HRMConfig:
    """HRMæ¨¡å‹é…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "HRM"
    model_type: str = "classification"  # classification, regression
    
    # æ¨¡å‹æ¶æ„é…ç½®
    input_dim: Optional[int] = None  # åŠ¨æ€æ¨æ–­
    d_model: int = 512
    d_ff: int = 2048
    n_layers: int = 8
    n_heads: int = 8
    hierarchical_levels: int = 3
    reasoning_depth: int = 4
    h_cycles: int = 3
    l_cycles: int = 2
    halt_max_steps: int = 10
    dropout: float = 0.1
    attention_dropout: float = 0.1
    activation: str = 'swiglu'
    norm_type: str = 'rms'
    norm_eps: float = 1e-6
    num_classes: int = 2
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 100
    weight_decay: float = 1e-5
    optimizer: str = 'adam'
    scheduler: str = 'cosine'
    gradient_clip_norm: float = 1.0
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True  # æ··åˆç²¾åº¦è®­ç»ƒ
    dataloader_num_workers: int = 4  # æ•°æ®åŠ è½½å¹¶å‘æ•°
    pin_memory: bool = True  # å›ºå®šå†…å­˜
    gradient_clip_value: float = 1.0  # æ¢¯åº¦è£å‰ª
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    seed: int = 42
    
    # è·¯å¾„é…ç½®
    data_path: str = "./data"
    checkpoint_dir: str = "./checkpoints"
    results_dir: str = "./results"
    logs_dir: str = "./logs"

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    import logging
    logger = logging.getLogger(__name__)

    if device_choice == "cpu":
        device = torch.device('cpu')
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
        logger.info("è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨CPU")
    elif device_choice == "cuda":
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            logger.info(f"è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨GPU - {gpu_name} ({gpu_memory:.1f}GB)")
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            cached_memory = torch.cuda.memory_reserved() / 1024**3

            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU: {gpu_name}")
            print(f"   ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f}GB æ€»é‡")
            print(f"   ğŸ“Š GPUæ•°é‡: {gpu_count}")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨: {current_memory:.2f}GB")
            print(f"   ğŸ“Š ç¼“å­˜ä½¿ç”¨: {cached_memory:.2f}GB")

            logger.info(f"è®¾å¤‡é…ç½®: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU")
            logger.info(f"GPUä¿¡æ¯: {gpu_name}, {gpu_memory:.1f}GBæ€»å†…å­˜, {gpu_count}ä¸ªGPU")
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {current_memory:.2f}GBå·²ç”¨, {cached_memory:.2f}GBç¼“å­˜")

            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            logger.info("è®¾å¤‡é…ç½®: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    if log_filename:
        # ä½¿ç”¨å›ºå®šçš„æ—¥å¿—æ–‡ä»¶å
        log_file = log_dir / log_filename
    else:
        # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶åï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{prefix}_{timestamp}.log"
    
    # æ¸…é™¤ä¹‹å‰çš„handlersï¼Œé¿å…é‡å¤æ—¥å¿—
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger

def create_directories(config: HRMConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    dirs = [config.checkpoint_dir, config.results_dir, config.logs_dir]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

# =============================================================================
# HRMæ¨¡å‹æ¶æ„ç»„ä»¶
# =============================================================================

class RMSNorm(nn.Module):
    """RMSå½’ä¸€åŒ–å±‚"""
    
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = float(eps)
        self.weight = nn.Parameter(torch.ones(dim))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        norm = x.norm(dim=-1, keepdim=True) * (x.size(-1) ** -0.5)
        return x / (norm + self.eps) * self.weight

class SwiGLU(nn.Module):
    """SwiGLUæ¿€æ´»å‡½æ•°"""
    
    def __init__(self, dim: int):
        super().__init__()
        self.w1 = nn.Linear(dim, dim)
        self.w2 = nn.Linear(dim, dim)
        self.w3 = nn.Linear(dim, dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.w2(F.silu(self.w1(x))) * self.w3(x)

class HierarchicalAttention(nn.Module):
    """å±‚çº§æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.q_proj = nn.Linear(d_model, d_model)
        self.k_proj = nn.Linear(d_model, d_model)
        self.v_proj = nn.Linear(d_model, d_model)
        self.out_proj = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = self.d_k ** -0.5
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size, seq_len, _ = x.size()
        
        # è®¡ç®—Q, K, V
        q = self.q_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        out = torch.matmul(attn_weights, v)
        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        out = self.out_proj(out)
        
        return out, attn_weights

class ReasoningBlock(nn.Module):
    """æ¨ç†æ¨¡å—"""
    
    def __init__(self, d_model: int, d_ff: int, n_heads: int, dropout: float = 0.1, 
                 activation: str = 'swiglu', norm_type: str = 'rms', norm_eps: float = 1e-6):
        super().__init__()
        self.d_model = d_model
        
        # æ³¨æ„åŠ›å±‚
        self.self_attn = HierarchicalAttention(d_model, n_heads, dropout)
        
        # å‰é¦ˆç½‘ç»œ
        if activation == 'swiglu':
            self.ffn = SwiGLU(d_model)
        else:
            self.ffn = nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.ReLU() if activation == 'relu' else nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(d_ff, d_model)
            )
        
        # å½’ä¸€åŒ–å±‚
        if norm_type == 'rms':
            self.norm1 = RMSNorm(d_model, float(norm_eps))
            self.norm2 = RMSNorm(d_model, float(norm_eps))
        else:
            self.norm1 = nn.LayerNorm(d_model, eps=float(norm_eps))
            self.norm2 = nn.LayerNorm(d_model, eps=float(norm_eps))
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        # è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        attn_out, attn_weights = self.self_attn(self.norm1(x), mask)
        x = x + self.dropout(attn_out)
        
        # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        ffn_out = self.ffn(self.norm2(x))
        x = x + self.dropout(ffn_out)
        
        return x, attn_weights

class FeatureExtractor(nn.Module):
    """ç‰¹å¾æå–å™¨"""
    
    def __init__(self, input_dim: int, d_model: int, dropout: float = 0.1):
        super().__init__()
        self.input_projection = nn.Linear(input_dim, d_model)
        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(d_model)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦
        x = self.input_projection(x)
        x = self.dropout(x)
        x = self.norm(x)
        
        # æ·»åŠ åºåˆ—ç»´åº¦
        x = x.unsqueeze(1)  # [batch_size, 1, d_model]
        
        return x

# =============================================================================
# æ•°æ®å¤„ç†ç±»
# =============================================================================

class HRMDataset(Dataset):
    """HRMæ¨¡å‹ä¸“ç”¨æ•°æ®é›†ç±»"""
    
    def __init__(self, features: np.ndarray, labels: np.ndarray, transform=None):
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
        self.transform = transform
    
    def __len__(self) -> int:
        return len(self.features)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        feature = self.features[idx]
        label = self.labels[idx]
        
        if self.transform:
            feature = self.transform(feature)
            
        return feature, label

class HRMDataLoaderManager:
    """HRMæ•°æ®åŠ è½½å™¨ç®¡ç†ç±»"""
    
    def __init__(self, data_config_path: str, config: HRMConfig):
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.data_split_info = self.data_config.get('data_split', {})
        self.scaler = StandardScaler()
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ï¼Œé»˜è®¤ä½¿ç”¨Phase 1é…ç½®"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰phase_1é…ç½®ï¼Œä½¿ç”¨å®ƒä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²ä¿¡æ¯
            if 'phase_1' in config:
                phase_1_config = config['phase_1']
                # æ„å»ºæ ‡å‡†çš„data_splitæ ¼å¼
                config['data_split'] = {
                    "workflow_type": "time_series",
                    "train": {
                        "start_date": phase_1_config['train_data']['start_date'],
                        "end_date": phase_1_config['train_data']['end_date'],
                        "samples": phase_1_config['train_data']['samples'],
                        "duration_years": phase_1_config['train_data']['duration_years'],
                        "file": phase_1_config['train_data']['file']
                    },
                    "validation": {
                        "start_date": phase_1_config['valid_data']['start_date'],
                        "end_date": phase_1_config['valid_data']['end_date'],
                        "samples": phase_1_config['valid_data']['samples'],
                        "duration_years": phase_1_config['valid_data']['duration_years'],
                        "file": phase_1_config['valid_data']['file']
                    },
                    "test": {
                        "start_date": phase_1_config['test_data']['start_date'],
                        "end_date": phase_1_config['test_data']['end_date'],
                        "samples": phase_1_config['test_data']['samples'],
                        "duration_years": phase_1_config['test_data']['duration_years'],
                        "file": phase_1_config['test_data']['file']
                    }
                }
                print(f"âœ… ä½¿ç”¨Phase 1é…ç½®ä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²")
                print(f"  è®­ç»ƒæœŸ: {config['data_split']['train']['start_date']} åˆ° {config['data_split']['train']['end_date']}")
                print(f"  éªŒè¯æœŸ: {config['data_split']['validation']['start_date']} åˆ° {config['data_split']['validation']['end_date']}")
                print(f"  æµ‹è¯•æœŸ: {config['data_split']['test']['start_date']} åˆ° {config['data_split']['test']['end_date']}")
            
            return config
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path}: {e}")
    
    def get_input_dim_from_dataframe(self, df):
        """ä»DataFrameè·å–ç‰¹å¾ç»´åº¦ï¼ˆæ’é™¤æ ‡ç­¾åˆ—ï¼‰"""
        # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
        feature_cols = [col for col in df.columns if '@' in col]
        
        if not feature_cols:
            # å¦‚æœæ²¡æœ‰@ç¬¦å·çš„åˆ—ï¼Œä½¿ç”¨é™¤äº†å·²çŸ¥éç‰¹å¾åˆ—å¤–çš„æ‰€æœ‰æ•°å€¼åˆ—
            exclude_cols = ['symbol', 'date', 'time', 'class', 'target', 'label']
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            feature_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        features = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {features}")
        return features
    
    def validate_input_dimensions(self, config, actual_input_dim):
        """éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„è¾“å…¥ç»´åº¦ä¸å®é™…æ•°æ®æ˜¯å¦ä¸€è‡´"""
        config_input_dim = getattr(config, 'input_dim', None)
        
        if config_input_dim is None:
            print(f"ğŸ“‹ é…ç½®æ–‡ä»¶ä¸­æœªæŒ‡å®šinput_dimï¼Œä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        if config_input_dim != actual_input_dim:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…ï¼é…ç½®: {config_input_dim}, å®é™…: {actual_input_dim}")
            print(f"ğŸ”§ ä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        print(f"âœ… è¾“å…¥ç»´åº¦éªŒè¯é€šè¿‡: {actual_input_dim}")
        return actual_input_dim
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        # æ ¹æ®è®¾å¤‡ç±»å‹è°ƒæ•´æ•°æ®åŠ è½½å™¨é…ç½®
        pin_memory = torch.cuda.is_available()
        num_workers = 4 if torch.cuda.is_available() else 0  # CPUæ—¶å‡å°‘workeræ•°é‡é¿å…é—®é¢˜
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=num_workers > 0,  # åªæœ‰åœ¨æœ‰workeræ—¶æ‰å¯ç”¨
            prefetch_factor=2 if num_workers > 0 else None
        )
    
    def _load_real_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ä»çœŸå®parquetæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            # è·å–æ•°æ®è·¯å¾„
            data_folder = self.data_config.get('data_paths', {}).get('data_folder', './data/feature_set')
            data_phase = self.data_config.get('data_paths', {}).get('data_phase', 1)
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®è·¯å¾„
            possible_paths = [
                data_folder,
                '../../../b_model_reproduction_agent/data_1/feature_set',
                '../../../data_1/feature_set',
                '../../../data/feature_set',
                '../../data/feature_set',
                './data/feature_set',
                '../data/feature_set',
                '/home/feng.hao.jie/deployment/model_explorer/b_model_reproduction_agent/data_1/feature_set'
            ]
            
            data_files = []
            pattern = f"mrmr_task_{data_phase}_*.pq"
            
            for path in possible_paths:
                if os.path.exists(path):
                    files = glob.glob(os.path.join(path, pattern))
                    if files:
                        data_files = files
                        data_folder = path
                        break
            
            if not data_files:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°parquetæ–‡ä»¶ï¼Œå°è¯•æŸ¥æ‰¾CSVæ–‡ä»¶
                for path in possible_paths:
                    if os.path.exists(path):
                        csv_pattern = f"*task_{data_phase}*.csv"
                        files = glob.glob(os.path.join(path, csv_pattern))
                        if files:
                            data_files = files
                            data_folder = path
                            break
            
            if not data_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œæœç´¢æ¨¡å¼: {pattern}")
            
            # åŠ è½½æ•°æ®
            print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶: {data_files[0]}")
            if data_files[0].endswith('.pq'):
                df = pd.read_parquet(data_files[0])
            else:
                df = pd.read_csv(data_files[0])
            
            # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
            feature_cols = [col for col in df.columns if '@' in col]
            
            if not feature_cols:
                # å¦‚æœæ²¡æœ‰@ç¬¦å·çš„åˆ—ï¼Œä½¿ç”¨é™¤äº†å·²çŸ¥éç‰¹å¾åˆ—å¤–çš„æ‰€æœ‰æ•°å€¼åˆ—
                exclude_cols = ['symbol', 'date', 'time', 'class', 'target', 'label']
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                feature_cols = [col for col in numeric_cols if col not in exclude_cols]
            
            if not feature_cols:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾åˆ—")
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            X = df[feature_cols].values
            y = df['class'].values
            
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
            print(f"ğŸ“Š ç‰¹å¾åˆ—æ•°: {len(feature_cols)}")
            print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
            
            # æ•°æ®æ ‡å‡†åŒ–
            X = self.scaler.fit_transform(X)
            
            # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆåŸºäºdateå­—æ®µï¼‰
            if 'date' in df.columns:
                df_sorted = df.sort_values('date')
                train_size = int(0.7 * len(df_sorted))
                val_size = int(0.15 * len(df_sorted))
                
                train_idx = df_sorted.index[:train_size]
                val_idx = df_sorted.index[train_size:train_size + val_size]
                test_idx = df_sorted.index[train_size + val_size:]
                
                X_train, y_train = X[train_idx], y[train_idx]
                X_val, y_val = X[val_idx], y[val_idx]
                X_test, y_test = X[test_idx], y[test_idx]
            else:
                # éšæœºåˆ†å‰²
                X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
                X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)
            
            # åˆ›å»ºDataFrameç”¨äºè¿”å›
            train_df = pd.DataFrame(X_train, columns=feature_cols)
            train_df['class'] = y_train
            
            val_df = pd.DataFrame(X_val, columns=feature_cols)
            val_df['class'] = y_val
            
            test_df = pd.DataFrame(X_test, columns=feature_cols)
            test_df['class'] = y_test
            
            # æ›´æ–°é…ç½®ä¸­çš„ç‰¹å¾æ•°é‡
            input_dim = self.validate_input_dimensions(self.config, X.shape[1])
            self.config.num_classes = len(np.unique(y))
            
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒé›†: {len(train_df)}, éªŒè¯é›†: {len(val_df)}, æµ‹è¯•é›†: {len(test_df)}")
            
            return train_df, val_df, test_df
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_data_loaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """åŠ è½½çœŸå®æ•°æ®çš„DataLoader"""
        # ä»çœŸå®æ•°æ®æ–‡ä»¶åŠ è½½
        train_df, val_df, test_df = self._load_real_dataframes()
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        feature_cols = [col for col in train_df.columns if col != 'class']
        
        X_train = train_df[feature_cols].values
        y_train = train_df['class'].values
        
        X_val = val_df[feature_cols].values
        y_val = val_df['class'].values
        
        X_test = test_df[feature_cols].values
        y_test = test_df['class'].values
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = HRMDataset(X_train, y_train)
        val_dataset = HRMDataset(X_val, y_val)
        test_dataset = HRMDataset(X_test, y_test)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self._create_dataloader(train_dataset, shuffle=True, batch_size=batch_size)
        val_loader = self._create_dataloader(val_dataset, shuffle=False, batch_size=batch_size)
        test_loader = self._create_dataloader(test_dataset, shuffle=False, batch_size=batch_size)
        
        return train_loader, val_loader, test_loader
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_split_info
    
    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç›¸å…³ä¿¡æ¯"""
        return {
            "workflow_type": self.data_split_info.get("workflow_type", "unknown"),
            "train_period": self.data_split_info.get("train", {}),
            "validation_period": self.data_split_info.get("validation", {}),
            "test_period": self.data_split_info.get("test", {}),
            "data_config": self.data_config
        }

# =============================================================================
# æ¨¡å‹åŸºç±»
# =============================================================================

class BaseModel(nn.Module, ABC):
    """æ¨¡å‹åŸºç±»"""
    
    def __init__(self, config: HRMConfig):
        super().__init__()
        self.config = config
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        }

# =============================================================================
# HRMæ¨¡å‹å®ç°
# =============================================================================

class HRMModel(BaseModel):
    """HRMä¸»æ¨¡å‹ç±»"""
    
    def __init__(self, config: HRMConfig):
        super().__init__(config)
        
        # è¾“å…¥ç»´åº¦å¿…é¡»ä»é…ç½®ä¸­è·å–ï¼Œä¸èƒ½ç¡¬ç¼–ç 
        self.input_dim = getattr(config, 'input_dim', None)
        
        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰input_dimï¼Œå¿…é¡»åœ¨è®­ç»ƒæ—¶åŠ¨æ€è®¾ç½®
        if self.input_dim is None:
            print("âš ï¸ é…ç½®ä¸­æœªæŒ‡å®šinput_dimï¼Œå°†åœ¨æ•°æ®åŠ è½½æ—¶è‡ªåŠ¨æ£€æµ‹")
        
        # æ¨¡å‹å‚æ•°
        self.d_model = config.d_model
        self.n_layers = config.n_layers
        self.n_heads = config.n_heads
        self.hierarchical_levels = config.hierarchical_levels
        self.reasoning_depth = config.reasoning_depth
        self.h_cycles = config.h_cycles
        self.l_cycles = config.l_cycles
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç½‘ç»œå±‚ï¼Œç­‰å¾…input_dimç¡®å®š
        self.layers = None
        
    def _build_layers(self, input_dim):
        """ä½¿ç”¨åŠ¨æ€è¾“å…¥ç»´åº¦æ„å»ºç½‘ç»œï¼Œç»å¯¹ä¸èƒ½ç¡¬ç¼–ç """
        print(f"ğŸ”§ æ„å»ºç½‘ç»œå±‚ï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
        
        # ç‰¹å¾æå–å™¨
        feature_extractor = FeatureExtractor(input_dim, self.d_model, self.config.dropout)
        
        # æ¨ç†å±‚
        reasoning_layers = nn.ModuleList([
            ReasoningBlock(
                d_model=self.d_model,
                d_ff=self.config.d_ff,
                n_heads=self.n_heads,
                dropout=self.config.dropout,
                activation=self.config.activation,
                norm_type=self.config.norm_type,
                norm_eps=self.config.norm_eps
            ) for _ in range(self.n_layers)
        ])
        
        # å±‚çº§æ¨ç†æ¨¡å—
        hierarchical_reasoning = nn.ModuleList([
            nn.Sequential(
                nn.Linear(self.d_model, self.d_model),
                nn.ReLU(),
                nn.Dropout(self.config.dropout)
            ) for _ in range(self.hierarchical_levels)
        ])
        
        # è¾“å‡ºæŠ•å½±
        output_projection = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.config.dropout),
            nn.Linear(self.d_model // 2, self.config.num_classes)
        )
        
        return nn.ModuleDict({
            'feature_extractor': feature_extractor,
            'reasoning_layers': reasoning_layers,
            'hierarchical_reasoning': hierarchical_reasoning,
            'output_projection': output_projection
        })
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # ç¡®ä¿è¾“å…¥æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        try:
            model_params = list(self.parameters())
            if len(model_params) > 0:
                model_device = next(iter(model_params)).device
                if x.device != model_device:
                    x = x.to(model_device)
        except StopIteration:
            # å¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰å‚æ•°ï¼Œè·³è¿‡è®¾å¤‡æ£€æŸ¥
            pass
        
        # é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºç½‘ç»œ
        if self.layers is None:
            if self.input_dim is None:
                self.input_dim = x.shape[-1]  # ä»è¾“å…¥æ•°æ®è‡ªåŠ¨æ£€æµ‹
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç»´åº¦: {self.input_dim}")
            self.layers = self._build_layers(self.input_dim)
            # ç¡®ä¿æ–°æ„å»ºçš„å±‚åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.layers.to(x.device)
            print(f"ğŸ”§ ç½‘ç»œå±‚æ„å»ºå®Œæˆï¼Œå·²ç§»åŠ¨åˆ°è®¾å¤‡: {x.device}")
        
        # ç‰¹å¾æå–
        features = self.layers['feature_extractor'](x)  # [batch_size, 1, d_model]
        
        # å­˜å‚¨æ³¨æ„åŠ›æƒé‡
        attention_weights = []
        
        # å±‚çº§æ¨ç†å¾ªç¯
        for h_cycle in range(self.h_cycles):
            # é«˜å±‚æ¨ç†
            for layer in self.layers['reasoning_layers']:
                features, attn_weights = layer(features)
                attention_weights.append(attn_weights)
            
            # å±‚çº§ç‰¹å¾èåˆ
            for level, hierarchical_layer in enumerate(self.layers['hierarchical_reasoning']):
                if level < len(self.layers['hierarchical_reasoning']) - 1:
                    features = features + hierarchical_layer(features)
        
        # ä½å±‚æ¨ç†å¾ªç¯
        for l_cycle in range(self.l_cycles):
            for layer in self.layers['reasoning_layers'][:self.reasoning_depth]:
                features, attn_weights = layer(features)
                attention_weights.append(attn_weights)
        
        # å…¨å±€æ± åŒ–
        features = features.mean(dim=1)  # [batch_size, d_model]
        
        # è¾“å‡ºæŠ•å½±
        output = self.layers['output_projection'](features)  # [batch_size, num_classes]
        
        # å­˜å‚¨æ³¨æ„åŠ›æƒé‡ç”¨äºåˆ†æ
        self._last_attention_weights = attention_weights
        
        return output

# =============================================================================
# è®­ç»ƒå™¨ç±»
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config: HRMConfig, model: HRMModel, data_loader_manager: HRMDataLoaderManager,
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir  # Checkpointä¿å­˜ç›®å½•
        
        # è®¾å¤‡è®¾ç½®
        self.device = setup_device(getattr(config, 'device', 'auto'))
        self.rank = 0
        self.world_size = 1
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logging(config.logs_dir, "training")
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåªåœ¨GPUä¸Šå¯ç”¨ï¼‰
        self.scaler = GradScaler() if config.use_mixed_precision and torch.cuda.is_available() and self.device.type == 'cuda' else None
        self.use_amp = config.use_mixed_precision and torch.cuda.is_available() and self.device.type == 'cuda'
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_score = 0.0
        self.train_history = []
        
        # Checkpointè·Ÿè¸ªå™¨ï¼ˆæ”¯æŒå¤šåŒºé—´æœ€ä½³ï¼‰
        self.checkpoint_tracker = {
            'global_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'early_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'mid_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'late_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None}
        }
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(config.seed)
        
        # åˆ›å»ºç›®å½•
        create_directories(config)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡å¹¶è®°å½•GPUä¿¡æ¯
        self.model.to(self.device)
        
        # ç¡®ä¿æŸå¤±å‡½æ•°ä¹Ÿåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self.criterion, 'to'):
            self.criterion.to(self.device)
        
        self._log_device_info()
        
        # è®°å½•æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if self.use_amp:
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        else:
            self.logger.info("â„¹ï¸ æ··åˆç²¾åº¦è®­ç»ƒæœªå¯ç”¨ï¼ˆéœ€è¦GPUæ”¯æŒï¼‰")
        
        # è®°å½•æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_info = self.data_loader_manager.get_data_info()
        self.logger.info(f"æ•°æ®åˆ†å‰²ä¿¡æ¯: {data_info['workflow_type']}")
        self.logger.info(f"è®­ç»ƒæœŸé—´: {data_info['train_period']}")
        self.logger.info(f"éªŒè¯æœŸé—´: {data_info['validation_period']}")
        self.logger.info(f"æµ‹è¯•æœŸé—´: {data_info['test_period']}")
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡å’ŒGPUä½¿ç”¨ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            
            self.logger.info(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name}")
            self.logger.info(f"ğŸ“Š GPUæ€»å†…å­˜: {gpu_memory:.1f}GB")
            self.logger.info(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {current_memory:.2f}GB")
            self.logger.info(f"ğŸ“Š å¯ç”¨å†…å­˜: {gpu_memory - current_memory:.2f}GB")
        else:
            self.logger.info("ğŸ–¥ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            
        # è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f}MB")
    
    def _check_device_consistency(self, data: torch.Tensor, target: torch.Tensor) -> bool:
        """æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§"""
        try:
            # å®‰å…¨åœ°è·å–æ¨¡å‹è®¾å¤‡ï¼Œå¤„ç†æ¨¡å‹å‚æ•°ä¸ºç©ºçš„æƒ…å†µ
            model_params = list(self.model.parameters())
            if len(model_params) == 0:
                # å¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰å‚æ•°ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰ï¼Œä½¿ç”¨è®­ç»ƒå™¨çš„è®¾å¤‡
                model_device = self.device
            else:
                model_device = next(iter(model_params)).device
            
            data_device = data.device
            target_device = target.device
            
            if model_device != data_device or model_device != target_device:
                self.logger.warning(f"è®¾å¤‡ä¸åŒ¹é…ï¼æ¨¡å‹: {model_device}, æ•°æ®: {data_device}, æ ‡ç­¾: {target_device}")
                return False
            return True
        except Exception as e:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œè®°å½•è­¦å‘Šä½†ä¸ä¸­æ–­è®­ç»ƒ
            self.logger.warning(f"è®¾å¤‡ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # æŸå¤±å‡½æ•°ï¼ˆå¯ä»¥æå‰åˆå§‹åŒ–ï¼‰
        if self.config.model_type == "classification":
            self.criterion = nn.CrossEntropyLoss()
        else:
            self.criterion = nn.MSELoss()
        
        # ç¡®ä¿æŸå¤±å‡½æ•°åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.criterion.to(self.device)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨å°†åœ¨æ¨¡å‹ç½‘ç»œå±‚æ„å»ºåå»¶è¿Ÿåˆå§‹åŒ–
        self.optimizer = None
        self.scheduler = None
    
    def _setup_optimizer_and_scheduler(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆåœ¨æ¨¡å‹ç½‘ç»œå±‚æ„å»ºåï¼‰"""
        if self.optimizer is not None:
            return  # å·²ç»åˆå§‹åŒ–è¿‡äº†
        
        # ç¡®ä¿æ¨¡å‹æœ‰å‚æ•°
        model_params = list(self.model.parameters())
        if len(model_params) == 0:
            raise RuntimeError("æ¨¡å‹æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œæ— æ³•åˆ›å»ºä¼˜åŒ–å™¨")
        
        # ä¼˜åŒ–å™¨
        if self.config.optimizer == 'adamw':
            self.optimizer = optim.AdamW(
                model_params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        else:
            self.optimizer = optim.Adam(
                model_params,
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.config.scheduler == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.epochs
            )
        elif self.config.scheduler == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=10,
                gamma=0.1
            )
        else:
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='max', patience=5, factor=0.5
            )
        
        self.logger.info(f"âœ… ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹å‚æ•°æ•°é‡: {len(model_params)}")
        
        # è®°å½•æ¨¡å‹ä¿¡æ¯
        model_info = self.model.get_model_info()
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {model_info}")
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            try:
                # ç¡®ä¿æ•°æ®å’Œç›®æ ‡éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                # åœ¨ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­ååˆå§‹åŒ–ä¼˜åŒ–å™¨
                if self.optimizer is None:
                    # å…ˆè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ä»¥æ„å»ºæ¨¡å‹ç½‘ç»œå±‚
                    with torch.no_grad():
                        # ç¡®ä¿æ¨¡å‹å’Œæ•°æ®åœ¨åŒä¸€è®¾å¤‡ä¸Š
                        test_data = data[:1].to(self.device)
                        _ = self.model(test_data)  # åªç”¨ä¸€ä¸ªæ ·æœ¬æ¥è§¦å‘ç½‘ç»œæ„å»º
                    # ç°åœ¨åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
                    self._setup_optimizer_and_scheduler()
                    self.logger.info(f"âœ… ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æ­£å¼è®­ç»ƒ")
                
                # æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™å¼ºåˆ¶ç§»åŠ¨
                if not self._check_device_consistency(data, target):
                    data = data.to(self.device)
                    target = target.to(self.device)
                    self.logger.debug(f"å·²å¼ºåˆ¶ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡: {self.device}")
                
                self.optimizer.zero_grad()
                
                # æ··åˆç²¾åº¦è®­ç»ƒ
                if self.use_amp:
                    with autocast():
                        output = self.model(data)
                        loss = self.criterion(output, target)
                    
                    # æ£€æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
                    if torch.isnan(loss) or torch.isinf(loss):
                        self.logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆlosså€¼: {loss.item()}, è·³è¿‡æ­¤batch")
                        continue
                    
                    # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                    self.scaler.scale(loss).backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    if hasattr(self.config, 'gradient_clip_value'):
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                    
                    # ä¼˜åŒ–å™¨æ­¥è¿›
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # æ ‡å‡†è®­ç»ƒ
                    output = self.model(data)
                    loss = self.criterion(output, target)
                    
                    # æ£€æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
                    if torch.isnan(loss) or torch.isinf(loss):
                        self.logger.warning(f"æ£€æµ‹åˆ°æ— æ•ˆlosså€¼: {loss.item()}, è·³è¿‡æ­¤batch")
                        continue
                    
                    loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    if hasattr(self.config, 'gradient_clip_value'):
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                    
                    self.optimizer.step()
                
                total_loss += loss.item()
                
                if self.config.model_type == "classification":
                    preds = torch.argmax(output, dim=1)
                    all_preds.extend(preds.cpu().numpy())
                    all_targets.extend(target.cpu().numpy())
                
                # æ¯100ä¸ªbatchè®°å½•ä¸€æ¬¡è¿›åº¦
                if batch_idx % 100 == 0:
                    self.logger.debug(f"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
                    
            except Exception as e:
                self.logger.error(f"è®­ç»ƒbatch {batch_idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                # ç»§ç»­ä¸‹ä¸€ä¸ªbatchè€Œä¸æ˜¯ä¸­æ–­æ•´ä¸ªè®­ç»ƒ
                continue
        
        metrics = {"loss": total_loss / len(train_loader) if len(train_loader) > 0 else 0.0}
        
        if self.config.model_type == "classification" and len(all_targets) > 0:
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        
        return metrics
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(val_loader):
                try:
                    # ç¡®ä¿æ•°æ®å’Œç›®æ ‡éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    data = data.to(self.device, non_blocking=True)
                    target = target.to(self.device, non_blocking=True)
                    
                    # æ£€æŸ¥è®¾å¤‡ä¸€è‡´æ€§ï¼Œå¦‚æœä¸ä¸€è‡´åˆ™å¼ºåˆ¶ç§»åŠ¨
                    if not self._check_device_consistency(data, target):
                        data = data.to(self.device)
                        target = target.to(self.device)
                        self.logger.debug(f"éªŒè¯æ—¶å·²å¼ºåˆ¶ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡: {self.device}")
                    
                    # æ··åˆç²¾åº¦æ¨ç†
                    if self.use_amp:
                        with autocast():
                            output = self.model(data)
                            loss = self.criterion(output, target)
                    else:
                        output = self.model(data)
                        loss = self.criterion(output, target)
                    
                    # æ£€æŸ¥lossæ˜¯å¦æœ‰æ•ˆ
                    if torch.isnan(loss) or torch.isinf(loss):
                        self.logger.warning(f"éªŒè¯æ—¶æ£€æµ‹åˆ°æ— æ•ˆlosså€¼: {loss.item()}, è·³è¿‡æ­¤batch")
                        continue
                    
                    total_loss += loss.item()
                    
                    if self.config.model_type == "classification":
                        probs = torch.softmax(output, dim=1)
                        preds = torch.argmax(output, dim=1)
                        all_probs.extend(probs.cpu().numpy())
                        all_preds.extend(preds.cpu().numpy())
                        all_targets.extend(target.cpu().numpy())
                        
                except Exception as e:
                    self.logger.error(f"éªŒè¯batch {batch_idx} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    # æ¸…ç†GPUå†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    # ç»§ç»­ä¸‹ä¸€ä¸ªbatch
                    continue
        
        metrics = {"loss": total_loss / len(val_loader) if len(val_loader) > 0 else 0.0}
        
        if self.config.model_type == "classification" and len(all_targets) > 0:
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
            metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
            
            # è®¡ç®—AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡è¾“å‡ºï¼‰
            if len(all_probs) > 0 and len(np.unique(all_targets)) == 2:
                try:
                    all_probs_array = np.array(all_probs)
                    if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                        metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
                        # æ·»åŠ éªŒè¯é¢„æµ‹æ•°æ®ç”¨äºOptunaä¼˜åŒ–
                        metrics["y_true_val"] = all_targets
                        metrics["y_prob_val"] = all_probs_array[:, 1].tolist()
                except Exception:
                    pass  # å¦‚æœæ— æ³•è®¡ç®—AUCï¼Œè·³è¿‡
        
        return metrics
    
    def train(self, batch_size: int = None, no_save_model: bool = False) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        try:
            # è·å–æ•°æ®åŠ è½½å™¨
            batch_size = batch_size or self.config.batch_size
            train_loader, val_loader, test_loader = self.data_loader_manager.load_data_loaders(
                batch_size=batch_size)
            
            self.setup_training_components()
            
            self.logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
            self.logger.info(f"ğŸ“Š è®­ç»ƒé…ç½®: epochs={self.config.epochs}, batch_size={batch_size}, lr={self.config.learning_rate}")
            self.logger.info(f"ğŸ“Š æ•°æ®é›†å¤§å°: è®­ç»ƒ={len(train_loader.dataset)}, éªŒè¯={len(val_loader.dataset)}")
            
            start_time = time.time()
            epochs = self.config.epochs
            
            for epoch in range(epochs):
                epoch_start = time.time()
                
                try:
                    # è®­ç»ƒ
                    self.logger.info(f"ğŸ”„ å¼€å§‹è®­ç»ƒ Epoch {epoch+1}/{epochs}")
                    train_metrics = self.train_epoch(train_loader)
                    
                    # éªŒè¯
                    self.logger.info(f"ğŸ” å¼€å§‹éªŒè¯ Epoch {epoch+1}/{epochs}")
                    val_metrics = self.validate_epoch(val_loader)
                    
                    # è®°å½•å†å²
                    epoch_info = {
                        "epoch": epoch + 1,
                        "train_loss": train_metrics["loss"],
                        "val_loss": val_metrics["loss"],
                        "lr": self.optimizer.param_groups[0]['lr'] if self.optimizer is not None else self.config.learning_rate,
                        "time": time.time() - epoch_start
                    }
                    
                    if self.config.model_type == "classification":
                        epoch_info.update({
                            "train_acc": train_metrics.get("accuracy", 0),
                            "val_acc": val_metrics.get("accuracy", 0),
                            "val_f1": val_metrics.get("f1", 0)
                        })
                        if 'auc' in val_metrics:
                            epoch_info["train_auc"] = 0  # ç®€åŒ–ï¼Œè®­ç»ƒæ—¶ä¸è®¡ç®—AUC
                            epoch_info["val_auc"] = val_metrics["auc"]
                        current_score = val_metrics.get("accuracy", 0)
                    else:
                        current_score = -val_metrics["loss"]  # å›å½’ä»»åŠ¡ä½¿ç”¨è´ŸæŸå¤±
                    
                    self.train_history.append(epoch_info)
                    
                    # å­¦ä¹ ç‡è°ƒåº¦ï¼ˆç¡®ä¿è°ƒåº¦å™¨å·²åˆå§‹åŒ–ï¼‰
                    if self.scheduler is not None:
                        if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                            self.scheduler.step(current_score)
                        else:
                            self.scheduler.step()
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if current_score > self.best_val_score:
                        self.best_val_score = current_score
                        if not no_save_model:
                            self.save_checkpoint(epoch + 1, is_best=True)
                    
                    # GPUå†…å­˜ç›‘æ§ï¼ˆæ¯10ä¸ªepochè®°å½•ä¸€æ¬¡ï¼‰
                    if self.device.type == 'cuda' and (epoch + 1) % 10 == 0:
                        current_memory = torch.cuda.memory_allocated() / 1024**3
                        max_memory = torch.cuda.max_memory_allocated() / 1024**3
                        cached_memory = torch.cuda.memory_reserved() / 1024**3
                        self.logger.info(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨ (Epoch {epoch+1}): å½“å‰ {current_memory:.2f}GB, å³°å€¼ {max_memory:.2f}GB, ç¼“å­˜ {cached_memory:.2f}GB")
                    
                    # æ—¥å¿—è¾“å‡º
                    log_msg = (
                        f"âœ… Epoch {epoch+1}/{epochs} å®Œæˆ - "
                        f"Train Loss: {train_metrics['loss']:.4f}, "
                        f"Val Loss: {val_metrics['loss']:.4f}"
                    )

                    if self.config.model_type == "classification":
                        log_msg += f", Val Acc: {val_metrics.get('accuracy', 0):.4f}"
                        if 'auc' in val_metrics:
                            log_msg += f", Val AUC: {val_metrics['auc']:.4f}"

                    log_msg += f", Time: {epoch_info['time']:.2f}s"
                    self.logger.info(log_msg)
                    
                except Exception as e:
                    self.logger.error(f"âŒ Epoch {epoch+1} è®­ç»ƒå¤±è´¥: {e}")
                    # æ¸…ç†GPUå†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    # ç»§ç»­ä¸‹ä¸€ä¸ªepochè€Œä¸æ˜¯å®Œå…¨åœæ­¢è®­ç»ƒ
                    continue
            
            total_time = time.time() - start_time
            
            # ä¿å­˜è®­ç»ƒå†å²
            self.save_training_history()
            
            self.logger.info(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {total_time:.2f}s, æœ€ä½³éªŒè¯åˆ†æ•°: {self.best_val_score:.4f}")
            
            return {
                "best_val_score": self.best_val_score,
                "total_epochs": len(self.train_history),
                "total_time": total_time,
                "train_history": self.train_history
            }
            
        except Exception as e:
            self.logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            raise
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'best_val_score': self.best_val_score,
            'config': asdict(self.config)
        }
        
        # åªæœ‰åœ¨ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨å­˜åœ¨æ—¶æ‰ä¿å­˜å®ƒä»¬çš„çŠ¶æ€
        if self.optimizer is not None:
            checkpoint['optimizer_state_dict'] = self.optimizer.state_dict()
        if self.scheduler is not None:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        # ä½¿ç”¨ä¼ å…¥çš„checkpointç›®å½•ï¼ˆä¼˜å…ˆï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„
        checkpoint_dir = Path(self.checkpoint_dir) if self.checkpoint_dir else Path(self.config.checkpoint_dir)
        
        checkpoint_path = checkpoint_dir / "latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
    
    def save_checkpoint_if_best(self, epoch: int, val_loss: float):
        """æ™ºèƒ½checkpointä¿å­˜ï¼šå…¨å±€æœ€ä½³ + åŒºé—´æœ€ä½³"""
        # æ›´æ–°å…¨å±€æœ€ä½³
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        # æ›´æ–°åŒºé—´æœ€ä½³ï¼ˆå®æ—¶æ›´æ–°ï¼‰
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        # ğŸ’¡ å…³é”®ï¼šåœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³ä¸ºinterval_best
        if epoch == 29:  # epochä»0å¼€å§‹ï¼Œ29æ˜¯ç¬¬30ä¸ªepoch
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')
    
    def _save_checkpoint(self, epoch: int, val_loss: float, checkpoint_type: str):
        """ä¿å­˜å•ä¸ªcheckpoint"""
        import os
        
        # åˆ é™¤æ—§checkpoint
        old_path = self.checkpoint_tracker[checkpoint_type]['path']
        if old_path and os.path.exists(old_path):
            os.remove(old_path)
        
        # ä¿å­˜æ–°checkpoint
        checkpoint_name = f"checkpoint_{checkpoint_type}_epoch{epoch:03d}_val{val_loss:.4f}.pt"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'checkpoint_type': checkpoint_type,
            'config': asdict(self.config)
        }
        
        torch.save(checkpoint, checkpoint_path)
        
        self.checkpoint_tracker[checkpoint_type] = {
            'epoch': epoch,
            'val_loss': val_loss,
            'path': checkpoint_path
        }
        
        self.logger.info(f"ğŸ’¾ ä¿å­˜{checkpoint_type} checkpoint: epoch {epoch}, val_loss {val_loss:.4f}")
    
    def _save_interval_best(self, checkpoint_type: str, epoch_range: str):
        """åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³checkpointä¸ºinterval_best"""
        import shutil
        import os
        
        # è·å–å½“å‰åŒºé—´æœ€ä½³çš„checkpointè·¯å¾„
        source_path = self.checkpoint_tracker[checkpoint_type]['path']
        
        if source_path and os.path.exists(source_path):
            # åˆ›å»ºinterval_bestæ–‡ä»¶å
            interval_name = f"interval_best_{epoch_range}.pt"
            interval_path = os.path.join(self.checkpoint_dir, interval_name)
            
            # å¤åˆ¶checkpoint
            shutil.copy2(source_path, interval_path)
            
            epoch = self.checkpoint_tracker[checkpoint_type]['epoch']
            val_loss = self.checkpoint_tracker[checkpoint_type]['val_loss']
            
            self.logger.info(f"ğŸ“¦ å¤åˆ¶åŒºé—´æœ€ä½³ [{epoch_range}]: epoch {epoch}, val_loss {val_loss:.4f}")
            self.logger.info(f"   æºæ–‡ä»¶: {os.path.basename(source_path)}")
            self.logger.info(f"   ç›®æ ‡æ–‡ä»¶: {interval_name}")
        else:
            self.logger.warning(f"âš ï¸ åŒºé—´ [{epoch_range}] æ²¡æœ‰æ‰¾åˆ°æœ€ä½³checkpoint")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = Path(self.config.results_dir) / "training_history.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        self.logger.info(f"è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

# =============================================================================
# æ¨ç†å™¨ç±»
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, config: HRMConfig, model: HRMModel, data_loader_manager: HRMDataLoaderManager):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.device = setup_device()
        self.logger = setup_logging(config.logs_dir, "inference")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)
        self.model.eval()
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        return checkpoint
    
    def predict_batch(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """æ‰¹é‡æ¨ç†"""
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in data_loader:
                # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                data = data.to(self.device, non_blocking=True)
                outputs = self.model(data)
                
                if self.config.model_type == "classification":
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(outputs, dim=1)
                    all_probs.extend(probs.cpu().numpy())
                    all_preds.extend(preds.cpu().numpy())
                else:
                    all_preds.extend(outputs.cpu().numpy())
                    all_probs.extend(outputs.cpu().numpy())  # å›å½’ä»»åŠ¡æ¦‚ç‡å³ä¸ºé¢„æµ‹å€¼
        
        return np.array(all_preds), np.array(all_probs)
    
    def predict_single(self, data: np.ndarray) -> Tuple[Union[int, float], np.ndarray]:
        """å•æ ·æœ¬æ¨ç†"""
        data_tensor = torch.FloatTensor(data).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            output = self.model(data_tensor)
            
            if self.config.model_type == "classification":
                probs = torch.softmax(output, dim=1)
                pred = torch.argmax(output, dim=1).item()
                return pred, probs.cpu().numpy()[0]
            else:
                pred = output.item()
                return pred, np.array([pred])
    
    def evaluate(self) -> Dict[str, float]:
        """æ¨¡å‹è¯„ä¼°"""
        # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                # ç¡®ä¿æ•°æ®å’Œç›®æ ‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                outputs = self.model(data)
                
                if self.config.model_type == "classification":
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(outputs, dim=1)
                    all_probs.extend(probs.cpu().numpy())
                    all_preds.extend(preds.cpu().numpy())
                else:
                    all_preds.extend(outputs.cpu().numpy())
                
                all_targets.extend(target.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        
        if self.config.model_type == "classification":
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
            metrics["precision"] = precision_score(all_targets, all_preds, average='weighted')
            metrics["recall"] = recall_score(all_targets, all_preds, average='weighted')
            metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
            
            # AUCæŒ‡æ ‡è®¡ç®—
            if len(np.unique(all_targets)) == 2:  # äºŒåˆ†ç±»
                all_probs_positive = np.array(all_probs)[:, 1]
                metrics["auc"] = roc_auc_score(all_targets, all_probs_positive)
            elif len(np.unique(all_targets)) > 2:  # å¤šåˆ†ç±»
                try:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs, multi_class='ovr', average='weighted')
                except ValueError:
                    # å¦‚æœæ— æ³•è®¡ç®—å¤šåˆ†ç±»AUCï¼Œè·³è¿‡
                    pass
        else:
            from sklearn.metrics import mean_squared_error, mean_absolute_error
            metrics["mse"] = mean_squared_error(all_targets, all_preds)
            metrics["mae"] = mean_absolute_error(all_targets, all_preds)
            metrics["rmse"] = np.sqrt(metrics["mse"])
        
        return metrics
    
    def save_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, 
                        output_path: str):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        results = {
            "predictions": predictions.tolist(),
            "probabilities": probabilities.tolist(),
            "config": asdict(self.config),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: HRMConfig, model: HRMModel):
        self.config = config
        self.model = model
    
    def generate_model_md(self, output_path: str = "MODEL.md"):
        """ç”ŸæˆMODEL.mdæ–‡æ¡£"""
        model_info = self.model.get_model_info()
        
        content = f"""# {self.config.model_name} Model

## æ¨¡å‹æ¦‚è¿°

{self.config.model_name} æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„{self.config.model_type}æ¨¡å‹ï¼Œé‡‡ç”¨å±‚çº§æ¨ç†æœºåˆ¶ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- **ä»»åŠ¡ç±»å‹**: {self.config.model_type}
- **æ¨¡å‹å‚æ•°**: {model_info['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°**: {model_info['trainable_parameters']:,}
- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.2f} MB

## æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

- **ç‰¹å¾æå–å™¨**: å°†è¾“å…¥ç‰¹å¾æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦
- **å±‚çº§æ¨ç†æ¨¡å—**: å¤šå±‚æ¬¡çš„æ¨ç†æœºåˆ¶
- **æ³¨æ„åŠ›æœºåˆ¶**: å±‚çº§æ³¨æ„åŠ›æœºåˆ¶
- **è¾“å‡ºæŠ•å½±**: æœ€ç»ˆåˆ†ç±»/å›å½’è¾“å‡º

### ç½‘ç»œç»“æ„

```
è¾“å…¥ç‰¹å¾ -> ç‰¹å¾æå–å™¨ -> å±‚çº§æ¨ç†å¾ªç¯ -> è¾“å‡ºæŠ•å½± -> é¢„æµ‹ç»“æœ
```

## æŠ€æœ¯åŸç†

HRMæ¨¡å‹é‡‡ç”¨å±‚çº§æ¨ç†æœºåˆ¶ï¼Œé€šè¿‡å¤šä¸ªæ¨ç†å¾ªç¯å’Œæ³¨æ„åŠ›æœºåˆ¶æ¥å¤„ç†å¤æ‚çš„ç‰¹å¾å…³ç³»ã€‚

## é…ç½®å‚æ•°

### è®­ç»ƒé…ç½®
- å­¦ä¹ ç‡: {self.config.learning_rate}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- è®­ç»ƒè½®æ•°: {self.config.epochs}
- ä¼˜åŒ–å™¨: {self.config.optimizer}

### æ¨¡å‹é…ç½®
- æ¨¡å‹ç»´åº¦: {self.config.d_model}
- æ³¨æ„åŠ›å¤´æ•°: {self.config.n_heads}
- å±‚çº§æ•°é‡: {self.config.hierarchical_levels}
- æ¨ç†æ·±åº¦: {self.config.reasoning_depth}

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```python
python HRM_unified.py train --config config.yaml --data-config data.yaml
```

### æ¨¡å‹æ¨ç†

```python
python HRM_unified.py inference --checkpoint best_model.pth
```

### æ¨¡å‹è¯„ä¼°

```python
python HRM_unified.py inference --checkpoint best_model.pth --evaluate
```

## æ€§èƒ½æŒ‡æ ‡

æ¨¡å‹æ”¯æŒä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡ï¼š
- å‡†ç¡®ç‡ (Accuracy)
- ç²¾ç¡®ç‡ (Precision)
- å¬å›ç‡ (Recall)
- F1åˆ†æ•° (F1-Score)
- AUC (Area Under Curve)

## æ³¨æ„äº‹é¡¹

- æ¨¡å‹æ”¯æŒåŠ¨æ€è¾“å…¥ç»´åº¦é€‚é…
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒä»¥æå‡æ€§èƒ½
- æ”¯æŒGPUåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†

## æ›´æ–°æ—¥å¿—

- åˆå§‹ç‰ˆæœ¬: {datetime.now().strftime('%Y-%m-%d')}

"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¨¡å‹æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")

# =============================================================================
# ä¸»è¦æ¥å£å‡½æ•°
# =============================================================================

def create_model_factory(config: HRMConfig) -> HRMModel:
    """æ¨¡å‹å·¥å‚å‡½æ•°"""
    return HRMModel(config)

def create_data_loader_manager(data_config_path: str, config: HRMConfig) -> HRMDataLoaderManager:
    """æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨å·¥å‚å‡½æ•°"""
    return HRMDataLoaderManager(data_config_path, config)

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               optuna_config_path: str = None, device: str = "auto",
               epochs_override: int = None, no_save_model: bool = False,
               seed: int = 42, checkpoint_dir: str = "checkpoints"):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    import random
    import numpy as np
    import torch
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    logger.info(f"ğŸ’¾ Checkpointä¿å­˜ç›®å½•: {checkpoint_dir}")
    
    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # 1. åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config_dict = {}
    
    # å¤„ç†architectureéƒ¨åˆ†
    if 'architecture' in config_dict:
        arch = config_dict['architecture']
        # å°†architectureä¸­çš„å‚æ•°æå–åˆ°é¡¶å±‚
        for key, value in arch.items():
            if hasattr(HRMConfig, key):
                config_dict[key] = value
        # ç§»é™¤architectureéƒ¨åˆ†
        del config_dict['architecture']
    
    # ç§»é™¤å…¶ä»–ä¸å±äºHRMConfigçš„éƒ¨åˆ†
    valid_keys = {field.name for field in HRMConfig.__dataclass_fields__.values()}
    filtered_config = {k: v for k, v in config_dict.items() if k in valid_keys}
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = HRMConfig(**filtered_config)
    
    # åº”ç”¨Optunaé…ç½®è¦†ç›–
    if optuna_config_path and os.path.exists(optuna_config_path):
        with open(optuna_config_path, 'r', encoding='utf-8') as f:
            optuna_config = yaml.safe_load(f)
        
        # åº”ç”¨è¶…å‚æ•°è¦†ç›–
        for key, value in optuna_config.items():
            if hasattr(config, key):
                setattr(config, key, value)
                print(f"ğŸ”§ Optunaè¦†ç›–å‚æ•°: {key} = {value}")
    
    # è¦†ç›–é…ç½®
    if device != "auto":
        config.device = device
    if epochs_override:
        config.epochs = epochs_override
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # 3. åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¼ é€’checkpointç›®å½•ï¼‰
    trainer = UnifiedTrainer(config, model, data_loader_manager, checkpoint_dir=checkpoint_dir)
    
    # 5. æ‰§è¡Œè®­ç»ƒ
    results = trainer.train(no_save_model=no_save_model)
    
    # 6. è¾“å‡ºJSONæ ¼å¼ç»“æœä¾›Optunaè§£æ
    final_results = {
        "final_results": {
            "best_val_score": results['best_val_score'],
            "total_epochs": results['total_epochs'],
            "total_time": results['total_time'],
            "final_train_acc": results['train_history'][-1].get('train_acc', 0),
            "final_val_acc": results['train_history'][-1].get('val_acc', 0),
            "train_auc": results['train_history'][-1].get('train_auc', 0),
            "val_auc": results['train_history'][-1].get('val_auc', 0),
            "train_losses": [epoch['train_loss'] for epoch in results['train_history']],
            "val_losses": [epoch['val_loss'] for epoch in results['train_history']],
            "train_accuracies": [epoch.get('train_acc', 0) for epoch in results['train_history']],
            "val_accuracies": [epoch.get('val_acc', 0) for epoch in results['train_history']]
        }
    }
    print(json.dumps(final_results, indent=2))

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "best_model.pth"):
    """ä¸»æ¨ç†å‡½æ•°"""
    # 1. åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config_dict = {}
    
    # å¤„ç†architectureéƒ¨åˆ†
    if 'architecture' in config_dict:
        arch = config_dict['architecture']
        # å°†architectureä¸­çš„å‚æ•°æå–åˆ°é¡¶å±‚
        for key, value in arch.items():
            if hasattr(HRMConfig, key):
                config_dict[key] = value
        # ç§»é™¤architectureéƒ¨åˆ†
        del config_dict['architecture']
    
    # ç§»é™¤å…¶ä»–ä¸å±äºHRMConfigçš„éƒ¨åˆ†
    valid_keys = {field.name for field in HRMConfig.__dataclass_fields__.values()}
    filtered_config = {k: v for k, v in config_dict.items() if k in valid_keys}
    
    config = HRMConfig(**filtered_config)
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # 3. åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # 4. åˆ›å»ºæ¨ç†å™¨
    inferencer = UnifiedInferencer(config, model, data_loader_manager)
    
    # 5. åŠ è½½æ£€æŸ¥ç‚¹
    if os.path.exists(checkpoint_path):
        checkpoint = inferencer.load_checkpoint(checkpoint_path)
    else:
        print(f"âš ï¸ æ£€æŸ¥ç‚¹æ–‡ä»¶ {checkpoint_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
    
    # 6. æ‰§è¡Œæ¨ç†å’Œè¯„ä¼°
    metrics = inferencer.evaluate()
    
    # 7. è¾“å‡ºç»“æœ
    print("è¯„ä¼°ç»“æœ:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ä¸»æ–‡æ¡£ç”Ÿæˆå‡½æ•°"""
    # 1. åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config_dict = {}
    
    # å¤„ç†architectureéƒ¨åˆ†
    if 'architecture' in config_dict:
        arch = config_dict['architecture']
        # å°†architectureä¸­çš„å‚æ•°æå–åˆ°é¡¶å±‚
        for key, value in arch.items():
            if hasattr(HRMConfig, key):
                config_dict[key] = value
        # ç§»é™¤architectureéƒ¨åˆ†
        del config_dict['architecture']
    
    # ç§»é™¤å…¶ä»–ä¸å±äºHRMConfigçš„éƒ¨åˆ†
    valid_keys = {field.name for field in HRMConfig.__dataclass_fields__.values()}
    filtered_config = {k: v for k, v in config_dict.items() if k in valid_keys}
    
    config = HRMConfig(**filtered_config)
    
    # 2. åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # 3. ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config, model)
    doc_generator.generate_model_md("MODEL.md")

# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="HRMç»Ÿä¸€æ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·")
    parser.add_argument("mode", choices=["train", "inference", "docs"], help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="config.yaml", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-config", default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default="best_model.pth", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--data", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # Optunaä¼˜åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--optuna-config", help="Optunaè¯•éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--no-save-model", action="store_true", help="ä¸ä¿å­˜æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°æ€§ï¼‰")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    
    return parser.parse_args()

if __name__ == "__main__":
    # è®¾ç½®å›ºå®šçš„æ—¥å¿—è·¯å¾„
    log_dir = "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/training/logs/HRM_5021096712"
    logger = setup_logging(log_dir=log_dir, log_filename="log.txt")
    
    args = parse_args()
    
    if args.mode == "train":
        main_train(
            config_path=args.config,
            data_config_path=args.data_config,
            optuna_config_path=args.optuna_config,
            device=args.device,
            epochs_override=args.epochs,
            no_save_model=args.no_save_model,
            seed=args.seed,
            checkpoint_dir=args.checkpoint_dir
        )
    elif args.mode == "inference":
        main_inference(args.config, args.data_config, args.checkpoint)
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data_config)