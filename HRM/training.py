#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
training.py - HRM (Hierarchical Reasoning Model) è®­ç»ƒè„šæœ¬

è¯¥æ–‡ä»¶å®ç°äº†HRMæ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯
- æŸå¤±è®¡ç®—å’Œä¼˜åŒ–
- æ¨¡å‹ä¿å­˜å’Œè¯„ä¼°
- å¿«é€ŸéªŒè¯åŠŸèƒ½
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, TensorDataset
import pandas as pd
import numpy as np
import yaml
import argparse
import logging
import os
import json
import random
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List, Union
from datetime import datetime
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹
try:
    from model import HRM, HRMConfig, setup_device, set_all_seeds, create_model
except ImportError as e:
    print(f"âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥æ¨¡å‹æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿ model.py æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®")
    # åˆ›å»ºå ä½ç¬¦å‡½æ•°ä»¥é¿å…è¿è¡Œæ—¶é”™è¯¯
    def setup_device():
        import torch
        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def set_all_seeds(seed):
        import torch
        import numpy as np
        import random
        torch.manual_seed(seed)
        np.random.seed(seed)
        random.seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
    
    # åˆ›å»ºå ä½ç¬¦ç±»
    class HRM(torch.nn.Module):
        def __init__(self, config):
            super().__init__()
            self.linear = torch.nn.Linear(config.arch['input_dim'], config.arch['num_classes'])
        
        def forward(self, x):
            return self.linear(x)
    
    class HRMConfig:
        def __init__(self, config_path):
            self.arch = {'input_dim': 100, 'num_classes': 2, 'hidden_dim': 128}
            self.training = {'batch_size': 32, 'learning_rate': 0.001, 'weight_decay': 1e-5, 'epochs': 10}
        
        def to_dict(self):
            return {'arch': self.arch, 'training': self.training}
    
    def create_model(config):
        return HRM(config)


class HRMDataset(Dataset):
    """HRMæ¨¡å‹ä¸“ç”¨æ•°æ®é›†ç±»"""
    
    def __init__(self, features: np.ndarray, labels: np.ndarray, transform=None):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            features: ç‰¹å¾æ•°æ®
            labels: æ ‡ç­¾æ•°æ®
            transform: æ•°æ®å˜æ¢å‡½æ•°
        """
        self.features = torch.FloatTensor(features)
        self.labels = torch.LongTensor(labels)
        self.transform = transform
    
    def __len__(self) -> int:
        return len(self.features)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        feature = self.features[idx]
        label = self.labels[idx]
        
        if self.transform:
            feature = self.transform(feature)
            
        return feature, label


class HRMTrainer:
    """HRMæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            data_config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # è®¾ç½®æ—¥å¿—ï¼ˆå¿…é¡»å…ˆè®¾ç½®ï¼Œå› ä¸ºå…¶ä»–æ–¹æ³•ä¼šç”¨åˆ°loggerï¼‰
        self._setup_logging()
        
        # åŠ è½½é…ç½®
        try:
            self.config = HRMConfig(config_path)
        except Exception as e:
            self.logger.error(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            # åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„é…ç½®å¯¹è±¡
            self.config = self._create_default_config()
        
        self.data_config = self._load_data_config(data_config_path)
        
        # è®¾ç½®ç¯å¢ƒ
        set_all_seeds(42)
        self.device = setup_device()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.scaler = StandardScaler()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_acc = 0.0
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        
    def _load_data_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(config_path):
                self.logger.warning(f"æ•°æ®é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self._get_default_data_config()
            
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                if config is None:
                    self.logger.warning("æ•°æ®é…ç½®æ–‡ä»¶ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                    return self._get_default_data_config()
                return config
        except Exception as e:
            self.logger.error(f"åŠ è½½æ•°æ®é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_data_config()
    
    def _get_default_data_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ•°æ®é…ç½®"""
        return {
            'data_paths': {
                'data_folder': './data/feature_set',
                'data_phase': 1
            },
            'task': {
                'type': 'binary_classification'
            },
            'preprocessing': {
                'scaling': {
                    'method': 'standard'
                }
            }
        }
    
    def _create_default_config(self):
        """åˆ›å»ºé»˜è®¤æ¨¡å‹é…ç½®"""
        class DefaultConfig:
            def __init__(self):
                self.arch = {
                    'input_dim': 100,  # é»˜è®¤ç‰¹å¾ç»´åº¦
                    'hidden_dim': 128,
                    'num_classes': 2,
                    'num_layers': 3,
                    'dropout': 0.1
                }
                self.training = {
                    'batch_size': 32,
                    'learning_rate': 0.001,
                    'weight_decay': 1e-5,
                    'epochs': 10,
                    'optimizer': 'adam',
                    'scheduler': 'cosine',
                    'gradient_clip_norm': 1.0
                }
            
            def to_dict(self):
                return {
                    'arch': self.arch,
                    'training': self.training
                }
        
        return DefaultConfig()
    
    def _generate_mock_data(self, quick_test: bool = False) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        self.logger.info("ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•...")
        
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        n_samples = 100 if quick_test else 1000
        n_features = 50
        
        # ç”Ÿæˆç‰¹å¾æ•°æ®
        X = np.random.randn(n_samples, n_features)
        # ç”ŸæˆäºŒåˆ†ç±»æ ‡ç­¾
        y = np.random.randint(0, 2, n_samples)
        
        self.logger.info(f"æ¨¡æ‹Ÿæ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        self.logger.info(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
        
        # æ•°æ®æ ‡å‡†åŒ–
        X = self.scaler.fit_transform(X)
        
        # æ•°æ®åˆ†å‰²
        X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = HRMDataset(X_train, y_train)
        val_dataset = HRMDataset(X_val, y_val)
        test_dataset = HRMDataset(X_test, y_test)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = 16 if quick_test else 32
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
        
        # æ›´æ–°é…ç½®ä¸­çš„ç‰¹å¾æ•°é‡
        self.config.arch['input_dim'] = X.shape[1]
        self.config.arch['num_classes'] = len(np.unique(y))
        
        self.logger.info(f"æ¨¡æ‹Ÿæ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}, æµ‹è¯•é›†: {len(test_dataset)}")
        
        return train_loader, val_loader, test_loader
    
    def _setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "training.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self, quick_test: bool = False) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        
        Args:
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨
        """
        try:
            # è·å–æ•°æ®è·¯å¾„
            data_folder = self.data_config.get('data_paths', {}).get('data_folder', './data/feature_set')
            data_phase = self.data_config.get('data_paths', {}).get('data_phase', 1)
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®è·¯å¾„
            possible_paths = [
                data_folder,
                '../../../data/feature_set',
                '../../data/feature_set',
                './data/feature_set',
                '../data/feature_set'
            ]
            
            data_files = []
            pattern = f"mrmr_task_{data_phase}_*.pq"
            
            for path in possible_paths:
                if os.path.exists(path):
                    files = glob.glob(os.path.join(path, pattern))
                    if files:
                        data_files = files
                        data_folder = path
                        break
            
            if not data_files:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°parquetæ–‡ä»¶ï¼Œå°è¯•æŸ¥æ‰¾CSVæ–‡ä»¶
                for path in possible_paths:
                    if os.path.exists(path):
                        csv_pattern = f"*task_{data_phase}*.csv"
                        files = glob.glob(os.path.join(path, csv_pattern))
                        if files:
                            data_files = files
                            data_folder = path
                            break
            
            if not data_files:
                # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
                self.logger.warning(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•")
                return self._generate_mock_data(quick_test)
            
            # åŠ è½½æ•°æ®
            self.logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {data_files[0]}")
            if data_files[0].endswith('.pq'):
                df = pd.read_parquet(data_files[0])
            else:
                df = pd.read_csv(data_files[0])
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨å°‘é‡æ•°æ®
            if quick_test:
                df = df.head(100)
                self.logger.info("å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨100ä¸ªæ ·æœ¬")
            
            # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
            feature_cols = [col for col in df.columns if '@' in col]
            
            if not feature_cols:
                raise ValueError("æœªæ‰¾åˆ°ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰")
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            X = df[feature_cols].values
            y = df['class'].values
            
            self.logger.info(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
            self.logger.info(f"ç‰¹å¾åˆ—æ•°: {len(feature_cols)}")
            self.logger.info(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
            
            # æ•°æ®æ ‡å‡†åŒ–
            X = self.scaler.fit_transform(X)
            
            # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆåŸºäºdateå­—æ®µï¼‰
            if 'date' in df.columns:
                df_sorted = df.sort_values('date')
                train_size = int(0.7 * len(df_sorted))
                val_size = int(0.15 * len(df_sorted))
                
                train_idx = df_sorted.index[:train_size]
                val_idx = df_sorted.index[train_size:train_size + val_size]
                test_idx = df_sorted.index[train_size + val_size:]
                
                X_train, y_train = X[train_idx], y[train_idx]
                X_val, y_val = X[val_idx], y[val_idx]
                X_test, y_test = X[test_idx], y[test_idx]
            else:
                # éšæœºåˆ†å‰²
                X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
                X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = HRMDataset(X_train, y_train)
            val_dataset = HRMDataset(X_val, y_val)
            test_dataset = HRMDataset(X_test, y_test)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            batch_size = 16 if quick_test else self.config.training['batch_size']
            
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
            
            # æ›´æ–°é…ç½®ä¸­çš„ç‰¹å¾æ•°é‡
            self.config.arch['input_dim'] = X.shape[1]
            self.config.arch['num_classes'] = len(np.unique(y))
            
            self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ - è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}, æµ‹è¯•é›†: {len(test_dataset)}")
            
            return train_loader, val_loader, test_loader
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def build_model(self) -> None:
        """æ„å»ºæ¨¡å‹"""
        try:
            self.model = HRM(self.config).to(self.device)
            
            # è®¾ç½®æŸå¤±å‡½æ•°
            self.criterion = nn.CrossEntropyLoss()
            
            # è®¾ç½®ä¼˜åŒ–å™¨
            training_config = self.config.training
            learning_rate = float(training_config['learning_rate'])
            weight_decay = float(training_config['weight_decay'])
            
            if training_config.get('optimizer') == 'adamw':
                self.optimizer = optim.AdamW(
                    self.model.parameters(),
                    lr=learning_rate,
                    weight_decay=weight_decay
                )
            else:
                self.optimizer = optim.Adam(
                    self.model.parameters(),
                    lr=learning_rate,
                    weight_decay=weight_decay
                )
            
            # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
            training_config = self.config.training
            scheduler_type = training_config.get('scheduler', 'cosine')
            if scheduler_type == 'cosine':
                self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                    self.optimizer,
                    T_max=training_config['epochs']
                )
            elif scheduler_type == 'step':
                self.scheduler = optim.lr_scheduler.StepLR(
                    self.optimizer,
                    step_size=10,
                    gamma=0.1
                )
            
            self.logger.info(f"æ¨¡å‹æ„å»ºå®Œæˆ - å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹æ„å»ºå¤±è´¥: {e}")
            raise
    
    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        """
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            gradient_clip = self.config.training.get('gradient_clip_norm', 0)
            if gradient_clip > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    gradient_clip
                )
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 10 == 0:
                self.logger.info(f'è®­ç»ƒæ‰¹æ¬¡ {batch_idx}/{len(train_loader)}, æŸå¤±: {loss.item():.6f}')
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate(self, val_loader: DataLoader) -> Tuple[float, float, Dict[str, float]]:
        """
        éªŒè¯æ¨¡å‹
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡æŸå¤±ã€å‡†ç¡®ç‡å’Œè¯¦ç»†æŒ‡æ ‡
        """
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                pred = output.argmax(dim=1)
                prob = torch.softmax(output, dim=1)
                
                all_preds.extend(pred.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_probs.extend(prob.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        all_preds = np.array(all_preds)
        all_targets = np.array(all_targets)
        all_probs = np.array(all_probs)
        
        accuracy = accuracy_score(all_targets, all_preds)
        precision = precision_score(all_targets, all_preds, average='weighted', zero_division=0)
        recall = recall_score(all_targets, all_preds, average='weighted', zero_division=0)
        f1 = f1_score(all_targets, all_preds, average='weighted', zero_division=0)
        
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
        
        # è®¡ç®—AUCï¼ˆå¦‚æœæ˜¯äºŒåˆ†ç±»ï¼‰
        if len(np.unique(all_targets)) == 2:
            try:
                auc = roc_auc_score(all_targets, all_probs[:, 1])
                metrics['auc'] = auc
            except:
                metrics['auc'] = 0.0
        
        return avg_loss, accuracy * 100, metrics
    
    def save_model(self, epoch: int, val_acc: float, is_best: bool = False) -> None:
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            epoch: å½“å‰epoch
            val_acc: éªŒè¯å‡†ç¡®ç‡
            is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
        """
        try:
            # åˆ›å»ºä¿å­˜ç›®å½•
            save_dir = Path("checkpoints")
            save_dir.mkdir(exist_ok=True)
            
            # ä¿å­˜çŠ¶æ€
            state = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                'val_acc': val_acc,
                'config': self.config.to_dict(),
                'scaler': self.scaler
            }
            
            # ä¿å­˜æœ€æ–°æ¨¡å‹
            torch.save(state, save_dir / "latest_model.pth")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if is_best:
                torch.save(state, save_dir / "best_model.pth")
                self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ - Epoch {epoch}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, quick_test: bool = False) -> None:
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
        """
        epochs = 2 if quick_test else self.config.training['epochs']
        
        self.logger.info(f"å¼€å§‹è®­ç»ƒ - æ€»epochæ•°: {epochs}")
        
        for epoch in range(epochs):
            start_time = datetime.now()
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc, val_metrics = self.validate(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            if self.scheduler:
                self.scheduler.step()
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accs.append(train_acc)
            self.val_accs.append(val_acc)
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
            
            # ä¿å­˜æ¨¡å‹
            self.save_model(epoch + 1, val_acc, is_best)
            
            # æ‰“å°ç»“æœ
            duration = datetime.now() - start_time
            self.logger.info(
                f"Epoch {epoch+1}/{epochs} - "
                f"è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}% - "
                f"éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}% - "
                f"ç”¨æ—¶: {duration.total_seconds():.1f}s"
            )
            
            # æ‰“å°è¯¦ç»†æŒ‡æ ‡
            for metric, value in val_metrics.items():
                self.logger.info(f"  {metric}: {value:.4f}")
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šæå‰ç»“æŸ
            if quick_test and epoch >= 0:  # è‡³å°‘è®­ç»ƒ1ä¸ªepoch
                self.logger.info("å¿«é€Ÿæµ‹è¯•æ¨¡å¼å®Œæˆ")
                break
        
        self.logger.info(f"è®­ç»ƒå®Œæˆ - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {self.best_val_acc:.2f}%")
    
    def evaluate(self, test_loader: DataLoader) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        self.logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        try:
            checkpoint = torch.load("checkpoints/best_model.pth", map_location=self.device, weights_only=False)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info("åŠ è½½æœ€ä½³æ¨¡å‹æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹æˆ–åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹")
        
        # è¯„ä¼°
        test_loss, test_acc, test_metrics = self.validate(test_loader)
        
        self.logger.info(f"æµ‹è¯•ç»“æœ - æŸå¤±: {test_loss:.4f}, å‡†ç¡®ç‡: {test_acc:.2f}%")
        for metric, value in test_metrics.items():
            self.logger.info(f"  {metric}: {value:.4f}")
        
        return test_metrics
    
    def save_training_history(self) -> None:
        """ä¿å­˜è®­ç»ƒå†å²"""
        try:
            history = {
                'train_losses': self.train_losses,
                'val_losses': self.val_losses,
                'train_accs': self.train_accs,
                'val_accs': self.val_accs,
                'best_val_acc': self.best_val_acc
            }
            
            with open('training_history.json', 'w') as f:
                json.dump(history, f, indent=2)
            
            self.logger.info("è®­ç»ƒå†å²ä¿å­˜å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è®­ç»ƒå†å²å¤±è´¥: {e}")
    
    def quick_validation(self) -> None:
        """å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯ä»£ç æ­£ç¡®æ€§"""
        print("ğŸš€ å¼€å§‹ training.py å¿«é€ŸéªŒè¯...")
        
        try:
            # 1. åŠ è½½å°‘é‡æ•°æ®
            train_loader, val_loader, test_loader = self.load_data(quick_test=True)
            print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
            
            # 2. æ„å»ºæ¨¡å‹
            self.build_model()
            print("âœ… æ¨¡å‹æ„å»ºæˆåŠŸ")
            
            # 3. å¿«é€Ÿè®­ç»ƒ
            self.train(train_loader, val_loader, quick_test=True)
            print("âœ… è®­ç»ƒæµç¨‹éªŒè¯æˆåŠŸ")
            
            # 4. å¿«é€Ÿè¯„ä¼°
            metrics = self.evaluate(test_loader)
            print("âœ… è¯„ä¼°æµç¨‹éªŒè¯æˆåŠŸ")
            
            # 5. ä¿å­˜å†å²
            self.save_training_history()
            print("âœ… å†å²ä¿å­˜éªŒè¯æˆåŠŸ")
            
            print("ğŸ‰ training.py å¿«é€ŸéªŒè¯å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸è¿è¡Œã€‚")
            
        except Exception as e:
            print(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
            raise


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥è¿è¡Œç¯å¢ƒ...")
    
    try:
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        import sys
        python_version = sys.version_info
        print(f"  Pythonç‰ˆæœ¬: {python_version.major}.{python_version.minor}.{python_version.micro}")
        
        # æ£€æŸ¥condaç¯å¢ƒï¼ˆå®‰å…¨æ£€æŸ¥ï¼Œé¿å…ç¯å¢ƒé”™è¯¯ï¼‰
        try:
            conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')
            conda_prefix = os.environ.get('CONDA_PREFIX', 'None')
            print(f"  Condaç¯å¢ƒ: {conda_env}")
            print(f"  Condaè·¯å¾„: {conda_prefix}")
            
            # å¦‚æœæ£€æµ‹åˆ°factorç¯å¢ƒä½†ä¸å­˜åœ¨ï¼Œç»™å‡ºè­¦å‘Š
            if conda_env == 'factor' and not os.path.exists(conda_prefix):
                print("  âš ï¸ è­¦å‘Š: factorç¯å¢ƒè·¯å¾„ä¸å­˜åœ¨ï¼Œä½†è„šæœ¬å¯ä»¥åœ¨å½“å‰ç¯å¢ƒè¿è¡Œ")
        except Exception as e:
            print(f"  Condaç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
        
        # æ£€æŸ¥å½“å‰å·¥ä½œç›®å½•
        print(f"  å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬
        print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        if torch.cuda.is_available():
            print(f"  CUDAå¯ç”¨: âœ… (è®¾å¤‡æ•°é‡: {torch.cuda.device_count()})")
            for i in range(torch.cuda.device_count()):
                print(f"    GPU {i}: {torch.cuda.get_device_name(i)}")
        else:
            print("  CUDAå¯ç”¨: âŒ (å°†ä½¿ç”¨CPU)")
        
        # æ£€æŸ¥å…³é”®ä¾èµ–
        try:
            import pandas
            print(f"  Pandasç‰ˆæœ¬: {pandas.__version__}")
        except ImportError:
            print("  Pandas: âŒ æœªå®‰è£…")
        
        try:
            import sklearn
            print(f"  Scikit-learnç‰ˆæœ¬: {sklearn.__version__}")
        except ImportError:
            print("  Scikit-learn: âŒ æœªå®‰è£…")
        
        # æ£€æŸ¥æ•°æ®ç›®å½•
        data_paths = ['./data', '../data', '../../data', '../../../data']
        for path in data_paths:
            if os.path.exists(path):
                print(f"  æ•°æ®ç›®å½• {path}: âœ…")
                break
        else:
            print("  æ•°æ®ç›®å½•: âŒ æœªæ‰¾åˆ°")
        
        print("âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ\n")
        
    except Exception as e:
        print(f"âŒ ç¯å¢ƒæ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("ç»§ç»­æ‰§è¡Œè„šæœ¬...\n")


def check_conda_environment_compatibility():
    """æ£€æŸ¥condaç¯å¢ƒå…¼å®¹æ€§ï¼Œå¤„ç†factorç¯å¢ƒä¸å­˜åœ¨çš„é—®é¢˜"""
    try:
        conda_env = os.environ.get('CONDA_DEFAULT_ENV', 'None')
        conda_prefix = os.environ.get('CONDA_PREFIX', 'None')
        
        # å¦‚æœå½“å‰ç¯å¢ƒæ˜¯factorä½†è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯´æ˜ç¯å¢ƒé…ç½®æœ‰é—®é¢˜
        if conda_env == 'factor' and conda_prefix != 'None' and not os.path.exists(conda_prefix):
            print("âš ï¸ æ£€æµ‹åˆ°factorç¯å¢ƒé…ç½®é—®é¢˜ï¼Œä½†è„šæœ¬å¯ä»¥åœ¨å½“å‰Pythonç¯å¢ƒä¸­æ­£å¸¸è¿è¡Œ")
            print(f"   ç¯å¢ƒåç§°: {conda_env}")
            print(f"   ç¯å¢ƒè·¯å¾„: {conda_prefix}")
            print("   è§£å†³æ–¹æ¡ˆ: è„šæœ¬å°†ä½¿ç”¨å½“å‰Pythonç¯å¢ƒç»§ç»­æ‰§è¡Œ")
            return False
        
        return True
        
    except Exception as e:
        print(f"âš ï¸ ç¯å¢ƒå…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
        return True  # ç»§ç»­æ‰§è¡Œ


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥condaç¯å¢ƒå…¼å®¹æ€§
    check_conda_environment_compatibility()
    
    # æ£€æŸ¥è¿è¡Œç¯å¢ƒ
    check_environment()
    
    parser = argparse.ArgumentParser(description='HRMæ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='config.yaml', help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data-config', type=str, default='data.yaml', help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quick-test', action='store_true', help='å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯è®­ç»ƒè„šæœ¬æ­£ç¡®æ€§')
    parser.add_argument('--eval-only', action='store_true', help='ä»…è¯„ä¼°æ¨¡å¼')
    
    args = parser.parse_args()
    
    try:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.config):
            print(f"âš ï¸ è­¦å‘Š: é…ç½®æ–‡ä»¶ {args.config} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
        if not os.path.exists(args.data_config):
            print(f"âš ï¸ è­¦å‘Š: æ•°æ®é…ç½®æ–‡ä»¶ {args.data_config} ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤é…ç½®")
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨
        trainer = HRMTrainer(args.config, args.data_config)
        
        if args.quick_test:
            # å¿«é€ŸéªŒè¯æ¨¡å¼
            trainer.quick_validation()
        else:
            # æ­£å¸¸è®­ç»ƒæ¨¡å¼
            # åŠ è½½æ•°æ®
            train_loader, val_loader, test_loader = trainer.load_data()
            
            # æ„å»ºæ¨¡å‹
            trainer.build_model()
            
            if not args.eval_only:
                # è®­ç»ƒæ¨¡å‹
                trainer.train(train_loader, val_loader)
                
                # ä¿å­˜è®­ç»ƒå†å²
                trainer.save_training_history()
            
            # è¯„ä¼°æ¨¡å‹
            metrics = trainer.evaluate(test_loader)
            
            print("\n" + "="*50)
            print("ğŸ¯ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
            for metric, value in metrics.items():
                print(f"  {metric}: {value:.4f}")
            print("="*50)
    
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        print("ğŸ’¡ æç¤º: å¦‚æœé‡åˆ°condaç¯å¢ƒé—®é¢˜ï¼Œè¯·ç¡®ä¿:")
        print("   1. ä½¿ç”¨æ­£ç¡®çš„Pythonç¯å¢ƒè¿è¡Œè„šæœ¬")
        print("   2. æˆ–è€…ç›´æ¥ä½¿ç”¨ 'python training.py' è€Œä¸æ˜¯ 'conda run -n factor python training.py'")
        raise


if __name__ == "__main__":
    main()