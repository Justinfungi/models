#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
wanglang_20250916_ConvM_Lstm_unified.py - ç»Ÿä¸€æ¨¡å‹å®ç°
æ•´åˆè®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½ï¼Œæ”¯æŒOptunaè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class wanglang_20250916_ConvM_LstmConfig:
    """wanglang_20250916_ConvM_Lstmæ¨¡å‹é…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "wanglang_20250916_ConvM_Lstm"
    model_type: str = "classification"
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 100
    weight_decay: float = 1e-5
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True
    dataloader_num_workers: int = 4
    pin_memory: bool = True
    gradient_clip_value: float = 1.0
    
    # æ¨¡å‹æ¶æ„é…ç½®
    seq_len: int = 60
    num_features: int = 20
    input_dim: Optional[int] = None  # åŠ¨æ€è®¾ç½®
    
    # å·ç§¯åˆ†æ”¯é…ç½®
    conv_num_branches: int = 4
    conv_kernel_sizes: List[List[int]] = None
    conv_out_channels: int = 32
    
    # LSTMé…ç½®
    lstm_hidden_size: int = 128
    lstm_num_layers: int = 1
    lstm_bidirectional: bool = False
    lstm_dropout: float = 0.2
    
    # å…¨è¿æ¥å±‚é…ç½®
    fc_hidden_dim: int = 256
    fc_dropout: float = 0.3
    num_classes: int = 2
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    seed: int = 42
    
    # è·¯å¾„é…ç½®
    data_path: str = "./data"
    checkpoint_dir: str = "./checkpoints"
    results_dir: str = "./results"
    logs_dir: str = "./logs"
    
    def __post_init__(self):
        if self.conv_kernel_sizes is None:
            self.conv_kernel_sizes = [[1, self.num_features], [3, self.num_features], 
                                    [5, self.num_features], [7, self.num_features]]

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    logger = logging.getLogger(__name__)

    if device_choice == "cpu":
        device = torch.device('cpu')
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
        logger.info("è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨CPU")
    elif device_choice == "cuda":
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            logger.info(f"è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨GPU - {gpu_name} ({gpu_memory:.1f}GB)")
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            cached_memory = torch.cuda.memory_reserved() / 1024**3

            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU: {gpu_name}")
            print(f"   ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f}GB æ€»é‡")
            print(f"   ğŸ“Š GPUæ•°é‡: {gpu_count}")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨: {current_memory:.2f}GB")
            print(f"   ğŸ“Š ç¼“å­˜ä½¿ç”¨: {cached_memory:.2f}GB")

            logger.info(f"è®¾å¤‡é…ç½®: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU")
            logger.info(f"GPUä¿¡æ¯: {gpu_name}, {gpu_memory:.1f}GBæ€»å†…å­˜, {gpu_count}ä¸ªGPU")
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {current_memory:.2f}GBå·²ç”¨, {cached_memory:.2f}GBç¼“å­˜")

            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            logger.info("è®¾å¤‡é…ç½®: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    if log_filename:
        log_file = log_dir / log_filename
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{prefix}_{timestamp}.log"
    
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger

def create_directories(config: wanglang_20250916_ConvM_LstmConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    dirs = [config.checkpoint_dir, config.results_dir, config.logs_dir]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

# =============================================================================
# æ•°æ®å¤„ç†ç±»
# =============================================================================

class wanglang_20250916_ConvM_LstmDataLoaderManager:
    """æ•°æ®åŠ è½½å™¨ç®¡ç†ç±»"""
    
    def __init__(self, data_config_path: str, config: wanglang_20250916_ConvM_LstmConfig):
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.data_split_info = self.data_config.get('data_split', {})
        self.scaler = StandardScaler()
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            if 'phase_1' in config:
                phase_1_config = config['phase_1']
                config['data_split'] = {
                    "workflow_type": "time_series",
                    "train": {
                        "start_date": phase_1_config['train_data']['start_date'],
                        "end_date": phase_1_config['train_data']['end_date'],
                        "samples": phase_1_config['train_data']['samples'],
                        "duration_years": phase_1_config['train_data']['duration_years'],
                        "file": phase_1_config['train_data']['file']
                    },
                    "validation": {
                        "start_date": phase_1_config['valid_data']['start_date'],
                        "end_date": phase_1_config['valid_data']['end_date'],
                        "samples": phase_1_config['valid_data']['samples'],
                        "duration_years": phase_1_config['valid_data']['duration_years'],
                        "file": phase_1_config['valid_data']['file']
                    },
                    "test": {
                        "start_date": phase_1_config['test_data']['start_date'],
                        "end_date": phase_1_config['test_data']['end_date'],
                        "samples": phase_1_config['test_data']['samples'],
                        "duration_years": phase_1_config['test_data']['duration_years'],
                        "file": phase_1_config['test_data']['file']
                    }
                }
                print(f"âœ… ä½¿ç”¨Phase 1é…ç½®ä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²")
            
            return config
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path}: {e}")
    
    def get_input_dim_from_dataframe(self, df: pd.DataFrame) -> int:
        """ä»DataFrameè·å–ç‰¹å¾ç»´åº¦"""
        feature_cols = [col for col in df.columns if '@' in col]
        if not feature_cols:
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            feature_cols = numeric_cols[:-1] if len(numeric_cols) > 1 else numeric_cols
        
        features = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {features}")
        return features
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=4,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )
    
    def _load_real_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ä»çœŸå®parquetæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            data_split = self.data_split_info
            
            # å®šä¹‰å¯èƒ½çš„æ•°æ®è·¯å¾„
            possible_data_paths = [
                "/home/feng.hao.jie/deployment/model_explorer/b_model_reproduction_agent/data/feature_set/",
                "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/data/feature_set/",
                "./data/feature_set/",
                "./",
                ""
            ]
            
            def find_data_file(filename):
                """æŸ¥æ‰¾æ•°æ®æ–‡ä»¶çš„å®é™…è·¯å¾„"""
                # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ä¸”å­˜åœ¨ï¼Œç›´æ¥è¿”å›
                if os.path.isabs(filename) and os.path.exists(filename):
                    return filename
                
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ä¸”å­˜åœ¨ï¼Œç›´æ¥è¿”å›
                if os.path.exists(filename):
                    return filename
                
                # åœ¨å¯èƒ½çš„è·¯å¾„ä¸­æœç´¢
                for base_path in possible_data_paths:
                    full_path = os.path.join(base_path, filename)
                    if os.path.exists(full_path):
                        print(f"ğŸ” æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {full_path}")
                        return full_path
                
                # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼ŒæŠ›å‡ºé”™è¯¯
                raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {filename}")
            
            # åŠ è½½è®­ç»ƒæ•°æ®
            train_file = data_split['train']['file']
            train_file_path = find_data_file(train_file)
            train_df = pd.read_parquet(train_file_path)
            
            # åŠ è½½éªŒè¯æ•°æ®
            valid_file = data_split['validation']['file']
            valid_file_path = find_data_file(valid_file)
            valid_df = pd.read_parquet(valid_file_path)
            
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_file = data_split['test']['file']
            test_file_path = find_data_file(test_file)
            test_df = pd.read_parquet(test_file_path)
            
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
            print(f"   è®­ç»ƒé›†: {train_df.shape}")
            print(f"   éªŒè¯é›†: {valid_df.shape}")
            print(f"   æµ‹è¯•é›†: {test_df.shape}")
            
            return train_df, valid_df, test_df
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½çœŸå®æ•°æ®å¤±è´¥: {e}")
    
    def load_data_loaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """åŠ è½½æ•°æ®åŠ è½½å™¨"""
        train_df, valid_df, test_df = self._load_real_dataframes()
        
        # åŠ¨æ€æ£€æµ‹ç‰¹å¾ç»´åº¦
        feature_cols = [col for col in train_df.columns if '@' in col]
        if not feature_cols:
            numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
            feature_cols = numeric_cols[:-1] if len(numeric_cols) > 1 else numeric_cols
        
        input_dim = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ° {input_dim} ä¸ªç‰¹å¾")
        
        # æ›´æ–°é…ç½® - ç¡®ä¿æ‰€æœ‰ç›¸å…³ç»´åº¦éƒ½æ­£ç¡®è®¾ç½®
        self.config.input_dim = input_dim
        self.config.num_features = input_dim
        
        # è°ƒæ•´åºåˆ—é•¿åº¦ä»¥é€‚åº”ç‰¹å¾æ•°é‡
        # å¦‚æœç‰¹å¾æ•°é‡è¾ƒå¤šï¼Œå¯èƒ½éœ€è¦è°ƒæ•´åºåˆ—é•¿åº¦
        if input_dim > 60:
            # å¯¹äºå¤§é‡ç‰¹å¾ï¼Œä½¿ç”¨è¾ƒå°çš„åºåˆ—é•¿åº¦
            self.config.seq_len = max(10, min(30, input_dim // 3))
        else:
            # å¯¹äºè¾ƒå°‘ç‰¹å¾ï¼Œä½¿ç”¨æ ‡å‡†åºåˆ—é•¿åº¦
            self.config.seq_len = min(60, max(10, input_dim))
        
        print(f"ğŸ”§ è°ƒæ•´åºåˆ—é•¿åº¦ä¸º: {self.config.seq_len}")
        print(f"ğŸ”§ è®¾ç½®ç‰¹å¾ç»´åº¦ä¸º: {self.config.num_features}")
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        target_col = 'label'
        if target_col not in train_df.columns:
            target_col = train_df.columns[-1]
        
        # å¤„ç†è®­ç»ƒæ•°æ®
        X_train = train_df[feature_cols].values
        y_train = train_df[target_col].values
        
        # å¤„ç†éªŒè¯æ•°æ®
        X_valid = valid_df[feature_cols].values
        y_valid = valid_df[target_col].values
        
        # å¤„ç†æµ‹è¯•æ•°æ®
        X_test = test_df[feature_cols].values
        y_test = test_df[target_col].values
        
        # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°ç±»å‹
        y_train = y_train.astype(int)
        y_valid = y_valid.astype(int)
        y_test = y_test.astype(int)
        
        # æ•°æ®æ ‡å‡†åŒ–
        X_train = self.scaler.fit_transform(X_train)
        X_valid = self.scaler.transform(X_valid)
        X_test = self.scaler.transform(X_test)
        
        # é‡å¡‘ä¸ºæ—¶é—´åºåˆ—æ ¼å¼
        seq_len = self.config.seq_len
        
        def reshape_to_sequences(X, y):
            # ç¡®ä¿æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„ç‰¹å¾æ¥é‡å¡‘
            available_features = X.shape[1]
            
            # å¦‚æœå¯ç”¨ç‰¹å¾æ•°é‡å°‘äºéœ€è¦çš„ï¼Œè¿›è¡Œå¡«å……
            if available_features < input_dim:
                padding = np.zeros((X.shape[0], input_dim - available_features))
                X = np.concatenate([X, padding], axis=1)
                print(f"âš ï¸ ç‰¹å¾ä¸è¶³ï¼Œä» {available_features} å¡«å……åˆ° {input_dim}")
            elif available_features > input_dim:
                # å¦‚æœç‰¹å¾å¤ªå¤šï¼Œæˆªå–å‰input_dimä¸ª
                X = X[:, :input_dim]
                print(f"âš ï¸ ç‰¹å¾è¿‡å¤šï¼Œä» {available_features} æˆªå–åˆ° {input_dim}")
            
            # ç°åœ¨é‡å¡‘ä¸ºåºåˆ—æ ¼å¼
            # æ¯ä¸ªæ ·æœ¬é‡å¤seq_lenæ¬¡æ¥åˆ›å»ºæ—¶é—´åºåˆ—
            X_expanded = np.tile(X[:, np.newaxis, :], (1, seq_len, 1))
            
            return X_expanded, y
        
        X_train_seq, y_train = reshape_to_sequences(X_train, y_train)
        X_valid_seq, y_valid = reshape_to_sequences(X_valid, y_valid)
        X_test_seq, y_test = reshape_to_sequences(X_test, y_test)
        
        # è½¬æ¢ä¸ºå¼ é‡
        train_dataset = TensorDataset(torch.FloatTensor(X_train_seq), torch.LongTensor(y_train))
        valid_dataset = TensorDataset(torch.FloatTensor(X_valid_seq), torch.LongTensor(y_valid))
        test_dataset = TensorDataset(torch.FloatTensor(X_test_seq), torch.LongTensor(y_test))
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self._create_dataloader(train_dataset, shuffle=True, batch_size=batch_size)
        valid_loader = self._create_dataloader(valid_dataset, shuffle=False, batch_size=batch_size)
        test_loader = self._create_dataloader(test_dataset, shuffle=False, batch_size=batch_size)
        
        return train_loader, valid_loader, test_loader
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_split_info
    
    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç›¸å…³ä¿¡æ¯"""
        return {
            "workflow_type": self.data_split_info.get("workflow_type", "unknown"),
            "train_period": self.data_split_info.get("train", {}),
            "validation_period": self.data_split_info.get("validation", {}),
            "test_period": self.data_split_info.get("test", {}),
            "data_config": self.data_config
        }

# =============================================================================
# æ¨¡å‹å®ç°
# =============================================================================

class wanglang_20250916_ConvM_LstmModel(nn.Module):
    """wanglang_20250916_ConvM_Lstmæ¨¡å‹å®ç°"""
    
    def __init__(self, config: wanglang_20250916_ConvM_LstmConfig):
        super().__init__()
        self.config = config
        
        # åŠ¨æ€è·å–è¾“å…¥ç»´åº¦
        self.input_dim = getattr(config, 'input_dim', None)
        if self.input_dim is None:
            self.input_dim = config.num_features
            print("âš ï¸ é…ç½®ä¸­æœªæŒ‡å®šinput_dimï¼Œä½¿ç”¨num_features")
        
        self.seq_len = config.seq_len
        self.num_features = self.input_dim
        
        # å·ç§¯åˆ†æ”¯é…ç½®
        self.num_branches = config.conv_num_branches
        self.kernel_sizes = config.conv_kernel_sizes
        self.out_channels = config.conv_out_channels
        
        # æ›´æ–°kernel_sizesä»¥åŒ¹é…å®é™…ç‰¹å¾æ•°
        self.kernel_sizes = [[k[0], self.num_features] for k in self.kernel_sizes]
        
        # LSTMé…ç½®
        self.lstm_hidden_size = config.lstm_hidden_size
        self.lstm_num_layers = config.lstm_num_layers
        self.lstm_bidirectional = config.lstm_bidirectional
        self.lstm_dropout = config.lstm_dropout
        
        # å…¨è¿æ¥å±‚é…ç½®
        self.fc_hidden_dim = config.fc_hidden_dim
        self.fc_dropout = config.fc_dropout
        self.num_classes = config.num_classes
        
        # æ„å»ºæ¨¡å‹å±‚
        self._build_conv_branches()
        self._build_lstm()
        self._build_classifier()
        
    def _build_conv_branches(self):
        """æ„å»ºå¤šåˆ†æ”¯å·ç§¯å±‚"""
        self.conv_branches = nn.ModuleList()
        
        for i, kernel_size in enumerate(self.kernel_sizes):
            branch = nn.Sequential(
                nn.Conv2d(
                    in_channels=1,
                    out_channels=self.out_channels,
                    kernel_size=kernel_size,
                    padding=(kernel_size[0]//2, 0)
                ),
                nn.BatchNorm2d(self.out_channels),
                nn.ReLU(inplace=True),
                nn.AdaptiveAvgPool2d((1, 1))
            )
            self.conv_branches.append(branch)
            
    def _build_lstm(self):
        """æ„å»ºLSTMå±‚"""
        lstm_input_dim = self.num_branches * self.out_channels + self.num_features
        
        self.lstm = nn.LSTM(
            input_size=lstm_input_dim,
            hidden_size=self.lstm_hidden_size,
            num_layers=self.lstm_num_layers,
            bidirectional=self.lstm_bidirectional,
            dropout=self.lstm_dropout if self.lstm_num_layers > 1 else 0,
            batch_first=True
        )
        
        self.lstm_output_dim = self.lstm_hidden_size * (2 if self.lstm_bidirectional else 1)
        
    def _build_classifier(self):
        """æ„å»ºåˆ†ç±»å™¨"""
        self.classifier = nn.Sequential(
            nn.Linear(self.lstm_output_dim, self.fc_hidden_dim),
            nn.Tanh(),
            nn.Dropout(self.fc_dropout),
            nn.Linear(self.fc_hidden_dim, self.num_classes)
        )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        batch_size, seq_len, num_features = x.shape
        
        # å‡†å¤‡å·ç§¯è¾“å…¥
        conv_input = x.view(batch_size * seq_len, 1, 1, num_features)
        
        # å¤šåˆ†æ”¯å·ç§¯ç‰¹å¾æå–
        conv_features = []
        for branch in self.conv_branches:
            branch_output = branch(conv_input)
            branch_output = branch_output.squeeze(-1).squeeze(-1)
            conv_features.append(branch_output)
        
        # åˆå¹¶å·ç§¯ç‰¹å¾
        conv_output = torch.cat(conv_features, dim=1)
        conv_output = conv_output.view(batch_size, seq_len, -1)
        
        # åŸå§‹ç‰¹å¾
        original_features = x
        
        # åˆå¹¶å·ç§¯ç‰¹å¾å’ŒåŸå§‹ç‰¹å¾
        lstm_input = torch.cat([conv_output, original_features], dim=2)
        
        # LSTMå¤„ç†
        lstm_output, (hidden, cell) = self.lstm(lstm_input)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        if self.lstm_bidirectional:
            final_hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)
        else:
            final_hidden = hidden[-1]
        
        # åˆ†ç±»
        output = self.classifier(final_hidden)
        
        return output
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024),
            "input_dim": self.input_dim,
            "seq_len": self.seq_len,
            "num_classes": self.num_classes
        }

# =============================================================================
# è®­ç»ƒå™¨ç±»
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config: wanglang_20250916_ConvM_LstmConfig, model: wanglang_20250916_ConvM_LstmModel, 
                 data_loader_manager: wanglang_20250916_ConvM_LstmDataLoaderManager, checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir
        
        # è®¾å¤‡è®¾ç½®
        self.device = setup_device(getattr(config, 'device', 'auto'))
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config.use_mixed_precision and torch.cuda.is_available() else None
        self.use_amp = config.use_mixed_precision and torch.cuda.is_available()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_score = 0.0
        self.train_history = []
        
        # Checkpointè·Ÿè¸ªå™¨
        self.checkpoint_tracker = {
            'global_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'early_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'mid_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'late_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None}
        }
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)
        self._log_device_info()
        
        if self.use_amp:
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_info = self.data_loader_manager.get_data_info()
        self.logger.info(f"æ•°æ®åˆ†å‰²ä¿¡æ¯: {data_info['workflow_type']}")
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡å’ŒGPUä½¿ç”¨ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            
            self.logger.info(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name}")
            self.logger.info(f"ğŸ“Š GPUæ€»å†…å­˜: {gpu_memory:.1f}GB")
            self.logger.info(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {current_memory:.2f}GB")
        else:
            self.logger.info("ğŸ–¥ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    def setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', patience=5, factor=0.5
        )
        
        self.criterion = nn.CrossEntropyLoss()
    
    def _save_checkpoint(self, epoch, val_loss, checkpoint_type):
        """ä¿å­˜checkpoint"""
        old_path = self.checkpoint_tracker[checkpoint_type]['path']
        if old_path and os.path.exists(old_path):
            os.remove(old_path)
        
        checkpoint_name = f"checkpoint_{checkpoint_type}_epoch{epoch:03d}_val{val_loss:.4f}.pt"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
            'checkpoint_type': checkpoint_type
        }, checkpoint_path)
        
        self.checkpoint_tracker[checkpoint_type] = {
            'epoch': epoch,
            'val_loss': val_loss,
            'path': checkpoint_path
        }
        self.logger.info(f"ğŸ’¾ ä¿å­˜{checkpoint_type} checkpoint: epoch {epoch}, val_loss {val_loss:.4f}")
    
    def _save_interval_best(self, checkpoint_type, epoch_range):
        """åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³checkpointä¸ºinterval_best"""
        source_path = self.checkpoint_tracker[checkpoint_type]['path']
        
        if source_path and os.path.exists(source_path):
            interval_name = f"interval_best_{epoch_range}.pt"
            interval_path = os.path.join(self.checkpoint_dir, interval_name)
            
            shutil.copy2(source_path, interval_path)
            
            epoch = self.checkpoint_tracker[checkpoint_type]['epoch']
            val_loss = self.checkpoint_tracker[checkpoint_type]['val_loss']
            
            self.logger.info(f"ğŸ“¦ å¤åˆ¶åŒºé—´æœ€ä½³ [{epoch_range}]: epoch {epoch}, val_loss {val_loss:.4f}")
        else:
            self.logger.warning(f"âš ï¸ åŒºé—´ [{epoch_range}] æ²¡æœ‰æ‰¾åˆ°æœ€ä½³checkpoint")
    
    def save_checkpoint_if_best(self, epoch, val_loss):
        """ä¿å­˜æœ€ä½³checkpoint"""
        # æ›´æ–°å…¨å±€æœ€ä½³
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        # æ›´æ–°åŒºé—´æœ€ä½³
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        # åœ¨åŒºé—´ç»“æŸæ—¶å¤åˆ¶åŒºé—´æœ€ä½³
        if epoch == 29:
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            if self.use_amp:
                with autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                self.scaler.scale(loss).backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.optimizer.step()
            
            total_loss += loss.item()
            
            probs = torch.softmax(output, dim=1)
            preds = torch.argmax(output, dim=1)
            all_probs.extend(probs.detach().cpu().numpy())
            all_preds.extend(preds.detach().cpu().numpy())
            all_targets.extend(target.detach().cpu().numpy())
        
        metrics = {"loss": total_loss / len(train_loader)}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        
        # è®¡ç®—AUC
        if len(np.unique(all_targets)) == 2:
            try:
                all_probs_array = np.array(all_probs)
                metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
            except Exception:
                metrics["auc"] = 0.0
        
        return metrics
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                if self.use_amp:
                    with autocast():
                        output = self.model(data)
                        loss = self.criterion(output, target)
                else:
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                total_loss += loss.item()
                
                probs = torch.softmax(output, dim=1)
                preds = torch.argmax(output, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.detach().cpu().numpy())
                all_targets.extend(target.detach().cpu().numpy())
        
        metrics = {"loss": total_loss / len(val_loader)}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
        
        # è®¡ç®—AUC
        if len(np.unique(all_targets)) == 2:
            try:
                all_probs_array = np.array(all_probs)
                metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
                # æ·»åŠ éªŒè¯é¢„æµ‹æ•°æ®ç”¨äºOptunaä¼˜åŒ–
                metrics["y_true_val"] = all_targets
                metrics["y_prob_val"] = all_probs_array[:, 1].tolist()
            except Exception:
                metrics["auc"] = 0.0
        
        return metrics
    
    def train(self, batch_size: int = None, no_save_model: bool = False) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        batch_size = batch_size or self.config.batch_size
        train_loader, val_loader, test_loader = self.data_loader_manager.load_data_loaders(batch_size=batch_size)
        
        self.setup_training_components()
        
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {self.model.get_model_info()}")
        
        start_time = time.time()
        epochs = self.config.epochs
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # è®°å½•å†å²
            epoch_info = {
                "epoch": epoch + 1,
                "train_loss": train_metrics["loss"],
                "val_loss": val_metrics["loss"],
                "lr": self.optimizer.param_groups[0]['lr'],
                "time": time.time() - epoch_start,
                "train_acc": train_metrics.get("accuracy", 0),
                "val_acc": val_metrics.get("accuracy", 0),
                "train_auc": train_metrics.get("auc", 0),
                "val_auc": val_metrics.get("auc", 0),
                "val_f1": val_metrics.get("f1", 0)
            }
            
            self.train_history.append(epoch_info)
            
            current_score = val_metrics.get("accuracy", 0)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(current_score)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_score > self.best_val_score:
                self.best_val_score = current_score
                if not no_save_model:
                    self.save_checkpoint_if_best(epoch, val_metrics["loss"])
            
            # æ—¥å¿—è¾“å‡º
            log_msg = (
                f"Epoch {epoch+1}/{epochs} - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}, "
                f"Val Acc: {val_metrics.get('accuracy', 0):.4f}"
            )
            
            if 'auc' in val_metrics:
                log_msg += f", Val AUC: {val_metrics['auc']:.4f}"
            
            log_msg += f", Time: {epoch_info['time']:.2f}s"
            self.logger.info(log_msg)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        return {
            "best_val_score": self.best_val_score,
            "total_epochs": len(self.train_history),
            "total_time": total_time,
            "train_history": self.train_history
        }
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        # ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
        results_dir = Path(self.config.results_dir)
        results_dir.mkdir(parents=True, exist_ok=True)
        
        history_path = results_dir / "training_history.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        self.logger.info(f"è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

# =============================================================================
# æ¨ç†å™¨ç±»
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, config: wanglang_20250916_ConvM_LstmConfig, model: wanglang_20250916_ConvM_LstmModel, 
                 data_loader_manager: wanglang_20250916_ConvM_LstmDataLoaderManager):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.device = setup_device()
        self.logger = logging.getLogger(__name__)
        
        self.model.to(self.device)
        self.model.eval()
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        return checkpoint
    
    def predict_batch(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """æ‰¹é‡æ¨ç†"""
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in data_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.detach().cpu().numpy())
        
        return np.array(all_preds), np.array(all_probs)
    
    def evaluate(self) -> Dict[str, float]:
        """æ¨¡å‹è¯„ä¼°"""
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.detach().cpu().numpy())
                all_targets.extend(target.detach().cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        metrics["precision"] = precision_score(all_targets, all_preds, average='weighted')
        metrics["recall"] = recall_score(all_targets, all_preds, average='weighted')
        metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
        
        # AUCæŒ‡æ ‡è®¡ç®—
        if len(np.unique(all_targets)) == 2:
            all_probs_positive = np.array(all_probs)[:, 1]
            metrics["auc"] = roc_auc_score(all_targets, all_probs_positive)
        
        return metrics
    
    def generate_predictions(self, start_date: str = None, end_date: str = None, 
                           output_path: str = None, output_format: str = "parquet") -> pd.DataFrame:
        """ç”Ÿæˆé¢„æµ‹æ•°æ®æ–‡ä»¶ç”¨äºå›æµ‹"""
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in test_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.detach().cpu().numpy())
        
        # ç”Ÿæˆæ—¥æœŸåºåˆ—
        if start_date and end_date:
            date_range = pd.date_range(start=start_date, end=end_date, periods=len(all_preds))
        else:
            date_range = pd.date_range(start='2020-01-01', periods=len(all_preds), freq='D')
        
        # åˆ›å»ºé¢„æµ‹DataFrame
        all_probs_array = np.array(all_probs)
        predictions_df = pd.DataFrame({
            'date': date_range,
            'prediction': all_preds,
            'confidence': all_probs_array[:, 1] if all_probs_array.shape[1] > 1 else all_probs_array[:, 0],
            'probability_class_0': all_probs_array[:, 0] if all_probs_array.shape[1] > 1 else 1 - all_probs_array[:, 0],
            'probability_class_1': all_probs_array[:, 1] if all_probs_array.shape[1] > 1 else all_probs_array[:, 0],
            'model_name': self.config.model_name,
            'timestamp': datetime.now().isoformat()
        })
        
        # ä¿å­˜æ–‡ä»¶
        if output_path:
            if output_format == "parquet":
                predictions_df.to_parquet(output_path, index=False)
            else:
                predictions_df.to_csv(output_path, index=False)
            
            self.logger.info(f"é¢„æµ‹æ–‡ä»¶å·²ä¿å­˜: {output_path}")
        
        return predictions_df

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: wanglang_20250916_ConvM_LstmConfig, model: wanglang_20250916_ConvM_LstmModel):
        self.config = config
        self.model = model
    
    def generate_model_md(self, output_path: str = "MODEL.md"):
        """ç”ŸæˆMODEL.mdæ–‡æ¡£"""
        model_info = self.model.get_model_info()
        
        content = f"""# {self.config.model_name} Model

## æ¨¡å‹æ¦‚è¿°

{self.config.model_name} æ˜¯ä¸€ä¸ªç»“åˆå¤šåˆ†æ”¯å·ç§¯å’ŒLSTMçš„æ—¶é—´åºåˆ—åˆ†ç±»æ¨¡å‹ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- **ä»»åŠ¡ç±»å‹**: {self.config.model_type}
- **æ¨¡å‹å‚æ•°**: {model_info['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°**: {model_info['trainable_parameters']:,}
- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.2f} MB
- **è¾“å…¥ç»´åº¦**: {model_info['input_dim']}
- **åºåˆ—é•¿åº¦**: {model_info['seq_len']}

## æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **å¤šåˆ†æ”¯å·ç§¯å±‚**: ä½¿ç”¨ä¸åŒkernel sizeæå–å¤šå°ºåº¦ç‰¹å¾
2. **LSTMå±‚**: æ•è·æ—¶é—´åºåˆ—çš„é•¿æœŸä¾èµ–å…³ç³»
3. **å…¨è¿æ¥å±‚**: è¿›è¡Œæœ€ç»ˆåˆ†ç±»

### ç½‘ç»œç»“æ„

- å·ç§¯åˆ†æ”¯æ•°: {self.config.conv_num_branches}
- LSTMéšè—å±‚å¤§å°: {self.config.lstm_hidden_size}
- åˆ†ç±»å™¨éšè—å±‚: {self.config.fc_hidden_dim}

## é…ç½®å‚æ•°

### è®­ç»ƒé…ç½®
- å­¦ä¹ ç‡: {self.config.learning_rate}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- è®­ç»ƒè½®æ•°: {self.config.epochs}
- æƒé‡è¡°å‡: {self.config.weight_decay}

### æ•°æ®é…ç½®
- åºåˆ—é•¿åº¦: {self.config.seq_len}
- ç‰¹å¾æ•°é‡: {self.config.num_features}

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```bash
python wanglang_20250916_ConvM_Lstm_unified.py train --config config.yaml --data-config data.yaml
```

### æ¨¡å‹æ¨ç†

```bash
python wanglang_20250916_ConvM_Lstm_unified.py inference --checkpoint best_model.pth
```

### ç”Ÿæˆæ–‡æ¡£

```bash
python wanglang_20250916_ConvM_Lstm_unified.py docs
```

## æ›´æ–°æ—¥å¿—

- åˆå§‹ç‰ˆæœ¬: {datetime.now().strftime('%Y-%m-%d')}

"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¨¡å‹æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")

# =============================================================================
# ä¸»è¦æ¥å£å‡½æ•°
# =============================================================================

def apply_optuna_config(config: wanglang_20250916_ConvM_LstmConfig, optuna_config_path: str):
    """åº”ç”¨Optunaé…ç½®æ–‡ä»¶ä¸­çš„è¶…å‚æ•°"""
    if not optuna_config_path or not os.path.exists(optuna_config_path):
        return config
    
    try:
        with open(optuna_config_path, 'r', encoding='utf-8') as f:
            optuna_config = yaml.safe_load(f)
        
        # åº”ç”¨è¶…å‚æ•°è¦†ç›–
        if 'hyperparameters' in optuna_config:
            hyperparams = optuna_config['hyperparameters']
            for key, value in hyperparams.items():
                if hasattr(config, key):
                    setattr(config, key, value)
                    print(f"ğŸ”§ åº”ç”¨Optunaè¶…å‚æ•°: {key} = {value}")
                else:
                    print(f"âš ï¸ å¿½ç•¥æ— æ•ˆçš„Optunaè¶…å‚æ•°: {key} = {value}")
        
        return config
    except Exception as e:
        print(f"âš ï¸ åº”ç”¨Optunaé…ç½®å¤±è´¥: {e}")
        return config

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               optuna_config_path: str = None, device: str = "auto",
               epochs_override: int = None, no_save_model: bool = False, seed: int = 42,
               checkpoint_dir: str = "checkpoints"):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    # è®¾ç½®å›ºå®šçš„æ—¥å¿—è·¯å¾„
    log_dir = "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/training/logs/wanglang_20250916_ConvM_Lstm_2609327506"
    logger = setup_logging(log_dir=log_dir, log_filename="log.txt")
    
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    logger.info(f"ğŸ’¾ Checkpointä¿å­˜ç›®å½•: {checkpoint_dir}")
    
    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except:
        config_dict = {}
    
    # è¿‡æ»¤æ‰é…ç½®ç±»ä¸­ä¸å­˜åœ¨çš„å‚æ•°
    import inspect
    valid_params = set(inspect.signature(wanglang_20250916_ConvM_LstmConfig.__init__).parameters.keys())
    valid_params.discard('self')  # ç§»é™¤selfå‚æ•°
    
    filtered_config_dict = {k: v for k, v in config_dict.items() if k in valid_params}
    if len(filtered_config_dict) != len(config_dict):
        ignored_keys = set(config_dict.keys()) - set(filtered_config_dict.keys())
        logger.info(f"âš ï¸ å¿½ç•¥é…ç½®æ–‡ä»¶ä¸­çš„æ— æ•ˆå‚æ•°: {ignored_keys}")
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = wanglang_20250916_ConvM_LstmConfig(**filtered_config_dict)
    
    # åº”ç”¨Optunaé…ç½®
    config = apply_optuna_config(config, optuna_config_path)
    
    # è¦†ç›–é…ç½®
    if device != "auto":
        config.device = device
    if epochs_override:
        config.epochs = epochs_override
    
    config.checkpoint_dir = checkpoint_dir
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = wanglang_20250916_ConvM_LstmDataLoaderManager(data_config_path, config)
    
    # é¢„åŠ è½½æ•°æ®ä»¥è·å–æ­£ç¡®çš„ç»´åº¦ä¿¡æ¯
    try:
        train_loader, _, _ = data_loader_manager.load_data_loaders(batch_size=config.batch_size)
        logger.info(f"âœ… æ•°æ®é¢„åŠ è½½å®Œæˆï¼Œé…ç½®å·²æ›´æ–°")
        logger.info(f"ğŸ“Š æœ€ç»ˆé…ç½® - ç‰¹å¾ç»´åº¦: {config.input_dim}, åºåˆ—é•¿åº¦: {config.seq_len}")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é¢„åŠ è½½å¤±è´¥: {e}")
        raise
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆç°åœ¨é…ç½®å·²ç»æ˜¯æ­£ç¡®çš„ï¼‰
    model = wanglang_20250916_ConvM_LstmModel(config)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UnifiedTrainer(config, model, data_loader_manager, checkpoint_dir=checkpoint_dir)
    
    # æ‰§è¡Œè®­ç»ƒ
    results = trainer.train(no_save_model=no_save_model)
    
    # è¾“å‡ºJSONç»“æœ
    final_results = {
        "final_results": {
            "best_val_score": results['best_val_score'],
            "total_epochs": results['total_epochs'],
            "total_time": results['total_time'],
            "final_train_acc": results['train_history'][-1].get('train_acc', 0),
            "final_val_acc": results['train_history'][-1].get('val_acc', 0),
            "train_auc": results['train_history'][-1].get('train_auc', 0),
            "val_auc": results['train_history'][-1].get('val_auc', 0),
            "train_losses": [epoch['train_loss'] for epoch in results['train_history']],
            "val_losses": [epoch['val_loss'] for epoch in results['train_history']],
            "train_accuracies": [epoch.get('train_acc', 0) for epoch in results['train_history']],
            "val_accuracies": [epoch.get('val_acc', 0) for epoch in results['train_history']]
        }
    }
    print(json.dumps(final_results, indent=2))

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "best_model.pth", mode: str = "eval",
                  start_date: str = None, end_date: str = None, 
                  output_path: str = None, output_format: str = "parquet"):
    """ä¸»æ¨ç†å‡½æ•°"""
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except:
        config_dict = {}
    
    # è¿‡æ»¤æ‰é…ç½®ç±»ä¸­ä¸å­˜åœ¨çš„å‚æ•°
    import inspect
    valid_params = set(inspect.signature(wanglang_20250916_ConvM_LstmConfig.__init__).parameters.keys())
    valid_params.discard('self')  # ç§»é™¤selfå‚æ•°
    
    filtered_config_dict = {k: v for k, v in config_dict.items() if k in valid_params}
    
    config = wanglang_20250916_ConvM_LstmConfig(**filtered_config_dict)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = wanglang_20250916_ConvM_LstmDataLoaderManager(data_config_path, config)
    
    # åˆ›å»ºæ¨¡å‹
    model = wanglang_20250916_ConvM_LstmModel(config)
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = UnifiedInferencer(config, model, data_loader_manager)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if os.path.exists(checkpoint_path):
        checkpoint = inferencer.load_checkpoint(checkpoint_path)
    
    if mode == "eval":
        # ä¼ ç»Ÿæ¨¡å‹è¯„ä¼°
        metrics = inferencer.evaluate()
        print(f"è¯„ä¼°ç»“æœ: {metrics}")
    elif mode == "test":
        # ç”Ÿæˆé¢„æµ‹æ•°æ®æ–‡ä»¶
        predictions_df = inferencer.generate_predictions(
            start_date=start_date, 
            end_date=end_date, 
            output_path=output_path, 
            output_format=output_format
        )
        print(f"ç”Ÿæˆé¢„æµ‹æ–‡ä»¶å®Œæˆï¼ŒåŒ…å« {len(predictions_df)} æ¡è®°å½•")

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ä¸»æ–‡æ¡£ç”Ÿæˆå‡½æ•°"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except:
        config_dict = {}
    
    # è¿‡æ»¤æ‰é…ç½®ç±»ä¸­ä¸å­˜åœ¨çš„å‚æ•°
    import inspect
    valid_params = set(inspect.signature(wanglang_20250916_ConvM_LstmConfig.__init__).parameters.keys())
    valid_params.discard('self')  # ç§»é™¤selfå‚æ•°
    
    filtered_config_dict = {k: v for k, v in config_dict.items() if k in valid_params}
    
    config = wanglang_20250916_ConvM_LstmConfig(**filtered_config_dict)
    
    # åˆ›å»ºæ¨¡å‹
    model = wanglang_20250916_ConvM_LstmModel(config)
    
    # ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config, model)
    doc_generator.generate_model_md("MODEL.md")

# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="wanglang_20250916_ConvM_Lstmç»Ÿä¸€æ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·")
    parser.add_argument("mode", choices=["train", "inference", "docs"], help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="config.yaml", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-config", default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default="best_model.pth", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # Optunaä¼˜åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--optuna-config", help="Optunaè¯•éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--no-save-model", action="store_true", help="ä¸ä¿å­˜æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°æ€§ï¼‰")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    
    # æ¨ç†ç›¸å…³å‚æ•°
    parser.add_argument("--inference-mode", choices=["test", "eval"], default="eval", 
                       help="æ¨ç†æ¨¡å¼ï¼štest(ç”Ÿæˆé¢„æµ‹æ–‡ä»¶) æˆ– eval(è¯„ä¼°)")
    parser.add_argument("--start-date", help="æ¨ç†å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end-date", help="æ¨ç†ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--format", choices=["parquet", "csv"], default="parquet", 
                       help="è¾“å‡ºæ ¼å¼")
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    
    if args.mode == "train":
        main_train(
            config_path=args.config,
            data_config_path=args.data_config,
            optuna_config_path=args.optuna_config,
            device=args.device,
            epochs_override=args.epochs,
            no_save_model=args.no_save_model,
            seed=args.seed,
            checkpoint_dir=args.checkpoint_dir
        )
    elif args.mode == "inference":
        main_inference(
            config_path=args.config,
            data_config_path=args.data_config,
            checkpoint_path=args.checkpoint,
            mode=args.inference_mode,
            start_date=args.start_date,
            end_date=args.end_date,
            output_path=args.output,
            output_format=args.format
        )
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data_config)