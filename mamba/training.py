#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
training.py - Mambaæ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„é«˜æ•ˆåºåˆ—å¤„ç†æ¶æ„ï¼Œé€‚é…äºŒå…ƒåˆ†ç±»ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹
"""

import os
import sys
import argparse
import logging
import json
import time
import random
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List, Union

import yaml
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import joblib

# å¯¼å…¥æ¨¡å‹
try:
    from model import create_model, MambaConfig
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥model.pyï¼Œå°†ä½¿ç”¨ç®€åŒ–æ¨¡å‹")
    create_model = None
    MambaConfig = None

warnings.filterwarnings('ignore')

class MambaTrainer:
    """Mambaæ¨¡å‹è®­ç»ƒå™¨ - æ ¸å¿ƒè®­ç»ƒç®¡ç†ç±»"""
    
    def __init__(self, config_path: str = "config.yaml", data_config_path: str = "data.yaml", quick_test: bool = False):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ŒåŠ è½½é…ç½®ï¼Œè®¾ç½®ç¯å¢ƒ
        
        Args:
            config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            data_config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
        """
        self.quick_test = quick_test
        
        # ç¡¬ç¼–ç ç¯å¢ƒè®¾ç½® - å…ˆè®¾ç½®æ—¥å¿—ç³»ç»Ÿ
        self.setup_device()
        self.setup_logging()
        self.set_seed()
        
        # ç„¶ååŠ è½½é…ç½®
        self.config = self.load_config(config_path)
        self.data_config = self.load_config(data_config_path)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        self.scaler = None
        self.best_val_auc = 0.0
        self.patience_counter = 0
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.create_directories()
        
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """
        åŠ è½½YAMLé…ç½®æ–‡ä»¶
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            é…ç½®å­—å…¸
        """
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            self.logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            return config
        except FileNotFoundError:
            self.logger.error(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}")
            # è¿”å›é»˜è®¤é…ç½®
            if 'config.yaml' in config_path:
                return self.get_default_config()
            else:
                return self.get_default_data_config()
        except Exception as e:
            self.logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
            
    def get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ¨¡å‹é…ç½®"""
        return {
            'architecture': {
                'd_model': 256,
                'd_state': 16,
                'd_conv': 4,
                'expand': 2,
                'n_layer': 8,
                'dropout': 0.1
            },
            'training': {
                'optimizer': 'adamw',
                'learning_rate': 0.0003,
                'weight_decay': 1e-5,
                'epochs': 100,
                'batch_size': 32,
                'patience': 10,
                'gradient_clip_value': 1.0,
                'scheduler': 'cosine_annealing_warm_restarts',
                'scheduler_params': {
                    'T_0': 10,
                    'T_mult': 2,
                    'eta_min': 1e-5
                }
            },
            'inference': {
                'batch_size': 64,
                'max_length': 512
            }
        }
        
    def get_default_data_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ•°æ®é…ç½®"""
        return {
            'data_format': {
                'input_type': 'tabular',
                'file_format': 'parquet',
                'feature_identifier': '@',
                'label_position': 'last'
            },
            'task': {
                'type': 'binary_classification',
                'num_features': None,
                'num_classes': None
            },
            'data_paths': {
                'data_folder': '/home/feng.hao.jie/deployment/model_explorer/b_model_reproduction_agent/data/feature_set',
                'data_phase': 1,
                'data_file': 'mrmr_task_1_2013-01-01_2018-06-30.pq'
            },
            'preprocessing': {
                'feature_selection': {
                    'method': 'automatic'
                },
                'scaling': {
                    'method': 'standard'
                },
                'split': {
                    'method': 'time_series',
                    'train_duration': '1_year',
                    'test_duration': 'remaining'
                }
            }
        }
    
    def setup_logging(self):
        """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿï¼ˆç¡¬ç¼–ç é…ç½®ï¼‰"""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        self.logger.info("è®­ç»ƒæ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
        # è®°å½•è®¾å¤‡ä¿¡æ¯
        if torch.cuda.is_available():
            self.logger.info(f"ä½¿ç”¨GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
        else:
            self.logger.info("ä½¿ç”¨CPUè®¾å¤‡")
        
        # è®°å½•ç§å­ä¿¡æ¯
        self.logger.info(f"è®¾ç½®éšæœºç§å­: 42")
        
    def setup_device(self):
        """è‡ªåŠ¨è®¾å¤‡æ£€æµ‹å’Œé…ç½®ï¼ˆç¡¬ç¼–ç å¤„ç†ï¼‰"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        if torch.cuda.is_available():
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            # å»¶è¿Ÿæ—¥å¿—è®°å½•åˆ°loggeråˆå§‹åŒ–å
        else:
            # å»¶è¿Ÿæ—¥å¿—è®°å½•åˆ°loggeråˆå§‹åŒ–å
            pass
            
    def set_seed(self, seed: int = 42):
        """è®¾ç½®éšæœºç§å­ï¼ˆç¡¬ç¼–ç å¤„ç†ï¼‰"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # å»¶è¿Ÿæ—¥å¿—è®°å½•åˆ°loggeråˆå§‹åŒ–å
        
    def create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = ['checkpoints', 'results', 'logs']
        for dir_name in directories:
            Path(dir_name).mkdir(exist_ok=True)
            
    def load_data(self, data_path: Optional[str] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        åŠ è½½é¡¹ç›®ç‰¹å®šæ ¼å¼çš„Parquetæ•°æ®
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
            
        Returns:
            ç‰¹å¾æ•°ç»„å’Œæ ‡ç­¾æ•°ç»„çš„å…ƒç»„
        """
        try:
            if data_path is None:
                data_folder = self.data_config['data_paths']['data_folder']
                data_file = self.data_config['data_paths']['data_file']
                data_path = os.path.join(data_folder, data_file)
            
            self.logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(data_path):
                # å°è¯•æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
                data_folder = os.path.dirname(data_path)
                pattern = os.path.basename(data_path)
                
                if '*' in pattern:
                    import glob
                    matching_files = glob.glob(data_path)
                    if matching_files:
                        data_path = matching_files[0]
                        self.logger.info(f"æ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {data_path}")
                    else:
                        raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æ–‡ä»¶: {data_path}")
                else:
                    raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
            
            # åŠ è½½Parquetæ–‡ä»¶
            df = pd.read_parquet(data_path)
            self.logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            
            # è‡ªåŠ¨è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
            feature_identifier = self.data_config['data_format']['feature_identifier']
            feature_columns = [col for col in df.columns if feature_identifier in col]
            
            if not feature_columns:
                self.logger.warning("æœªæ‰¾åˆ°åŒ…å«@ç¬¦å·çš„ç‰¹å¾åˆ—ï¼Œä½¿ç”¨é™¤æœ€åä¸€åˆ—å¤–çš„æ‰€æœ‰åˆ—ä½œä¸ºç‰¹å¾")
                feature_columns = df.columns[:-1].tolist()
            
            # è·å–æ ‡ç­¾åˆ—ï¼ˆæœ€åä¸€åˆ—ï¼‰
            label_column = df.columns[-1]
            
            self.logger.info(f"ç‰¹å¾åˆ—æ•°é‡: {len(feature_columns)}")
            self.logger.info(f"æ ‡ç­¾åˆ—: {label_column}")
            
            # æå–ç‰¹å¾å’Œæ ‡ç­¾
            X = df[feature_columns].values.astype(np.float32)
            y = df[label_column].values.astype(np.int64)
            
            # å¤„ç†ç¼ºå¤±å€¼
            if np.isnan(X).any():
                self.logger.warning("æ£€æµ‹åˆ°ç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‡å€¼å¡«å……")
                X = np.nan_to_num(X, nan=np.nanmean(X))
            
            # æ›´æ–°é…ç½®ä¸­çš„ç‰¹å¾ç»´åº¦å’Œç±»åˆ«æ•°é‡
            self.data_config['task']['num_features'] = X.shape[1]
            self.data_config['task']['num_classes'] = len(np.unique(y))
            
            self.logger.info(f"ç‰¹å¾ç»´åº¦: {X.shape[1]}")
            self.logger.info(f"ç±»åˆ«æ•°é‡: {len(np.unique(y))}")
            self.logger.info(f"æ ·æœ¬æ•°é‡: {X.shape[0]}")
            
            return X, y
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
            
    def prepare_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        å‡†å¤‡æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å™¨ï¼Œå®ç°å›ºå®šæ—¶é—´åˆ†å‰²
        
        Args:
            X: ç‰¹å¾æ•°ç»„
            y: æ ‡ç­¾æ•°ç»„
            
        Returns:
            è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨çš„å…ƒç»„
        """
        try:
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨æå°‘æ•°æ®
            if self.quick_test:
                n_samples = min(100, X.shape[0])
                indices = np.random.choice(X.shape[0], n_samples, replace=False)
                X = X[indices]
                y = y[indices]
                self.logger.info(f"å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {n_samples} ä¸ªæ ·æœ¬")
            
            # æ ‡å‡†åŒ–å¤„ç†
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X)
            
            # ä¿å­˜scaler
            scaler_path = 'checkpoints/scaler.pkl'
            joblib.dump(self.scaler, scaler_path)
            self.logger.info(f"Scalerå·²ä¿å­˜åˆ°: {scaler_path}")
            
            # æ—¶é—´åºåˆ—åˆ†å‰²ï¼ˆç®€åŒ–ç‰ˆï¼šæŒ‰æ¯”ä¾‹åˆ†å‰²ï¼‰
            n_samples = X_scaled.shape[0]
            
            if self.quick_test:
                # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šç®€å•åˆ†å‰²
                train_size = int(0.6 * n_samples)
                val_size = int(0.2 * n_samples)
                
                train_indices = np.arange(train_size)
                val_indices = np.arange(train_size, train_size + val_size)
                test_indices = np.arange(train_size + val_size, n_samples)
            else:
                # æ­£å¸¸æ¨¡å¼ï¼šæ—¶é—´åºåˆ—åˆ†å‰²
                train_size = int(0.6 * n_samples)  # 60%ç”¨äºè®­ç»ƒ
                val_size = int(0.2 * n_samples)    # 20%ç”¨äºéªŒè¯
                
                train_indices = np.arange(train_size)
                val_indices = np.arange(train_size, train_size + val_size)
                test_indices = np.arange(train_size + val_size, n_samples)
            
            # åˆ›å»ºæ•°æ®é›†
            X_train, y_train = X_scaled[train_indices], y[train_indices]
            X_val, y_val = X_scaled[val_indices], y[val_indices]
            X_test, y_test = X_scaled[test_indices], y[test_indices]
            
            self.logger.info(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
            self.logger.info(f"éªŒè¯é›†å¤§å°: {X_val.shape[0]}")
            self.logger.info(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            train_dataset = TensorDataset(
                torch.FloatTensor(X_train),
                torch.LongTensor(y_train)
            )
            val_dataset = TensorDataset(
                torch.FloatTensor(X_val),
                torch.LongTensor(y_val)
            )
            test_dataset = TensorDataset(
                torch.FloatTensor(X_test),
                torch.LongTensor(y_test)
            )
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            batch_size = self.config['training']['batch_size']
            if self.quick_test:
                batch_size = min(16, batch_size)  # å¿«é€Ÿæµ‹è¯•ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡
            
            train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=0
            )
            val_loader = DataLoader(
                val_dataset,
                batch_size=self.config['inference']['batch_size'],
                shuffle=False,
                num_workers=0
            )
            test_loader = DataLoader(
                test_dataset,
                batch_size=self.config['inference']['batch_size'],
                shuffle=False,
                num_workers=0
            )
            
            return train_loader, val_loader, test_loader
            
        except Exception as e:
            self.logger.error(f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            raise
            
    def setup_model(self):
        """è®¾ç½®Mambaæ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°"""
        try:
            # è·å–æ¨¡å‹é…ç½®
            model_config = self.config['architecture'].copy()
            
            # è®¾ç½®è¾“å…¥ç»´åº¦
            input_dim = self.data_config['task']['num_features']
            num_classes = self.data_config['task']['num_classes']
            
            # åˆ›å»ºæ¨¡å‹
            if create_model is not None and MambaConfig is not None:
                # ä½¿ç”¨çœŸå®çš„Mambaæ¨¡å‹
                self.model = create_model(
                    config_path=None,  # ä½¿ç”¨é»˜è®¤é…ç½®
                    num_features=input_dim,
                    num_classes=num_classes
                )
            else:
                # ä½¿ç”¨ç®€åŒ–çš„æ›¿ä»£æ¨¡å‹
                self.model = SimpleMambaModel(
                    input_dim=input_dim,
                    d_model=model_config['d_model'],
                    n_layer=model_config['n_layer'],
                    num_classes=num_classes,
                    dropout=model_config['dropout']
                )
            
            self.model = self.model.to(self.device)
            
            # è®¾ç½®ä¼˜åŒ–å™¨
            optimizer_config = self.config['training']['optimizer']
            if isinstance(optimizer_config, dict):
                optimizer_name = optimizer_config.get('type', 'adamw').lower()
                lr = float(optimizer_config.get('lr', 0.0003))
                weight_decay = float(optimizer_config.get('weight_decay', 1e-5))
            else:
                optimizer_name = optimizer_config.lower()
                lr = float(self.config['training'].get('learning_rate', 0.0003))
                weight_decay = float(self.config['training'].get('weight_decay', 1e-5))
            
            if optimizer_name == 'adamw':
                self.optimizer = optim.AdamW(
                    self.model.parameters(),
                    lr=lr,
                    weight_decay=weight_decay
                )
            elif optimizer_name == 'adam':
                self.optimizer = optim.Adam(
                    self.model.parameters(),
                    lr=lr,
                    weight_decay=weight_decay
                )
            else:
                self.optimizer = optim.SGD(
                    self.model.parameters(),
                    lr=lr,
                    weight_decay=weight_decay,
                    momentum=0.9
                )
            
            # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler_name = self.config['training'].get('scheduler', 'cosine_annealing_warm_restarts')
            if scheduler_name == 'cosine_annealing_warm_restarts':
                scheduler_params = self.config['training'].get('scheduler_params', {})
                self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                    self.optimizer,
                    T_0=scheduler_params.get('T_0', 10),
                    T_mult=scheduler_params.get('T_mult', 2),
                    eta_min=scheduler_params.get('eta_min', 1e-5)
                )
            else:
                self.scheduler = optim.lr_scheduler.StepLR(
                    self.optimizer,
                    step_size=30,
                    gamma=0.1
                )
            
            # è®¾ç½®æŸå¤±å‡½æ•°
            self.criterion = nn.CrossEntropyLoss()
            
            # è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            
            self.logger.info(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")
            self.logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            raise
            
    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float, float]:
        """
        å•ä¸ªepochçš„è®­ç»ƒè¿‡ç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡æŸå¤±ã€AUCã€å‡†ç¡®ç‡çš„å…ƒç»„
        """
        self.model.train()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            gradient_clipping = self.config['training'].get('gradient_clipping', {})
            if gradient_clipping.get('enabled', True):
                clip_value = gradient_clipping.get('max_norm', 1.0)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_value)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            probabilities = torch.softmax(output, dim=1)
            predictions = probabilities[:, 1].detach().cpu().numpy()  # æ­£ç±»æ¦‚ç‡
            labels = target.detach().cpu().numpy()
            
            all_predictions.extend(predictions)
            all_labels.extend(labels)
            
            if batch_idx % 10 == 0:
                self.logger.debug(f'è®­ç»ƒæ‰¹æ¬¡ {batch_idx}/{len(train_loader)}, æŸå¤±: {loss.item():.6f}')
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(train_loader)
        
        try:
            auc = roc_auc_score(all_labels, all_predictions)
        except ValueError:
            auc = 0.0
            
        predicted_classes = (np.array(all_predictions) > 0.5).astype(int)
        accuracy = accuracy_score(all_labels, predicted_classes)
        
        return avg_loss, auc, accuracy
        
    def validate(self, val_loader: DataLoader) -> Tuple[float, float, float, Dict[str, float]]:
        """
        æ¨¡å‹éªŒè¯å’ŒæŒ‡æ ‡è®¡ç®—
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            
        Returns:
            å¹³å‡æŸå¤±ã€AUCã€å‡†ç¡®ç‡å’Œè¯¦ç»†æŒ‡æ ‡å­—å…¸çš„å…ƒç»„
        """
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                
                output = self.model(data)
                loss = self.criterion(output, target)
                total_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                probabilities = torch.softmax(output, dim=1)
                predictions = probabilities[:, 1].detach().cpu().numpy()
                labels = target.detach().cpu().numpy()
                
                all_predictions.extend(predictions)
                all_labels.extend(labels)
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(val_loader)
        
        try:
            auc = roc_auc_score(all_labels, all_predictions)
        except ValueError:
            auc = 0.0
            
        predicted_classes = (np.array(all_predictions) > 0.5).astype(int)
        accuracy = accuracy_score(all_labels, predicted_classes)
        
        # è¯¦ç»†æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy,
            'auc': auc,
            'precision': precision_score(all_labels, predicted_classes, average='binary', zero_division=0),
            'recall': recall_score(all_labels, predicted_classes, average='binary', zero_division=0),
            'f1': f1_score(all_labels, predicted_classes, average='binary', zero_division=0)
        }
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, predicted_classes)
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            metrics.update({
                'true_negative': int(tn),
                'false_positive': int(fp),
                'false_negative': int(fn),
                'true_positive': int(tp)
            })
        
        return avg_loss, auc, accuracy, metrics
        
    def train(self, train_loader: DataLoader, val_loader: DataLoader):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æ—©åœå’Œæ£€æŸ¥ç‚¹ä¿å­˜
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        """
        epochs = self.config['training'].get('max_epochs', self.config['training'].get('epochs', 100))
        patience = self.config['training'].get('early_stopping', {}).get('patience', 
                   self.config['training'].get('patience', 10))
        
        if self.quick_test:
            epochs = min(2, epochs)  # å¿«é€Ÿæµ‹è¯•åªè¿è¡Œ2ä¸ªepoch
            patience = 1
        
        self.logger.info(f"å¼€å§‹è®­ç»ƒï¼Œæ€»epochæ•°: {epochs}")
        
        for epoch in range(epochs):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_loss, train_auc, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_auc, val_acc, val_metrics = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
            
            epoch_time = time.time() - start_time
            
            # è®°å½•æ—¥å¿—
            self.logger.info(
                f"Epoch {epoch+1}/{epochs} - "
                f"è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒAUC: {train_auc:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f} - "
                f"éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯AUC: {val_auc:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.4f} - "
                f"æ—¶é—´: {epoch_time:.2f}s"
            )
            
            # æ—©åœæ£€æŸ¥
            if val_auc > self.best_val_auc:
                self.best_val_auc = val_auc
                self.patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_checkpoint(epoch, val_metrics)
                self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯AUC: {val_auc:.4f}")
            else:
                self.patience_counter += 1
                
            if self.patience_counter >= patience:
                self.logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                break
                
        self.logger.info("è®­ç»ƒå®Œæˆ")
        
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            epoch: å½“å‰epoch
            metrics: éªŒè¯æŒ‡æ ‡
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'best_val_auc': self.best_val_auc,
            'metrics': metrics,
            'config': self.config,
            'data_config': self.data_config
        }
        
        checkpoint_path = 'checkpoints/best_model.pth'
        torch.save(checkpoint, checkpoint_path)
        self.logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {checkpoint_path}")
        
    def test(self, test_loader: DataLoader) -> Dict[str, float]:
        """
        æœ€ç»ˆæµ‹è¯•è¯„ä¼°
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            
        Returns:
            æµ‹è¯•æŒ‡æ ‡å­—å…¸
        """
        # åŠ è½½æœ€ä½³æ¨¡å‹
        try:
            checkpoint_path = 'checkpoints/best_model.pth'
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info("åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•")
        except FileNotFoundError:
            self.logger.warning("æœªæ‰¾åˆ°ä¿å­˜çš„æ¨¡å‹ï¼Œä½¿ç”¨å½“å‰æ¨¡å‹è¿›è¡Œæµ‹è¯•")
        
        # æµ‹è¯•è¯„ä¼°
        test_loss, test_auc, test_acc, test_metrics = self.validate(test_loader)
        
        self.logger.info(f"æµ‹è¯•ç»“æœ - æŸå¤±: {test_loss:.4f}, AUC: {test_auc:.4f}, å‡†ç¡®ç‡: {test_acc:.4f}")
        self.logger.info(f"è¯¦ç»†æŒ‡æ ‡: {test_metrics}")
        
        return test_metrics
        
    def save_results(self, test_metrics: Dict[str, float]):
        """
        ä¿å­˜å®éªŒç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            test_metrics: æµ‹è¯•æŒ‡æ ‡
        """
        results = {
            'timestamp': datetime.now().isoformat(),
            'config': self.config,
            'data_config': self.data_config,
            'test_metrics': test_metrics,
            'best_val_auc': self.best_val_auc,
            'quick_test': self.quick_test
        }
        
        results_path = 'results/training_results.json'
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_path}")
        
    def quick_validation(self):
        """å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯ä»£ç æ­£ç¡®æ€§"""
        print("ğŸš€ å¼€å§‹ training.py å¿«é€ŸéªŒè¯...")
        
        try:
            # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
            print("ğŸ“Š ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®...")
            n_samples = 100
            n_features = 50
            X = np.random.randn(n_samples, n_features).astype(np.float32)
            y = np.random.randint(0, 2, n_samples).astype(np.int64)
            
            # æ›´æ–°é…ç½®
            self.data_config['task']['num_features'] = n_features
            self.data_config['task']['num_classes'] = 2
            
            print(f"âœ… æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå®Œæˆ: {X.shape}, {y.shape}")
            
            # 2. å‡†å¤‡æ•°æ®
            print("ğŸ”„ å‡†å¤‡æ•°æ®åŠ è½½å™¨...")
            train_loader, val_loader, test_loader = self.prepare_data(X, y)
            print("âœ… æ•°æ®åŠ è½½å™¨å‡†å¤‡å®Œæˆ")
            
            # 3. è®¾ç½®æ¨¡å‹
            print("ğŸ—ï¸ è®¾ç½®æ¨¡å‹...")
            self.setup_model()
            print("âœ… æ¨¡å‹è®¾ç½®å®Œæˆ")
            
            # 4. å¿«é€Ÿè®­ç»ƒéªŒè¯
            print("ğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒéªŒè¯...")
            self.train(train_loader, val_loader)
            print("âœ… è®­ç»ƒéªŒè¯å®Œæˆ")
            
            # 5. æµ‹è¯•éªŒè¯
            print("ğŸ§ª å¼€å§‹æµ‹è¯•éªŒè¯...")
            test_metrics = self.test(test_loader)
            print("âœ… æµ‹è¯•éªŒè¯å®Œæˆ")
            
            # 6. ä¿å­˜ç»“æœ
            print("ğŸ’¾ ä¿å­˜ç»“æœ...")
            self.save_results(test_metrics)
            print("âœ… ç»“æœä¿å­˜å®Œæˆ")
            
            print("ğŸ‰ training.py å¿«é€ŸéªŒè¯æˆåŠŸå®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


class SimpleMambaModel(nn.Module):
    """ç®€åŒ–çš„Mambaæ¨¡å‹æ›¿ä»£å®ç°"""
    
    def __init__(self, input_dim: int, d_model: int, n_layer: int, num_classes: int, dropout: float = 0.1):
        super().__init__()
        
        self.input_projection = nn.Linear(input_dim, d_model)
        
        # ç®€åŒ–çš„Transformer-likeå±‚
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=d_model,
                nhead=8,
                dim_feedforward=d_model * 4,
                dropout=dropout,
                batch_first=True
            )
            for _ in range(n_layer)
        ])
        
        self.norm = nn.LayerNorm(d_model)
        self.classifier = nn.Linear(d_model, num_classes)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # x shape: (batch_size, seq_len) or (batch_size, input_dim)
        if x.dim() == 2:
            x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        
        # æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦
        x = self.input_projection(x)
        x = self.dropout(x)
        
        # é€šè¿‡å±‚
        for layer in self.layers:
            x = layer(x)
        
        # å½’ä¸€åŒ–
        x = self.norm(x)
        
        # æ± åŒ–ï¼ˆå–å¹³å‡ï¼‰
        x = x.mean(dim=1)
        
        # åˆ†ç±»
        output = self.classifier(x)
        
        return output


def main():
    """ä¸»å‡½æ•° - å®Œæ•´è®­ç»ƒæµç¨‹æ‰§è¡Œ"""
    parser = argparse.ArgumentParser(description='Mambaæ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data-config', type=str, default='data.yaml',
                       help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data-path', type=str, default=None,
                       help='æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--quick-test', action='store_true',
                       help='å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯è®­ç»ƒè„šæœ¬æ­£ç¡®æ€§')
    
    args = parser.parse_args()
    
    try:
        input(args.quick_test)
        # 1. åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
        trainer = MambaTrainer(
            config_path=args.config,
            data_config_path=args.data_config,
            quick_test=args.quick_test
        )
        
        # å¿«é€ŸéªŒè¯æ¨¡å¼
        if args.quick_test:
            success = trainer.quick_validation()
            if success:
                print("âœ… å¿«é€ŸéªŒè¯æˆåŠŸï¼ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œã€‚")
                return 0
            else:
                print("âŒ å¿«é€ŸéªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥ä»£ç ã€‚")
                return 1
        
        # æ­£å¸¸è®­ç»ƒæ¨¡å¼
        print("ğŸš€ å¼€å§‹æ­£å¸¸è®­ç»ƒæ¨¡å¼...")
        
        # 2. åŠ è½½å’Œå‡†å¤‡æ•°æ®
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        X, y = trainer.load_data(args.data_path)
        
        print("ğŸ”„ å‡†å¤‡æ•°æ®...")
        train_loader, val_loader, test_loader = trainer.prepare_data(X, y)
        
        # 3. è®¾ç½®æ¨¡å‹
        print("ğŸ—ï¸ è®¾ç½®æ¨¡å‹...")
        trainer.setup_model()
        
        # 4. æ‰§è¡Œè®­ç»ƒ
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        trainer.train(train_loader, val_loader)
        
        # 5. è¿›è¡Œæµ‹è¯•
        print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
        test_metrics = trainer.test(test_loader)
        
        # 6. ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜ç»“æœ...")
        trainer.save_results(test_metrics)
        
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ç»ˆæµ‹è¯•ç»“æœ: {test_metrics}")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return 1
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)