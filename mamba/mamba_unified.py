#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
mamba_unified.py - ç»Ÿä¸€Mambaæ¨¡å‹å®ç°
åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹çš„é«˜æ•ˆåºåˆ—å¤„ç†æ¶æ„ï¼Œæ”¯æŒOptunaè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import joblib

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class MambaConfig:
    """Mambaæ¨¡å‹é…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "mamba"
    model_type: str = "classification"
    
    # Mambaç‰¹æœ‰é…ç½®
    d_model: int = 256
    n_layer: int = 8
    d_state: int = 16
    d_conv: int = 4
    expand: int = 2
    dt_rank: Union[int, str] = "auto"
    
    # é€‰æ‹©æ€§æ‰«æé…ç½®
    conv_bias: bool = True
    bias: bool = False
    use_fast_path: bool = True
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.0003
    batch_size: int = 32
    epochs: int = 100
    weight_decay: float = 1e-5
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True
    dataloader_num_workers: int = 4
    pin_memory: bool = True
    gradient_clip_value: float = 1.0
    
    # æ­£åˆ™åŒ–
    dropout: float = 0.1
    layer_norm_epsilon: float = 1e-5
    
    # åˆå§‹åŒ–é…ç½®
    initializer_range: float = 0.02
    rescale_prenorm_residual: bool = True
    
    # ä»»åŠ¡é…ç½®
    num_classes: int = 2
    input_dim: Optional[int] = None
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    seed: int = 42
    
    # è·¯å¾„é…ç½®
    data_path: str = "./data"
    checkpoint_dir: str = "./checkpoints"
    results_dir: str = "./results"
    logs_dir: str = "./logs"
    
    def __post_init__(self):
        """é…ç½®åå¤„ç†"""
        if self.dt_rank == "auto":
            self.dt_rank = max(1, self.d_model // 16)
        
        # è®¡ç®—å†…éƒ¨ç»´åº¦
        self.d_inner = int(self.expand * self.d_model)

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    import logging
    logger = logging.getLogger(__name__)

    if device_choice == "cpu":
        device = torch.device('cpu')
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
        logger.info("è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨CPU")
    elif device_choice == "cuda":
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            logger.info(f"è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨GPU - {gpu_name} ({gpu_memory:.1f}GB)")
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            cached_memory = torch.cuda.memory_reserved() / 1024**3

            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU: {gpu_name}")
            print(f"   ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f}GB æ€»é‡")
            print(f"   ğŸ“Š GPUæ•°é‡: {gpu_count}")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨: {current_memory:.2f}GB")
            print(f"   ğŸ“Š ç¼“å­˜ä½¿ç”¨: {cached_memory:.2f}GB")

            logger.info(f"è®¾å¤‡é…ç½®: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU")
            logger.info(f"GPUä¿¡æ¯: {gpu_name}, {gpu_memory:.1f}GBæ€»å†…å­˜, {gpu_count}ä¸ªGPU")
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {current_memory:.2f}GBå·²ç”¨, {cached_memory:.2f}GBç¼“å­˜")

            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            logger.info("è®¾å¤‡é…ç½®: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    if log_filename:
        log_file = log_dir / log_filename
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{prefix}_{timestamp}.log"
    
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger

def create_directories(config: MambaConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    dirs = [config.checkpoint_dir, config.results_dir, config.logs_dir]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

# =============================================================================
# Mambaæ¨¡å‹æ ¸å¿ƒç»„ä»¶
# =============================================================================

class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    
    def __init__(self, d_model: int, eps: float = 1e-5):
        super().__init__()
        self.eps = float(eps) if eps is not None else 1e-5
        self.weight = nn.Parameter(torch.ones(d_model))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return output * self.weight

class MambaBlock(nn.Module):
    """Mambaæ ¸å¿ƒå—ï¼šå®ç°é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹"""
    
    def __init__(self, config: MambaConfig):
        super().__init__()
        self.config = config
        
        # è¾“å…¥æŠ•å½±
        self.in_proj = nn.Linear(config.d_model, config.d_inner * 2, bias=config.bias)
        
        # 1Då·ç§¯å±‚
        self.conv1d = nn.Conv1d(
            in_channels=config.d_inner,
            out_channels=config.d_inner,
            kernel_size=config.d_conv,
            bias=config.conv_bias,
            groups=config.d_inner,
            padding=config.d_conv - 1,
        )
        
        # SSMå‚æ•°æŠ•å½±
        self.x_proj = nn.Linear(config.d_inner, config.dt_rank + config.d_state * 2, bias=False)
        self.dt_proj = nn.Linear(config.dt_rank, config.d_inner, bias=True)
        
        # çŠ¶æ€ç©ºé—´å‚æ•°
        A_log = torch.log(torch.arange(1, config.d_state + 1, dtype=torch.float32).repeat(config.d_inner, 1))
        self.A_log = nn.Parameter(A_log)
        self.D = nn.Parameter(torch.ones(config.d_inner))
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(config.d_inner, config.d_model, bias=config.bias)
        
        # Dropout
        self.dropout = nn.Dropout(config.dropout)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        batch_size, seq_len, d_model = x.shape
        
        # è¾“å…¥æŠ•å½±
        xz = self.in_proj(x)
        x, z = xz.chunk(2, dim=-1)
        
        # 1Då·ç§¯
        x = x.transpose(1, 2)
        x = self.conv1d(x)[:, :, :seq_len]
        x = x.transpose(1, 2)
        
        # æ¿€æ´»å‡½æ•°
        x = F.silu(x)
        
        # SSMè®¡ç®—
        y = self.ssm(x)
        
        # é—¨æ§æœºåˆ¶
        y = y * F.silu(z)
        
        # è¾“å‡ºæŠ•å½±
        output = self.out_proj(y)
        output = self.dropout(output)
        
        return output
    
    def ssm(self, x: torch.Tensor) -> torch.Tensor:
        """é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹æ ¸å¿ƒè®¡ç®—"""
        batch_size, seq_len, d_inner = x.shape
        
        # è®¡ç®—delta, B, Cå‚æ•°
        x_dbl = self.x_proj(x)
        delta, B, C = torch.split(x_dbl, [self.config.dt_rank, self.config.d_state, self.config.d_state], dim=-1)
        
        # è®¡ç®—delta
        delta = F.softplus(self.dt_proj(delta))
        
        # è·å–AçŸ©é˜µ
        A = -torch.exp(self.A_log.float())
        
        # é€‰æ‹©æ€§æ‰«æç®—æ³•
        y = self.selective_scan(x, delta, A, B, C, self.D)
        
        return y
    
    def selective_scan(self, u: torch.Tensor, delta: torch.Tensor, A: torch.Tensor, 
                      B: torch.Tensor, C: torch.Tensor, D: torch.Tensor) -> torch.Tensor:
        """é€‰æ‹©æ€§æ‰«æç®—æ³•å®ç°"""
        batch_size, seq_len, d_inner = u.shape
        d_state = A.shape[-1]
        
        # ç¦»æ•£åŒ–
        deltaA = torch.exp(torch.einsum('bld,dn->bldn', delta, A))
        deltaB_u = torch.einsum('bld,bln,bld->bldn', delta, B, u)
        
        # çŠ¶æ€æ›´æ–°
        x = torch.zeros(batch_size, d_inner, d_state, device=u.device, dtype=u.dtype)
        ys = []
        
        for i in range(seq_len):
            x = deltaA[:, i] * x + deltaB_u[:, i]
            y = torch.einsum('bdn,bn->bd', x, C[:, i])
            ys.append(y)
        
        y = torch.stack(ys, dim=1)
        
        # æ·»åŠ è·³è·ƒè¿æ¥
        y = y + u * D
        
        return y

class MambaLayer(nn.Module):
    """å®Œæ•´çš„Mambaå±‚ï¼ŒåŒ…å«æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–"""
    
    def __init__(self, config: MambaConfig):
        super().__init__()
        self.norm = RMSNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.mamba = MambaBlock(config)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x
        x = self.norm(x)
        x = self.mamba(x)
        return residual + x

# =============================================================================
# æ•°æ®å¤„ç†ç±»
# =============================================================================

class MambaDataLoaderManager:
    """Mambaæ•°æ®åŠ è½½å™¨ç®¡ç†ç±»"""
    
    def __init__(self, data_config_path: str, config: MambaConfig):
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.data_split_info = self.data_config.get('data_split', {})
        self.scaler = None
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            if 'phase_1' in config:
                phase_1_config = config['phase_1']
                config['data_split'] = {
                    "workflow_type": "time_series",
                    "train": {
                        "start_date": phase_1_config['train_data']['start_date'],
                        "end_date": phase_1_config['train_data']['end_date'],
                        "samples": phase_1_config['train_data']['samples'],
                        "duration_years": phase_1_config['train_data']['duration_years'],
                        "file": phase_1_config['train_data']['file']
                    },
                    "validation": {
                        "start_date": phase_1_config['valid_data']['start_date'],
                        "end_date": phase_1_config['valid_data']['end_date'],
                        "samples": phase_1_config['valid_data']['samples'],
                        "duration_years": phase_1_config['valid_data']['duration_years'],
                        "file": phase_1_config['valid_data']['file']
                    },
                    "test": {
                        "start_date": phase_1_config['test_data']['start_date'],
                        "end_date": phase_1_config['test_data']['end_date'],
                        "samples": phase_1_config['test_data']['samples'],
                        "duration_years": phase_1_config['test_data']['duration_years'],
                        "file": phase_1_config['test_data']['file']
                    }
                }
                print(f"âœ… ä½¿ç”¨Phase 1é…ç½®ä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²")
            
            return config
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path}: {e}")
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_split_info
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=4,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )
    
    def _load_real_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ä»çœŸå®parquetæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
            train_file = self.data_split_info['train']['file']
            valid_file = self.data_split_info['validation']['file']
            test_file = self.data_split_info['test']['file']
            
            # è·å–æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
            data_folder = self.data_config.get('data_paths', {}).get('data_folder', '')
            
            # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
            if not os.path.isabs(train_file):
                train_file = os.path.join(data_folder, train_file)
            if not os.path.isabs(valid_file):
                valid_file = os.path.join(data_folder, valid_file)
            if not os.path.isabs(test_file):
                test_file = os.path.join(data_folder, test_file)
            
            print(f"ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®: {train_file}")
            train_df = pd.read_parquet(train_file)
            
            print(f"ğŸ“Š åŠ è½½éªŒè¯æ•°æ®: {valid_file}")
            valid_df = pd.read_parquet(valid_file)
            
            print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
            test_df = pd.read_parquet(test_file)
            
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
            print(f"   è®­ç»ƒé›†: {train_df.shape}")
            print(f"   éªŒè¯é›†: {valid_df.shape}")
            print(f"   æµ‹è¯•é›†: {test_df.shape}")
            
            return train_df, valid_df, test_df
            
        except Exception as e:
            raise ValueError(f"åŠ è½½çœŸå®æ•°æ®å¤±è´¥: {e}")
    
    def get_input_dim_from_dataframe(self, df: pd.DataFrame) -> int:
        """ä»DataFrameè·å–ç‰¹å¾ç»´åº¦"""
        # è‡ªåŠ¨è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
        feature_columns = [col for col in df.columns if '@' in col]
        if not feature_columns:
            # å¦‚æœæ²¡æœ‰@ç¬¦å·åˆ—ï¼Œå‡è®¾æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾ï¼Œå…¶ä½™éƒ½æ˜¯ç‰¹å¾
            features = df.shape[1] - 1
        else:
            features = len(feature_columns)
        
        print(f"ğŸ” æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {features}")
        return features
    
    def validate_input_dimensions(self, config: MambaConfig, actual_input_dim: int) -> int:
        """éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„è¾“å…¥ç»´åº¦ä¸å®é™…æ•°æ®æ˜¯å¦ä¸€è‡´"""
        config_input_dim = getattr(config, 'input_dim', None)
        
        if config_input_dim is None:
            print(f"ğŸ“‹ é…ç½®æ–‡ä»¶ä¸­æœªæŒ‡å®šinput_dimï¼Œä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        if config_input_dim != actual_input_dim:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…ï¼é…ç½®: {config_input_dim}, å®é™…: {actual_input_dim}")
            print(f"ğŸ”§ ä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        print(f"âœ… è¾“å…¥ç»´åº¦éªŒè¯é€šè¿‡: {actual_input_dim}")
        return actual_input_dim
    
    def load_data_loaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """åŠ è½½çœŸå®æ•°æ®å¹¶åˆ›å»ºDataLoader"""
        # åŠ è½½çœŸå®æ•°æ®
        train_df, valid_df, test_df = self._load_real_dataframes()
        
        # åŠ¨æ€æ£€æµ‹ç‰¹å¾ç»´åº¦
        input_dim = self.get_input_dim_from_dataframe(train_df)
        
        # éªŒè¯å¹¶æ›´æ–°é…ç½®
        self.validate_input_dimensions(self.config, input_dim)
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        feature_columns = [col for col in train_df.columns if '@' in col]
        if not feature_columns:
            feature_columns = train_df.columns[:-1].tolist()
        
        label_column = train_df.columns[-1]
        
        # å‡†å¤‡æ•°æ®
        X_train = train_df[feature_columns].values.astype(np.float32)
        y_train = train_df[label_column].values.astype(np.int64)
        
        X_valid = valid_df[feature_columns].values.astype(np.float32)
        y_valid = valid_df[label_column].values.astype(np.int64)
        
        X_test = test_df[feature_columns].values.astype(np.float32)
        y_test = test_df[label_column].values.astype(np.int64)
        
        # å¤„ç†ç¼ºå¤±å€¼
        if np.isnan(X_train).any():
            print("âš ï¸ æ£€æµ‹åˆ°ç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‡å€¼å¡«å……")
            X_train = np.nan_to_num(X_train, nan=np.nanmean(X_train))
            X_valid = np.nan_to_num(X_valid, nan=np.nanmean(X_valid))
            X_test = np.nan_to_num(X_test, nan=np.nanmean(X_test))
        
        # æ ‡å‡†åŒ–
        self.scaler = StandardScaler()
        X_train = self.scaler.fit_transform(X_train)
        X_valid = self.scaler.transform(X_valid)
        X_test = self.scaler.transform(X_test)
        
        # ä¿å­˜scaler
        scaler_path = Path(self.config.checkpoint_dir) / 'scaler.pkl'
        scaler_path.parent.mkdir(parents=True, exist_ok=True)
        joblib.dump(self.scaler, scaler_path)
        print(f"ğŸ’¾ Scalerå·²ä¿å­˜åˆ°: {scaler_path}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))
        valid_dataset = TensorDataset(torch.FloatTensor(X_valid), torch.LongTensor(y_valid))
        test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))
        
        # åˆ›å»ºDataLoader
        train_loader = self._create_dataloader(train_dataset, shuffle=True, batch_size=batch_size)
        valid_loader = self._create_dataloader(valid_dataset, shuffle=False, batch_size=batch_size)
        test_loader = self._create_dataloader(test_dataset, shuffle=False, batch_size=batch_size)
        
        return train_loader, valid_loader, test_loader
    
    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç›¸å…³ä¿¡æ¯"""
        return {
            "workflow_type": self.data_split_info.get("workflow_type", "unknown"),
            "train_period": self.data_split_info.get("train", {}),
            "validation_period": self.data_split_info.get("validation", {}),
            "test_period": self.data_split_info.get("test", {}),
            "data_config": self.data_config
        }

# =============================================================================
# æ¨¡å‹åŸºç±»
# =============================================================================

class BaseModel(nn.Module, ABC):
    """æ¨¡å‹åŸºç±»"""
    
    def __init__(self, config: MambaConfig):
        super().__init__()
        self.config = config
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024)
        }

class MambaModel(BaseModel):
    """å®Œæ•´çš„Mambaæ¨¡å‹å®ç°"""
    
    def __init__(self, config: MambaConfig):
        super().__init__(config)
        self.config = config
        
        # è¾“å…¥ç»´åº¦åŠ¨æ€é…ç½®
        self.input_dim = getattr(config, 'input_dim', None)
        
        if self.input_dim is None:
            print("âš ï¸ é…ç½®ä¸­æœªæŒ‡å®šinput_dimï¼Œå°†åœ¨æ•°æ®åŠ è½½æ—¶è‡ªåŠ¨æ£€æµ‹")
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç½‘ç»œå±‚
        self.layers = None
        self.feature_projection = None
        self.norm_f = None
        self.classifier = None
        
    def _build_layers(self, input_dim: int):
        """ä½¿ç”¨åŠ¨æ€è¾“å…¥ç»´åº¦æ„å»ºç½‘ç»œ"""
        print(f"ğŸ”§ æ„å»ºç½‘ç»œå±‚ï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
        
        # è·å–å½“å‰è®¾å¤‡
        device = next(self.parameters()).device if list(self.parameters()) else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ç‰¹å¾æŠ•å½±å±‚
        self.feature_projection = nn.Linear(input_dim, self.config.d_model).to(device)
        
        # Mambaå±‚å †å 
        self.layers = nn.ModuleList([
            MambaLayer(self.config) for _ in range(self.config.n_layer)
        ]).to(device)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        self.norm_f = RMSNorm(self.config.d_model, eps=self.config.layer_norm_epsilon).to(device)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(self.config.d_model, self.config.d_model // 2),
            nn.ReLU(),
            nn.Dropout(self.config.dropout),
            nn.Linear(self.config.d_model // 2, self.config.num_classes)
        ).to(device)
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
        
        # ç§»åŠ¨æ–°åˆ›å»ºçš„å±‚åˆ°æ­£ç¡®çš„è®¾å¤‡
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å‚æ•°æ¥ç¡®å®šè®¾å¤‡
        existing_params = [p for name, p in self.named_parameters() if not any(layer_name in name for layer_name in ['feature_projection', 'layers', 'norm_f', 'classifier'])]
        if existing_params:
            device = existing_params[0].device
            self.to(device)
    
    def _init_weights(self, module: nn.Module) -> None:
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Conv1d):
            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºç½‘ç»œ
        if self.layers is None:
            if self.input_dim is None:
                self.input_dim = x.shape[-1]
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç»´åº¦: {self.input_dim}")
            self._build_layers(self.input_dim)
        
        # ç‰¹å¾æŠ•å½±
        if x.dim() == 2:
            x = self.feature_projection(x)
            x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        
        # ç¡®ä¿è¾“å…¥æ˜¯3Då¼ é‡
        if x.dim() == 2:
            x = x.unsqueeze(1)
        
        # é€šè¿‡Mambaå±‚
        for layer in self.layers:
            x = layer(x)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        x = self.norm_f(x)
        
        # åºåˆ—æ± åŒ–
        if x.shape[1] > 1:
            x = x.mean(dim=1)
        else:
            x = x.squeeze(1)
        
        # åˆ†ç±»
        logits = self.classifier(x)
        
        return logits

# =============================================================================
# è®­ç»ƒå™¨ç±»
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config: MambaConfig, model: BaseModel, data_loader_manager: MambaDataLoaderManager,
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir  # Checkpointä¿å­˜ç›®å½•
        
        # è®¾å¤‡è®¾ç½®
        self.device = setup_device(getattr(config, 'device', 'auto'))
        self.rank = 0
        self.world_size = 1
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logging(config.logs_dir, "training")
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config.use_mixed_precision and torch.cuda.is_available() else None
        self.use_amp = config.use_mixed_precision and torch.cuda.is_available()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_score = 0.0
        self.train_history = []
        
        # Checkpointè·Ÿè¸ªå™¨ï¼ˆæ”¯æŒå¤šåŒºé—´æœ€ä½³ï¼‰
        self.checkpoint_tracker = {
            'global_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'early_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'mid_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'late_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None}
        }
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(config.seed)
        
        # åˆ›å»ºç›®å½•
        create_directories(config)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡å¹¶è®°å½•GPUä¿¡æ¯
        self.model.to(self.device)
        self._log_device_info()
        
        # è®°å½•æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if self.use_amp:
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_info = self.data_loader_manager.get_data_info()
        self.logger.info(f"æ•°æ®åˆ†å‰²ä¿¡æ¯: {data_info['workflow_type']}")
        self.logger.info(f"è®­ç»ƒæœŸé—´: {data_info['train_period']}")
        self.logger.info(f"éªŒè¯æœŸé—´: {data_info['validation_period']}")
        self.logger.info(f"æµ‹è¯•æœŸé—´: {data_info['test_period']}")
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡å’ŒGPUä½¿ç”¨ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            
            self.logger.info(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name}")
            self.logger.info(f"ğŸ“Š GPUæ€»å†…å­˜: {gpu_memory:.1f}GB")
            self.logger.info(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {current_memory:.2f}GB")
            self.logger.info(f"ğŸ“Š å¯ç”¨å†…å­˜: {gpu_memory - current_memory:.2f}GB")
        else:
            self.logger.info("ğŸ–¥ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            
        # è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f}MB")
    
    def setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=10, T_mult=2, eta_min=1e-5
        )
        
        self.criterion = nn.CrossEntropyLoss()
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                self.scaler.scale(loss).backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            probs = torch.softmax(output, dim=1)
            preds = torch.argmax(output, dim=1)
            all_probs.extend(probs.detach().cpu().numpy())
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
        
        metrics = {"loss": total_loss / len(train_loader)}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        
        # è®¡ç®—AUC
        if len(np.unique(all_targets)) == 2:
            try:
                all_probs_array = np.array(all_probs)
                if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
            except Exception:
                pass
        
        return metrics
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                if self.use_amp:
                    with autocast():
                        output = self.model(data)
                        loss = self.criterion(output, target)
                else:
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                total_loss += loss.item()
                
                probs = torch.softmax(output, dim=1)
                preds = torch.argmax(output, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        metrics = {"loss": total_loss / len(val_loader)}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
        
        # è®¡ç®—AUC
        if len(np.unique(all_targets)) == 2:
            try:
                all_probs_array = np.array(all_probs)
                if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
                    # æ·»åŠ éªŒè¯é¢„æµ‹æ•°æ®ç”¨äºOptunaä¼˜åŒ–
                    metrics["y_true_val"] = all_targets
                    metrics["y_prob_val"] = all_probs_array[:, 1].tolist()
            except Exception:
                pass
        
        return metrics
    
    def train(self, batch_size: int = None, no_save_model: bool = False) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        # è·å–æ•°æ®åŠ è½½å™¨
        batch_size = batch_size or self.config.batch_size
        train_loader, val_loader, test_loader = self.data_loader_manager.load_data_loaders(batch_size=batch_size)
        
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆé€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼‰
        sample_batch = next(iter(train_loader))
        sample_data = sample_batch[0][:1].to(self.device)  # å–ä¸€ä¸ªæ ·æœ¬
        with torch.no_grad():
            _ = self.model(sample_data)  # è§¦å‘æ¨¡å‹åˆå§‹åŒ–
        
        # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        self.model.to(self.device)
        
        self.setup_training_components()
        
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {self.model.get_model_info()}")
        
        start_time = time.time()
        epochs = self.config.epochs
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)

            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # è®°å½•å†å²
            epoch_info = {
                "epoch": epoch + 1,
                "train_loss": train_metrics["loss"],
                "val_loss": val_metrics["loss"],
                "lr": self.optimizer.param_groups[0]['lr'],
                "time": time.time() - epoch_start
            }
            
            epoch_info.update({
                "train_acc": train_metrics.get("accuracy", 0),
                "val_acc": val_metrics.get("accuracy", 0),
                "val_f1": val_metrics.get("f1", 0)
            })
            
            if "auc" in train_metrics:
                epoch_info["train_auc"] = train_metrics["auc"]
            if "auc" in val_metrics:
                epoch_info["val_auc"] = val_metrics["auc"]
            
            current_score = val_metrics.get("accuracy", 0)
            
            self.train_history.append(epoch_info)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_score > self.best_val_score:
                self.best_val_score = current_score
                if not no_save_model:
                    self.save_checkpoint(epoch + 1, is_best=True)
            
            # GPUå†…å­˜ç›‘æ§
            if self.device.type == 'cuda' and (epoch + 1) % 10 == 0:
                current_memory = torch.cuda.memory_allocated() / 1024**3
                max_memory = torch.cuda.max_memory_allocated() / 1024**3
                cached_memory = torch.cuda.memory_reserved() / 1024**3
                self.logger.info(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨ (Epoch {epoch+1}): å½“å‰ {current_memory:.2f}GB, å³°å€¼ {max_memory:.2f}GB, ç¼“å­˜ {cached_memory:.2f}GB")
            
            # æ—¥å¿—è¾“å‡º
            log_msg = (
                f"Epoch {epoch+1}/{epochs} - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}, "
                f"Val Acc: {val_metrics.get('accuracy', 0):.4f}"
            )

            if 'auc' in val_metrics:
                log_msg += f", Val AUC: {val_metrics['auc']:.4f}"

            log_msg += f", Time: {epoch_info['time']:.2f}s"
            self.logger.info(log_msg)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        return {
            "best_val_score": self.best_val_score,
            "total_epochs": len(self.train_history),
            "total_time": total_time,
            "train_history": self.train_history
        }
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_score': self.best_val_score,
            'config': asdict(self.config)
        }
        
        checkpoint_path = Path(self.config.checkpoint_dir) / "latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        if is_best:
            best_path = Path(self.config.checkpoint_dir) / "best_model.pth"
            torch.save(checkpoint, best_path)
            self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
    
    def save_checkpoint_if_best(self, epoch: int, val_loss: float):
        """æ™ºèƒ½checkpointä¿å­˜ï¼šå…¨å±€æœ€ä½³ + åŒºé—´æœ€ä½³"""
        # æ›´æ–°å…¨å±€æœ€ä½³
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        # æ›´æ–°åŒºé—´æœ€ä½³ï¼ˆå®æ—¶æ›´æ–°ï¼‰
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        # ğŸ’¡ å…³é”®ï¼šåœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³ä¸ºinterval_best
        if epoch == 29:  # epochä»0å¼€å§‹ï¼Œ29æ˜¯ç¬¬30ä¸ªepoch
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')
    
    def _save_checkpoint(self, epoch: int, val_loss: float, checkpoint_type: str):
        """ä¿å­˜å•ä¸ªcheckpoint"""
        import os
        
        # åˆ é™¤æ—§checkpoint
        old_path = self.checkpoint_tracker[checkpoint_type]['path']
        if old_path and os.path.exists(old_path):
            os.remove(old_path)
        
        # ä¿å­˜æ–°checkpoint
        checkpoint_name = f"checkpoint_{checkpoint_type}_epoch{epoch:03d}_val{val_loss:.4f}.pt"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'checkpoint_type': checkpoint_type,
            'config': asdict(self.config)
        }
        
        torch.save(checkpoint, checkpoint_path)
        
        self.checkpoint_tracker[checkpoint_type] = {
            'epoch': epoch,
            'val_loss': val_loss,
            'path': checkpoint_path
        }
        
        self.logger.info(f"ğŸ’¾ ä¿å­˜{checkpoint_type} checkpoint: epoch {epoch}, val_loss {val_loss:.4f}")
    
    def _save_interval_best(self, checkpoint_type: str, epoch_range: str):
        """åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³checkpointä¸ºinterval_best"""
        import shutil
        import os
        
        # è·å–å½“å‰åŒºé—´æœ€ä½³çš„checkpointè·¯å¾„
        source_path = self.checkpoint_tracker[checkpoint_type]['path']
        
        if source_path and os.path.exists(source_path):
            # åˆ›å»ºinterval_bestæ–‡ä»¶å
            interval_name = f"interval_best_{epoch_range}.pt"
            interval_path = os.path.join(self.checkpoint_dir, interval_name)
            
            # å¤åˆ¶checkpoint
            shutil.copy2(source_path, interval_path)
            
            epoch = self.checkpoint_tracker[checkpoint_type]['epoch']
            val_loss = self.checkpoint_tracker[checkpoint_type]['val_loss']
            
            self.logger.info(f"ğŸ“¦ å¤åˆ¶åŒºé—´æœ€ä½³ [{epoch_range}]: epoch {epoch}, val_loss {val_loss:.4f}")
            self.logger.info(f"   æºæ–‡ä»¶: {os.path.basename(source_path)}")
            self.logger.info(f"   ç›®æ ‡æ–‡ä»¶: {interval_name}")
        else:
            self.logger.warning(f"âš ï¸ åŒºé—´ [{epoch_range}] æ²¡æœ‰æ‰¾åˆ°æœ€ä½³checkpoint")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = Path(self.config.results_dir) / "training_history.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        self.logger.info(f"è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

# =============================================================================
# æ¨ç†å™¨ç±»
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, config: MambaConfig, model: BaseModel, data_loader_manager: MambaDataLoaderManager,
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir  # Checkpointä¿å­˜ç›®å½•
        self.device = setup_device()
        self.logger = setup_logging(config.logs_dir, "inference")
        
        self.model.to(self.device)
        self.model.eval()
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        return checkpoint
    
    def predict_batch(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """æ‰¹é‡æ¨ç†"""
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in data_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
        
        return np.array(all_preds), np.array(all_probs)
    
    def evaluate(self) -> Dict[str, float]:
        """æ¨¡å‹è¯„ä¼°"""
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        metrics["precision"] = precision_score(all_targets, all_preds, average='weighted')
        metrics["recall"] = recall_score(all_targets, all_preds, average='weighted')
        metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
        
        # AUCæŒ‡æ ‡è®¡ç®—
        if len(np.unique(all_targets)) == 2:
            all_probs_positive = np.array(all_probs)[:, 1]
            metrics["auc"] = roc_auc_score(all_targets, all_probs_positive)
        
        return metrics

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: MambaConfig, model: BaseModel):
        self.config = config
        self.model = model
    
    def generate_model_md(self, output_path: str = "MODEL.md"):
        """ç”ŸæˆMODEL.mdæ–‡æ¡£"""
        model_info = self.model.get_model_info()
        
        content = f"""# {self.config.model_name} Model

## æ¨¡å‹æ¦‚è¿°

{self.config.model_name} æ˜¯ä¸€ä¸ªåŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹(Mamba)çš„{self.config.model_type}æ¨¡å‹ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- **ä»»åŠ¡ç±»å‹**: {self.config.model_type}
- **æ¨¡å‹å‚æ•°**: {model_info['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°**: {model_info['trainable_parameters']:,}
- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.2f} MB

## æ¨¡å‹æ¶æ„

### Mambaæ ¸å¿ƒç»„ä»¶

- **é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹**: é«˜æ•ˆçš„åºåˆ—å¤„ç†æ¶æ„
- **RMSNorm**: Root Mean Square Layer Normalization
- **é—¨æ§æœºåˆ¶**: é€‰æ‹©æ€§ä¿¡æ¯ä¼ é€’
- **1Då·ç§¯**: å±€éƒ¨ç‰¹å¾æå–

### ç½‘ç»œç»“æ„

```
è¾“å…¥ -> ç‰¹å¾æŠ•å½± -> Mambaå±‚å †å  -> å½’ä¸€åŒ– -> åˆ†ç±»å¤´ -> è¾“å‡º
```

## æŠ€æœ¯åŸç†

Mambaæ¨¡å‹åŸºäºé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹ï¼Œé€šè¿‡é€‰æ‹©æ€§æ‰«æç®—æ³•å®ç°é«˜æ•ˆçš„åºåˆ—å»ºæ¨¡ã€‚

## é…ç½®å‚æ•°

### è®­ç»ƒé…ç½®
- å­¦ä¹ ç‡: {self.config.learning_rate}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- è®­ç»ƒè½®æ•°: {self.config.epochs}
- æƒé‡è¡°å‡: {self.config.weight_decay}

### æ¨¡å‹é…ç½®
- æ¨¡å‹ç»´åº¦: {self.config.d_model}
- å±‚æ•°: {self.config.n_layer}
- çŠ¶æ€ç»´åº¦: {self.config.d_state}
- å·ç§¯æ ¸å¤§å°: {self.config.d_conv}

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```bash
python mamba_unified.py train --config config.yaml --data-config data.yaml
```

### æ¨¡å‹æ¨ç†

```bash
python mamba_unified.py inference --config config.yaml --data-config data.yaml --checkpoint best_model.pth
```

### æ¨¡å‹è¯„ä¼°

```bash
python mamba_unified.py inference --config config.yaml --data-config data.yaml
```

## æ€§èƒ½æŒ‡æ ‡

æ¨¡å‹æ”¯æŒä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡ï¼š
- å‡†ç¡®ç‡ (Accuracy)
- ç²¾ç¡®ç‡ (Precision)
- å¬å›ç‡ (Recall)
- F1åˆ†æ•° (F1-Score)
- AUC (Area Under Curve)

## æ³¨æ„äº‹é¡¹

- æ¨¡å‹æ”¯æŒåŠ¨æ€è¾“å…¥ç»´åº¦æ£€æµ‹
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒä»¥æå‡æ€§èƒ½
- æ”¯æŒGPUåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†

## æ›´æ–°æ—¥å¿—

- åˆå§‹ç‰ˆæœ¬: {datetime.now().strftime('%Y-%m-%d')}

"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¨¡å‹æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")

# =============================================================================
# Checkpointç®¡ç†æ–¹æ³•ï¼ˆæ·»åŠ åˆ°UnifiedTrainerç±»ä¸­ï¼‰
# =============================================================================

# æ³¨æ„ï¼šä»¥ä¸‹æ–¹æ³•åº”è¯¥æ·»åŠ åˆ°UnifiedTrainerç±»ä¸­ï¼Œè¿™é‡Œä½œä¸ºå‚è€ƒ

def _add_checkpoint_methods_to_trainer():
    """
    å°†ä»¥ä¸‹æ–¹æ³•æ·»åŠ åˆ°UnifiedTrainerç±»ä¸­ï¼š
    
    def save_checkpoint_if_best(self, epoch: int, val_loss: float):
        '''æ™ºèƒ½checkpointä¿å­˜ï¼šå…¨å±€æœ€ä½³ + åŒºé—´æœ€ä½³'''
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        if epoch == 29:
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')
    """
    pass

# =============================================================================
# ä¸»è¦æ¥å£å‡½æ•°
# =============================================================================

def create_model_factory(config: MambaConfig) -> BaseModel:
    """æ¨¡å‹å·¥å‚å‡½æ•°"""
    return MambaModel(config)

def create_data_loader_manager(data_config_path: str, config: MambaConfig) -> MambaDataLoaderManager:
    """æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨å·¥å‚å‡½æ•°"""
    return MambaDataLoaderManager(data_config_path, config)

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               optuna_config_path: str = None, device: str = "auto",
               epochs_override: int = None, no_save_model: bool = False,
               seed: int = 42, checkpoint_dir: str = "checkpoints"):
    # è®¾ç½®éšæœºç§å­
    import random
    import numpy as np
    import torch
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    logger.info(f"ğŸ’¾ Checkpointä¿å­˜ç›®å½•: {checkpoint_dir}")
    
    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config_dict = {}
    
    # æå–ç›¸å…³é…ç½®å¹¶åˆ›å»ºé…ç½®å¯¹è±¡
    mamba_config_dict = {}
    if 'architecture' in config_dict:
        arch_config = config_dict['architecture']
        mamba_config_dict.update({
            'd_model': arch_config.get('d_model', 256),
            'n_layer': arch_config.get('n_layer', 8),
            'd_state': arch_config.get('d_state', 16),
            'd_conv': arch_config.get('d_conv', 4),
            'expand': arch_config.get('expand', 2),
            'dropout': arch_config.get('dropout', 0.1),
        })
    
    if 'training' in config_dict:
        train_config = config_dict['training']
        mamba_config_dict.update({
            'learning_rate': train_config.get('learning_rate', 0.0003),
            'batch_size': train_config.get('batch_size', 32),
            'epochs': train_config.get('epochs', 100),
            'weight_decay': train_config.get('weight_decay', 1e-5),
        })
    
    config = MambaConfig(**mamba_config_dict)
    
    # åº”ç”¨Optunaé…ç½®è¦†ç›–
    if optuna_config_path and os.path.exists(optuna_config_path):
        with open(optuna_config_path, 'r', encoding='utf-8') as f:
            optuna_config = yaml.safe_load(f)
        
        # åº”ç”¨è¶…å‚æ•°è¦†ç›–
        for key, value in optuna_config.items():
            if hasattr(config, key):
                setattr(config, key, value)
                print(f"ğŸ”§ Optunaè¦†ç›–å‚æ•°: {key} = {value}")
    
    # è¦†ç›–é…ç½®
    if device != "auto":
        config.device = device
    if epochs_override:
        config.epochs = epochs_override
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = UnifiedTrainer(config, model, data_loader_manager, checkpoint_dir=checkpoint_dir)
    
    # æ‰§è¡Œè®­ç»ƒ
    results = trainer.train(no_save_model=no_save_model)
    
    # è¾“å‡ºJSONæ ¼å¼ç»“æœä¾›Optunaè§£æ
    final_results = {
        "final_results": {
            "best_val_score": results['best_val_score'],
            "total_epochs": results['total_epochs'],
            "total_time": results['total_time'],
            "final_train_acc": results['train_history'][-1].get('train_acc', 0),
            "final_val_acc": results['train_history'][-1].get('val_acc', 0),
            "train_auc": results['train_history'][-1].get('train_auc', 0),
            "val_auc": results['train_history'][-1].get('val_auc', 0),
            "train_losses": [epoch['train_loss'] for epoch in results['train_history']],
            "val_losses": [epoch['val_loss'] for epoch in results['train_history']],
            "train_accuracies": [epoch.get('train_acc', 0) for epoch in results['train_history']],
            "val_accuracies": [epoch.get('val_acc', 0) for epoch in results['train_history']]
        }
    }
    print(json.dumps(final_results, indent=2))
    
    return results

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "best_model.pth"):
    """ä¸»æ¨ç†å‡½æ•°"""
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config_dict = {}
    
    # æå–ç›¸å…³é…ç½®å¹¶åˆ›å»ºé…ç½®å¯¹è±¡
    mamba_config_dict = {}
    if 'architecture' in config_dict:
        arch_config = config_dict['architecture']
        mamba_config_dict.update({
            'd_model': arch_config.get('d_model', 256),
            'n_layer': arch_config.get('n_layer', 8),
            'd_state': arch_config.get('d_state', 16),
            'd_conv': arch_config.get('d_conv', 4),
            'expand': arch_config.get('expand', 2),
            'dropout': arch_config.get('dropout', 0.1),
        })
    
    config = MambaConfig(**mamba_config_dict)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = UnifiedInferencer(config, model, data_loader_manager)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    if os.path.exists(checkpoint_path):
        checkpoint = inferencer.load_checkpoint(checkpoint_path)
    else:
        print(f"âš ï¸ æ£€æŸ¥ç‚¹æ–‡ä»¶ {checkpoint_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹")
    
    # æ‰§è¡Œæ¨ç†å’Œè¯„ä¼°
    metrics = inferencer.evaluate()
    
    print("æ¨ç†ç»“æœ:")
    for key, value in metrics.items():
        print(f"  {key}: {value:.4f}")
    
    return metrics

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ä¸»æ–‡æ¡£ç”Ÿæˆå‡½æ•°"""
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config_dict = {}
    
    # æå–ç›¸å…³é…ç½®å¹¶åˆ›å»ºé…ç½®å¯¹è±¡
    mamba_config_dict = {}
    if 'architecture' in config_dict:
        arch_config = config_dict['architecture']
        mamba_config_dict.update({
            'd_model': arch_config.get('d_model', 256),
            'n_layer': arch_config.get('n_layer', 8),
            'd_state': arch_config.get('d_state', 16),
            'd_conv': arch_config.get('d_conv', 4),
            'expand': arch_config.get('expand', 2),
            'dropout': arch_config.get('dropout', 0.1),
        })
    
    config = MambaConfig(**mamba_config_dict)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config, model)
    doc_generator.generate_model_md("MODEL.md")
    
    print("æ–‡æ¡£ç”Ÿæˆå®Œæˆ")

# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€Mambaæ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·")
    parser.add_argument("mode", choices=["train", "inference", "docs"], help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="config.yaml", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-config", default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default="best_model.pth", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--data", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # Optunaä¼˜åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--optuna-config", help="Optunaè¯•éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--no-save-model", action="store_true", help="ä¸ä¿å­˜æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°æ€§ï¼‰")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    
    return parser.parse_args()

if __name__ == "__main__":
    # è®¾ç½®å›ºå®šçš„æ—¥å¿—è·¯å¾„
    log_dir = "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/training/logs/mamba_3493354528"
    logger = setup_logging(log_dir=log_dir, log_filename="log.txt")
    
    args = parse_args()
    
    if args.mode == "train":
        main_train(
            config_path=args.config,
            data_config_path=args.data_config,
            optuna_config_path=args.optuna_config,
            device=args.device,
            epochs_override=args.epochs,
            no_save_model=args.no_save_model,
            seed=args.seed,
            checkpoint_dir=args.checkpoint_dir
        )
    elif args.mode == "inference":
        main_inference(args.config, args.data_config, args.checkpoint)
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data_config)