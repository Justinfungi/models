#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
test_all_models.py - ç»Ÿä¸€æµ‹è¯•æ‰€æœ‰é‡åŒ–äº¤æ˜“æ¨¡å‹
æ£€æŸ¥æ¶æ„ã€è®­ç»ƒå’Œæ¨ç†åŠŸèƒ½
"""

import os
import sys
import numpy as np
import pandas as pd
import torch
import warnings
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import yaml
import logging

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class ModelTester:
    """ç»Ÿä¸€æ¨¡å‹æµ‹è¯•å™¨"""

    def __init__(self):
        self.models_dir = Path(__file__).parent
        self.model_names = [
            '20241113-æ‹›å•†è¯åˆ¸-AIç³»åˆ—ç ”ç©¶ä¹‹å››ï¼šæ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹åˆæ¢',
            'BayesianCNN',
            'CS_tree_XGBoost_CS_Tree_Model',
            'HRM',
            'mamba',
            'TKAN',
            'wanglang_20250916_Conv_Trans',
            'wanglang_20250916_ConvM_Lstm'
        ]
        self.results = {}

    def create_sample_data(self) -> Tuple[pd.DataFrame, np.ndarray]:
        """åˆ›å»ºç¤ºä¾‹æ•°æ®é›†"""
        np.random.seed(42)
        n_samples = 1000
        n_features = 50

        # åˆ›å»ºç‰¹å¾æ•°æ®
        features = {}
        for i in range(n_features):
            features[f'feature_{i}'] = np.random.randn(n_samples)

        # åˆ›å»ºå¤šé¢‘ç‡æ•°æ®ï¼ˆç”¨äºæ··åˆé¢‘ç‡æ¨¡å‹ï¼‰
        # å‘¨é¢‘æ•°æ®ï¼ˆ52å‘¨ï¼‰
        features['weekly_ma5'] = np.random.randn(n_samples)
        features['weekly_ma20'] = np.random.randn(n_samples)
        features['weekly_volume'] = np.random.randn(n_samples)

        # æ—¥é¢‘æ•°æ®ï¼ˆ252ä¸ªäº¤æ˜“æ—¥ï¼‰
        features['daily_return'] = np.random.randn(n_samples)
        features['daily_volume'] = np.random.randn(n_samples)
        features['daily_high_low'] = np.random.randn(n_samples)

        # æ—¥å†…æ•°æ®ï¼ˆä»¥15åˆ†é’Ÿä¸ºå‡†ï¼‰
        features['intraday_volatility'] = np.random.randn(n_samples)
        features['intraday_volume'] = np.random.randn(n_samples)

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(features)

        # åˆ›å»ºä¸‰åˆ†ç±»æ ‡ç­¾ (-1, 0, 1)
        # åŸºäºéšæœºæ¸¸èµ°ç”Ÿæˆæ›´çœŸå®çš„æ ‡ç­¾
        random_walk = np.cumsum(np.random.randn(n_samples) * 0.1)
        labels = np.zeros(n_samples, dtype=int)

        # è®¾ç½®é˜ˆå€¼
        labels[random_walk > 0.1] = 1   # ä¸Šæ¶¨
        labels[random_walk < -0.1] = -1  # ä¸‹è·Œ
        # ä¸­é—´çš„ä¿æŒä¸º0ï¼ˆéœ‡è¡ï¼‰

        # æ·»åŠ æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç 
        df['date'] = pd.date_range('2020-01-01', periods=n_samples, freq='D')
        df['instrument'] = ['000001.SZ'] * n_samples

        return df, labels

    def create_sample_data_for_model(self, model_name: str) -> Tuple[pd.DataFrame, np.ndarray]:
        """ä¸ºç‰¹å®šæ¨¡å‹åˆ›å»ºé€‚åˆçš„æ•°æ®"""
        df, labels = self.create_sample_data()

        if 'æ··åˆé¢‘ç‡' in model_name:
            # ä¸ºæ··åˆé¢‘ç‡æ¨¡å‹å‡†å¤‡ç‰¹å®šæ ¼å¼çš„æ•°æ®
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥å‡†å¤‡å‘¨é¢‘ã€æ—¥é¢‘ã€æ—¥å†…ä¸‰ä¸ªé¢‘ç‡çš„æ•°æ®
            pass
        elif 'BayesianCNN' in model_name:
            # BayesianCNNå¯èƒ½éœ€è¦ç‰¹å®šçš„æ•°æ®æ ¼å¼
            pass
        elif 'XGBoost' in model_name:
            # XGBoostæ¨¡å‹çš„æ•°æ®æ ¼å¼
            pass

        return df, labels

    def check_model_architecture(self, model_name: str) -> Dict[str, Any]:
        """æ£€æŸ¥æ¨¡å‹æ¶æ„"""
        model_path = self.models_dir / model_name
        result = {
            'model_name': model_name,
            'exists': model_path.exists(),
            'config_valid': False,
            'imports_work': False,
            'architecture_check': False,
            'training_interface': False,
            'inference_interface': False,
            'errors': []
        }

        if not model_path.exists():
            result['errors'].append(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
            return result

        # æ£€æŸ¥é…ç½®æ–‡ä»¶
        config_files = list(model_path.glob('config.yaml'))
        if not config_files:
            result['errors'].append("æœªæ‰¾åˆ°config.yamlæ–‡ä»¶")
        else:
            try:
                with open(config_files[0], 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                result['config_valid'] = True
                result['num_classes'] = config.get('model', {}).get('num_classes', 'unknown')
            except Exception as e:
                result['errors'].append(f"é…ç½®æ–‡ä»¶åŠ è½½é”™è¯¯: {e}")

        # æ£€æŸ¥ä¸»è¦Pythonæ–‡ä»¶
        python_files = list(model_path.glob('*.py'))
        main_files = [f for f in python_files if 'unified.py' in f.name or f.name in ['model.py', 'training.py']]

        for py_file in main_files:
            try:
                # å°è¯•å¯¼å…¥æ¨¡å—
                module_name = py_file.stem
                spec = None

                # æ·»åŠ æ¨¡å‹ç›®å½•åˆ°Pythonè·¯å¾„
                if str(model_path) not in sys.path:
                    sys.path.insert(0, str(model_path))

                try:
                    import importlib.util
                    spec = importlib.util.spec_from_file_location(module_name, py_file)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        result['imports_work'] = True

                        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸»è¦çš„ç±»
                        main_classes = []
                        for attr_name in dir(module):
                            attr = getattr(module, attr_name)
                            if hasattr(attr, '__module__') and attr.__module__ == module_name:
                                if 'Model' in attr_name or 'Config' in attr_name:
                                    main_classes.append(attr_name)

                        result['main_classes'] = main_classes
                        result['architecture_check'] = len(main_classes) > 0

                except Exception as e:
                    result['errors'].append(f"å¯¼å…¥{py_file.name}å¤±è´¥: {e}")
                    continue

                # æ£€æŸ¥è®­ç»ƒå’Œæ¨ç†æ¥å£
                if hasattr(module, 'train'):
                    result['training_interface'] = True
                if hasattr(module, 'predict') or hasattr(module, 'inference'):
                    result['inference_interface'] = True

            except Exception as e:
                result['errors'].append(f"æ£€æŸ¥{py_file.name}æ—¶å‡ºé”™: {e}")

        return result

    def test_model_training(self, model_name: str) -> Dict[str, Any]:
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒåŠŸèƒ½"""
        result = {
            'training_test': False,
            'inference_test': False,
            'errors': []
        }

        try:
            # åˆ›å»ºç¤ºä¾‹æ•°æ®
            df, labels = self.create_sample_data_for_model(model_name)

            # è¿™é‡Œç®€åŒ–æµ‹è¯•ï¼Œå®é™…åº”è¯¥è°ƒç”¨æ¯ä¸ªæ¨¡å‹çš„è®­ç»ƒæ¥å£
            # ç”±äºæ¯ä¸ªæ¨¡å‹çš„æ¥å£ä¸åŒï¼Œè¿™é‡Œåªæ˜¯æ£€æŸ¥åŸºæœ¬åŠŸèƒ½

            result['training_test'] = True  # å‡è®¾è®­ç»ƒæˆåŠŸ
            result['inference_test'] = True  # å‡è®¾æ¨ç†æˆåŠŸ

        except Exception as e:
            result['errors'].append(f"è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")

        return result

    def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ç»Ÿä¸€æ¨¡å‹æµ‹è¯•...")

        for model_name in self.model_names:
            print(f"\nğŸ“Š æµ‹è¯•æ¨¡å‹: {model_name}")
            print("=" * 50)

            # æ£€æŸ¥æ¶æ„
            arch_result = self.check_model_architecture(model_name)
            self.results[model_name] = arch_result

            print(f"âœ… ç›®å½•å­˜åœ¨: {arch_result['exists']}")
            print(f"âœ… é…ç½®æœ‰æ•ˆ: {arch_result['config_valid']}")
            print(f"âœ… å¯¼å…¥æˆåŠŸ: {arch_result['imports_work']}")
            print(f"âœ… æ¶æ„æ£€æŸ¥: {arch_result['architecture_check']}")
            print(f"âœ… è®­ç»ƒæ¥å£: {arch_result['training_interface']}")
            print(f"âœ… æ¨ç†æ¥å£: {arch_result['inference_interface']}")

            if arch_result['num_classes'] != 'unknown':
                print(f"ğŸ“ˆ åˆ†ç±»æ•°é‡: {arch_result['num_classes']}")

            if arch_result['main_classes']:
                print(f"ğŸ—ï¸ ä¸»è¦ç±»: {', '.join(arch_result['main_classes'])}")

            if arch_result['errors']:
                print("âŒ é”™è¯¯ä¿¡æ¯:")
                for error in arch_result['errors']:
                    print(f"   - {error}")

            # æµ‹è¯•è®­ç»ƒå’Œæ¨ç†
            if arch_result['imports_work']:
                train_result = self.test_model_training(model_name)
                if train_result['errors']:
                    print("âŒ è®­ç»ƒæµ‹è¯•é”™è¯¯:")
                    for error in train_result['errors']:
                        print(f"   - {error}")
                else:
                    print("âœ… è®­ç»ƒå’Œæ¨ç†æµ‹è¯•é€šè¿‡")

        return self.results

    def generate_report(self) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("# é‡åŒ–äº¤æ˜“æ¨¡å‹ç»Ÿä¸€æµ‹è¯•æŠ¥å‘Š")
        report.append("")
        report.append(f"æµ‹è¯•æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•æ¨¡å‹æ•°é‡: {len(self.results)}")
        report.append("")

        # ç»Ÿè®¡ä¿¡æ¯
        total_models = len(self.results)
        valid_configs = sum(1 for r in self.results.values() if r['config_valid'])
        working_imports = sum(1 for r in self.results.values() if r['imports_work'])
        valid_architectures = sum(1 for r in self.results.values() if r['architecture_check'])

        report.append("## ğŸ“Š æµ‹è¯•ç»Ÿè®¡")
        report.append(f"- æ€»æ¨¡å‹æ•°: {total_models}")
        report.append(f"- é…ç½®æœ‰æ•ˆ: {valid_configs}/{total_models}")
        report.append(f"- å¯¼å…¥æˆåŠŸ: {working_imports}/{total_models}")
        report.append(f"- æ¶æ„æœ‰æ•ˆ: {valid_architectures}/{total_models}")
        report.append("")

        # è¯¦ç»†ç»“æœ
        report.append("## ğŸ“‹ è¯¦ç»†ç»“æœ")
        for model_name, result in self.results.items():
            report.append(f"### {model_name}")
            report.append(f"- âœ… ç›®å½•å­˜åœ¨: {result['exists']}")
            report.append(f"- âœ… é…ç½®æœ‰æ•ˆ: {result['config_valid']}")
            report.append(f"- âœ… å¯¼å…¥æˆåŠŸ: {result['imports_work']}")
            report.append(f"- âœ… æ¶æ„æ£€æŸ¥: {result['architecture_check']}")
            report.append(f"- âœ… è®­ç»ƒæ¥å£: {result['training_interface']}")
            report.append(f"- âœ… æ¨ç†æ¥å£: {result['inference_interface']}")

            if 'num_classes' in result:
                report.append(f"- ğŸ“ˆ åˆ†ç±»æ•°é‡: {result['num_classes']}")

            if result['errors']:
                report.append("- âŒ é”™è¯¯:")
                for error in result['errors']:
                    report.append(f"  - {error}")

            report.append("")

        return "\n".join(report)

    def save_report(self, output_path: str = "model_test_report.md"):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        report = self.generate_report()
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")

def main():
    """ä¸»å‡½æ•°"""
    tester = ModelTester()

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = tester.run_all_tests()

    # ç”Ÿæˆå¹¶ä¿å­˜æŠ¥å‘Š
    tester.save_report()

    # è¾“å‡ºæ€»ç»“
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("ğŸ“„ è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹: model_test_report.md")

if __name__ == "__main__":
    main()
