#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
training.py - æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹è®­ç»ƒè„šæœ¬
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader as TorchDataLoader, TensorDataset, random_split
import pandas as pd
import numpy as np
import yaml
import argparse
import os
import sys
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import time
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from model import MixedFrequencyFactorModel, ModelConfig

class DataConfig:
    """æ•°æ®é…ç½®ç±»"""
    
    def __init__(self, config_path: str = "data.yaml"):
        self.config_path = config_path
        self.config = self._load_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except FileNotFoundError:
            raise FileNotFoundError(f"æ•°æ®é…ç½®æ–‡ä»¶ {self.config_path} ä¸å­˜åœ¨")
        except yaml.YAMLError as e:
            raise ValueError(f"æ•°æ®é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")

class CustomDataLoader:
    """æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_config: DataConfig):
        self.data_config = data_config
        self.logger = logging.getLogger(__name__)
        
    def load_data(self, quick_test: bool = False) -> Tuple[pd.DataFrame, List[str]]:
        """
        åŠ è½½æ•°æ®
        
        Args:
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            æ•°æ®DataFrameå’Œç‰¹å¾åˆ—ååˆ—è¡¨
        """
        config = self.data_config.config
        
        # è·å–æ•°æ®è·¯å¾„
        main_path = config['data_source']['train_path']
        alternative_paths = config['data_source'].get('alternative_paths', [])
        
        # å°è¯•åŠ è½½æ•°æ®
        data_path = None
        for path in [main_path] + alternative_paths:
            if os.path.exists(path):
                data_path = path
                break
                
        if data_path is None:
            raise FileNotFoundError(f"æ— æ³•æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œå°è¯•çš„è·¯å¾„: {[main_path] + alternative_paths}")
            
        self.logger.info(f"ä» {data_path} åŠ è½½æ•°æ®")
        
        # åŠ è½½æ•°æ®
        if data_path.endswith('.pq') or data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
        elif data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_path}")
            
        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨å‰100è¡Œ
        if quick_test:
            df = df.head(100)
            self.logger.info(f"å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {len(df)} è¡Œæ•°æ®")
        
        # è·å–ç‰¹å¾åˆ—
        feature_columns = self._get_feature_columns(df)
        
        self.logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}, ç‰¹å¾åˆ—æ•°: {len(feature_columns)}")
        
        return df, feature_columns
        
    def _get_feature_columns(self, df: pd.DataFrame) -> List[str]:
        """è·å–ç‰¹å¾åˆ—å"""
        config = self.data_config.config
        
        # å¦‚æœé…ç½®ä¸­æŒ‡å®šäº†ç‰¹å¾åˆ—
        if config['data_schema'].get('feature_columns'):
            return config['data_schema']['feature_columns']
            
        # è‡ªåŠ¨æ£€æµ‹ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
        feature_columns = [col for col in df.columns if '@' in col]
        
        # å¦‚æœæ²¡æœ‰@ç¬¦å·çš„åˆ—ï¼Œä½¿ç”¨æ•°å€¼åˆ—ï¼ˆæ’é™¤å¿…éœ€åˆ—å’Œæ’é™¤åˆ—ï¼‰
        if not feature_columns:
            required_cols = config['data_schema']['required_columns']
            exclude_cols = config['data_schema'].get('exclude_columns', [])
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            feature_columns = [col for col in numeric_cols 
                             if col not in required_cols and col not in exclude_cols]
        
        return feature_columns

class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, data_config: DataConfig):
        self.data_config = data_config
        self.logger = logging.getLogger(__name__)
        
    def preprocess(self, df: pd.DataFrame, feature_columns: List[str]) -> Tuple[Tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:
        """
        é¢„å¤„ç†æ•°æ®
        
        Args:
            df: åŸå§‹æ•°æ®
            feature_columns: ç‰¹å¾åˆ—å
            
        Returns:
            (å‘¨é¢‘æ•°æ®, æ—¥é¢‘æ•°æ®, æ—¥å†…æ•°æ®)å…ƒç»„å’Œæ ‡ç­¾å¼ é‡
        """
        config = self.data_config.config
        
        # å¤„ç†ç¼ºå¤±å€¼
        df_processed = df.copy()
        
        # å¡«å……ç‰¹å¾åˆ—çš„ç¼ºå¤±å€¼
        for col in feature_columns:
            if col in df_processed.columns:
                if df_processed[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                    df_processed[col] = df_processed[col].fillna(df_processed[col].median())
                else:
                    df_processed[col] = df_processed[col].fillna(0)
        
        # æå–ç‰¹å¾
        X = df_processed[feature_columns].values.astype(np.float32)
        
        # å¤„ç†æ ‡ç­¾
        target_column = config['data_schema'].get('target_column', 'class')
        if target_column in df_processed.columns:
            y = df_processed[target_column].values
        else:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡åˆ—ï¼Œä½¿ç”¨æœ€åä¸€åˆ—
            y = df_processed.iloc[:, -1].values
            
        # æ ‡ç­¾ç¼–ç 
        if y.dtype == 'object' or len(np.unique(y)) <= 10:
            # åˆ†ç±»ä»»åŠ¡
            unique_labels = np.unique(y)
            label_map = {label: idx for idx, label in enumerate(unique_labels)}
            y = np.array([label_map[label] for label in y])
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        if config['data_preprocessing'].get('normalize_features', True):
            X = self._normalize_features(X)
            
        # å°†ç‰¹å¾åˆ†å‰²ä¸ºä¸‰ä¸ªé¢‘ç‡
        # ç®€å•åˆ†å‰²ï¼šå‡è®¾ç‰¹å¾æŒ‰é¡ºåºåˆ†ä¸ºä¸‰éƒ¨åˆ†
        n_features = X.shape[1]
        weekly_size = n_features // 3
        daily_size = n_features // 3
        intraday_size = n_features - weekly_size - daily_size
        
        weekly_data = X[:, :weekly_size]
        daily_data = X[:, weekly_size:weekly_size + daily_size]
        intraday_data = X[:, weekly_size + daily_size:]
        
        # è½¬æ¢ä¸ºå¼ é‡å¹¶æ·»åŠ åºåˆ—ç»´åº¦ï¼ˆLSTMéœ€è¦3Dè¾“å…¥ï¼‰
        weekly_tensor = torch.FloatTensor(weekly_data).unsqueeze(1)  # [batch, 1, features]
        daily_tensor = torch.FloatTensor(daily_data).unsqueeze(1)
        intraday_tensor = torch.FloatTensor(intraday_data).unsqueeze(1)
        y_tensor = torch.LongTensor(y)
        
        self.logger.info(f"é¢„å¤„ç†å®Œæˆ - å‘¨é¢‘å½¢çŠ¶: {weekly_tensor.shape}, æ—¥é¢‘å½¢çŠ¶: {daily_tensor.shape}, æ—¥å†…å½¢çŠ¶: {intraday_tensor.shape}")
        self.logger.info(f"æ ‡ç­¾å½¢çŠ¶: {y_tensor.shape}, æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")
        
        return (weekly_tensor, daily_tensor, intraday_tensor), y_tensor
        
    def _normalize_features(self, X: np.ndarray) -> np.ndarray:
        """æ ‡å‡†åŒ–ç‰¹å¾"""
        # Z-scoreæ ‡å‡†åŒ–
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
        std[std == 0] = 1  # é¿å…é™¤é›¶
        X_normalized = (X - mean) / std
        return X_normalized

class Trainer:
    """è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, model_config: ModelConfig, data_config: DataConfig):
        self.model = model
        self.model_config = model_config
        self.data_config = data_config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
        self._setup_training_components()
        
    def _setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        config = self.model_config.get_training_config()
        
        # ä¼˜åŒ–å™¨
        optimizer_config = config['optimizer']
        if optimizer_config['type'] == 'adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=optimizer_config['learning_rate'],
                weight_decay=optimizer_config.get('weight_decay', 0)
            )
        elif optimizer_config['type'] == 'sgd':
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=optimizer_config['learning_rate'],
                momentum=optimizer_config.get('momentum', 0.9),
                weight_decay=optimizer_config.get('weight_decay', 0)
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {optimizer_config['type']}")
            
        # æŸå¤±å‡½æ•°
        loss_config = config['loss']
        if loss_config['type'] == 'cross_entropy':
            self.criterion = nn.CrossEntropyLoss()
        elif loss_config['type'] == 'mse':
            self.criterion = nn.MSELoss()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æŸå¤±å‡½æ•°ç±»å‹: {loss_config['type']}")
            
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler_config = config.get('scheduler')
        if scheduler_config and scheduler_config['type'] == 'step':
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=scheduler_config['step_size'],
                gamma=scheduler_config['gamma']
            )
        elif scheduler_config and scheduler_config['type'] == 'cosine':
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=scheduler_config['T_max']
            )
        else:
            self.scheduler = None
            
    def train(self, X: Tuple[torch.Tensor, torch.Tensor, torch.Tensor], y: torch.Tensor, quick_test: bool = False) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X: (å‘¨é¢‘æ•°æ®, æ—¥é¢‘æ•°æ®, æ—¥å†…æ•°æ®)å…ƒç»„
            y: æ ‡ç­¾å¼ é‡
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            è®­ç»ƒå†å²
        """
        config = self.model_config.get_training_config()
        
        # å¿«é€Ÿæµ‹è¯•æ¨¡å¼å‚æ•°è°ƒæ•´
        if quick_test:
            epochs = 2
            batch_size = 16
            validation_split = 0.2
            self.logger.info("å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šepochs=2, batch_size=16")
        else:
            epochs = config['epochs']
            batch_size = config['batch_size']
            validation_split = config.get('validation_split', 0.2)
            
        # æ•°æ®åˆ†å‰²
        weekly_data, daily_data, intraday_data = X
        dataset_size = len(weekly_data)
        val_size = int(dataset_size * validation_split)
        train_size = dataset_size - val_size
        
        dataset = TensorDataset(weekly_data, daily_data, intraday_data, y)
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        
        train_loader = TorchDataLoader(
            train_dataset, batch_size=batch_size, shuffle=True
        )
        val_loader = TorchDataLoader(
            val_dataset, batch_size=batch_size, shuffle=False
        )
        
        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        self.logger.info(f"å¼€å§‹è®­ç»ƒ - è®¾å¤‡: {self.device}, è®­ç»ƒæ ·æœ¬: {train_size}, éªŒè¯æ ·æœ¬: {val_size}")
        
        best_val_acc = 0.0
        start_time = time.time()
        
        for epoch in range(epochs):
            epoch_start_time = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self._train_epoch(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc = self._validate_epoch(val_loader)
            
            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            if self.scheduler:
                self.scheduler.step()
                
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                self._save_model('best_model.pth')
                
            epoch_time = time.time() - epoch_start_time
            
            self.logger.info(
                f"Epoch {epoch+1}/{epochs} - "
                f"train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, "
                f"val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}, "
                f"time: {epoch_time:.2f}s"
            )
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šæå‰ç»“æŸ
            if quick_test and epoch >= 0:  # è‡³å°‘è¿è¡Œ1ä¸ªepoch
                self.logger.info("å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šæå‰ç»“æŸè®­ç»ƒ")
                break
                
        total_time = time.time() - start_time
        self.logger.info(f"è®­ç»ƒå®Œæˆ - æ€»æ—¶é—´: {total_time:.2f}s, æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self._save_model('final_model.pth')
        
        return history
        
    def _train_epoch(self, train_loader: TorchDataLoader) -> Tuple[float, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (weekly, daily, intraday, target) in enumerate(train_loader):
            weekly = weekly.to(self.device)
            daily = daily.to(self.device)
            intraday = intraday.to(self.device)
            target = target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(weekly, daily, intraday)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            pred = output.argmax(dim=1)
            correct += pred.eq(target).sum().item()
            total += target.size(0)
            
        avg_loss = total_loss / len(train_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
        
    def _validate_epoch(self, val_loader: TorchDataLoader) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for weekly, daily, intraday, target in val_loader:
                weekly = weekly.to(self.device)
                daily = daily.to(self.device)
                intraday = intraday.to(self.device)
                target = target.to(self.device)
                output = self.model(weekly, daily, intraday)
                loss = self.criterion(output, target)
                
                total_loss += loss.item()
                pred = output.argmax(dim=1)
                correct += pred.eq(target).sum().item()
                total += target.size(0)
                
        avg_loss = total_loss / len(val_loader)
        accuracy = correct / total
        
        return avg_loss, accuracy
        
    def _save_model(self, filename: str):
        """ä¿å­˜æ¨¡å‹"""
        save_path = os.path.join('checkpoints', filename)
        os.makedirs('checkpoints', exist_ok=True)
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_config': self.model_config.config,
        }, save_path)
        
        self.logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

def setup_logging(log_level: str = "INFO"):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('training.log')
        ]
    )

def quick_validation():
    """å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯ä»£ç æ­£ç¡®æ€§"""
    print("ğŸš€ å¼€å§‹ training.py å¿«é€ŸéªŒè¯...")
    
    try:
        # è®¾ç½®æ—¥å¿—
        setup_logging("INFO")
        logger = logging.getLogger(__name__)
        
        # åŠ è½½é…ç½®
        logger.info("åŠ è½½é…ç½®æ–‡ä»¶...")
        model_config = ModelConfig("config.yaml")
        data_config = DataConfig("data.yaml")
        
        # åŠ è½½æ•°æ®
        logger.info("åŠ è½½æ•°æ®...")
        data_loader = CustomDataLoader(data_config)
        df, feature_columns = data_loader.load_data(quick_test=True)
        
        # é¢„å¤„ç†æ•°æ®
        logger.info("é¢„å¤„ç†æ•°æ®...")
        preprocessor = DataPreprocessor(data_config)
        X, y = preprocessor.preprocess(df, feature_columns)
        
        # åˆ›å»ºæ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹...")
        model = MixedFrequencyFactorModel(model_config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        logger.info("åˆ›å»ºè®­ç»ƒå™¨...")
        trainer = Trainer(model, model_config, data_config)
        
        # å¿«é€Ÿè®­ç»ƒ
        logger.info("å¼€å§‹å¿«é€Ÿè®­ç»ƒ...")
        history = trainer.train(X, y, quick_test=True)
        
        logger.info("âœ… å¿«é€ŸéªŒè¯æˆåŠŸå®Œæˆï¼")
        logger.info(f"è®­ç»ƒå†å²: {history}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config.yaml', help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data-config', type=str, default='data.yaml', help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--log-level', type=str, default='INFO', help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--quick-test', action='store_true', help='å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯è®­ç»ƒè„šæœ¬æ­£ç¡®æ€§')
    
    args = parser.parse_args()
    
    # å¿«é€ŸéªŒè¯æ¨¡å¼
    if args.quick_test:
        success = quick_validation()
        sys.exit(0 if success else 1)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)
    
    try:
        # åŠ è½½é…ç½®
        logger.info("åŠ è½½é…ç½®æ–‡ä»¶...")
        model_config = ModelConfig(args.config)
        data_config = DataConfig(args.data_config)
        
        # åŠ è½½æ•°æ®
        logger.info("åŠ è½½æ•°æ®...")
        data_loader = CustomDataLoader(data_config)
        df, feature_columns = data_loader.load_data()
        
        # é¢„å¤„ç†æ•°æ®
        logger.info("é¢„å¤„ç†æ•°æ®...")
        preprocessor = DataPreprocessor(data_config)
        X, y = preprocessor.preprocess(df, feature_columns)
        
        # åˆ›å»ºæ¨¡å‹
        logger.info("åˆ›å»ºæ¨¡å‹...")
        model = MixedFrequencyFactorModel(model_config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        logger.info("åˆ›å»ºè®­ç»ƒå™¨...")
        trainer = Trainer(model, model_config, data_config)
        
        # è®­ç»ƒæ¨¡å‹
        logger.info("å¼€å§‹è®­ç»ƒ...")
        history = trainer.train(X, y)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open('training_history.json', 'w') as f:
            json.dump(history, f, indent=2)
            
        logger.info("è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()