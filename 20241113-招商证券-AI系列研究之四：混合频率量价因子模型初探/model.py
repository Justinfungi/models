#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
model.py - æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹æ¶æ„å®šä¹‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import yaml
import os
import sys
from typing import Dict, Any, Optional, Tuple, List
import numpy as np
from pathlib import Path

class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»ï¼Œä»YAMLæ–‡ä»¶åŠ è½½é…ç½®"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–é…ç½®
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except FileNotFoundError:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {self.config_path} ä¸å­˜åœ¨")
        except yaml.YAMLError as e:
            raise ValueError(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
    
    def get_model_config(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        return self.config.get('model', {})
    
    def get_training_config(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒé…ç½®"""
        return self.config.get('training', {})


class MultiFrequencyEncoder(nn.Module):
    """å¤šé¢‘ç‡æ•°æ®ç¼–ç å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å¤šé¢‘ç‡ç¼–ç å™¨
        
        Args:
            config: ç¼–ç å™¨é…ç½®
        """
        super().__init__()
        self.config = config
        
        # å‘¨é¢‘ç¼–ç å™¨
        weekly_config = config.get('weekly', {})
        self.weekly_encoder = self._build_lstm_encoder(
            input_dim=weekly_config.get('input_dim', 50),
            hidden_dim=weekly_config.get('hidden_dim', 128),
            num_layers=weekly_config.get('num_layers', 2),
            dropout=weekly_config.get('dropout', 0.2),
            bidirectional=weekly_config.get('bidirectional', True)
        )
        
        # æ—¥é¢‘ç¼–ç å™¨
        daily_config = config.get('daily', {})
        self.daily_encoder = self._build_lstm_encoder(
            input_dim=daily_config.get('input_dim', 100),
            hidden_dim=daily_config.get('hidden_dim', 256),
            num_layers=daily_config.get('num_layers', 2),
            dropout=daily_config.get('dropout', 0.2),
            bidirectional=daily_config.get('bidirectional', True)
        )
        
        # æ—¥å†…ç¼–ç å™¨
        intraday_config = config.get('intraday', {})
        self.intraday_encoder = self._build_lstm_encoder(
            input_dim=intraday_config.get('input_dim', 200),
            hidden_dim=intraday_config.get('hidden_dim', 512),
            num_layers=intraday_config.get('num_layers', 2),
            dropout=intraday_config.get('dropout', 0.2),
            bidirectional=intraday_config.get('bidirectional', True)
        )
        
        # è®¡ç®—è¾“å‡ºç»´åº¦
        self.output_dim = self._calculate_output_dim()
    
    def _build_lstm_encoder(self, input_dim: int, hidden_dim: int, num_layers: int, 
                           dropout: float, bidirectional: bool) -> nn.Module:
        """æ„å»ºLSTMç¼–ç å™¨"""
        return nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional,
            batch_first=True
        )
    
    def _calculate_output_dim(self) -> int:
        """è®¡ç®—è¾“å‡ºç»´åº¦"""
        weekly_dim = self.config['weekly']['hidden_dim'] * (2 if self.config['weekly']['bidirectional'] else 1)
        daily_dim = self.config['daily']['hidden_dim'] * (2 if self.config['daily']['bidirectional'] else 1)
        intraday_dim = self.config['intraday']['hidden_dim'] * (2 if self.config['intraday']['bidirectional'] else 1)
        return weekly_dim + daily_dim + intraday_dim
    
    def forward(self, weekly_data: torch.Tensor, daily_data: torch.Tensor, 
                intraday_data: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            weekly_data: å‘¨é¢‘æ•°æ® [batch_size, seq_len, weekly_features]
            daily_data: æ—¥é¢‘æ•°æ® [batch_size, seq_len, daily_features]
            intraday_data: æ—¥å†…æ•°æ® [batch_size, seq_len, intraday_features]
            
        Returns:
            ç¼–ç åçš„ç‰¹å¾ [batch_size, combined_features]
        """
        # å‘¨é¢‘ç¼–ç 
        weekly_output, (weekly_hidden, _) = self.weekly_encoder(weekly_data)
        weekly_features = weekly_hidden[-1] if not self.config['weekly']['bidirectional'] else \
                         torch.cat([weekly_hidden[-2], weekly_hidden[-1]], dim=1)
        
        # æ—¥é¢‘ç¼–ç 
        daily_output, (daily_hidden, _) = self.daily_encoder(daily_data)
        daily_features = daily_hidden[-1] if not self.config['daily']['bidirectional'] else \
                        torch.cat([daily_hidden[-2], daily_hidden[-1]], dim=1)
        
        # æ—¥å†…ç¼–ç 
        intraday_output, (intraday_hidden, _) = self.intraday_encoder(intraday_data)
        intraday_features = intraday_hidden[-1] if not self.config['intraday']['bidirectional'] else \
                           torch.cat([intraday_hidden[-2], intraday_hidden[-1]], dim=1)
        
        # æ‹¼æ¥æ‰€æœ‰é¢‘ç‡ç‰¹å¾
        combined_features = torch.cat([weekly_features, daily_features, intraday_features], dim=1)
        
        return combined_features


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            config: æ³¨æ„åŠ›é…ç½®
        """
        super().__init__()
        self.num_heads = config.get('num_heads', 8)
        self.hidden_dim = config.get('hidden_dim', 512)
        self.dropout = config.get('dropout', 0.1)
        self.use_residual = config.get('use_residual', True)
        
        assert self.hidden_dim % self.num_heads == 0
        self.head_dim = self.hidden_dim // self.num_heads
        
        self.query_proj = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.key_proj = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.value_proj = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.output_proj = nn.Linear(self.hidden_dim, self.hidden_dim)
        
        self.dropout_layer = nn.Dropout(self.dropout)
        self.layer_norm = nn.LayerNorm(self.hidden_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, seq_len, hidden_dim]
            
        Returns:
            æ³¨æ„åŠ›è¾“å‡º [batch_size, seq_len, hidden_dim]
        """
        batch_size, seq_len, _ = x.shape
        residual = x
        
        # è®¡ç®—Q, K, V
        Q = self.query_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout_layer(attention_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›
        attention_output = torch.matmul(attention_weights, V)
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_dim
        )
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_proj(attention_output)
        output = self.dropout_layer(output)
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        if self.use_residual:
            output = self.layer_norm(output + residual)
        else:
            output = self.layer_norm(output)
            
        return output


class FactorExtractor(nn.Module):
    """å› å­æå–å™¨"""
    
    def __init__(self, input_dim: int, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–å› å­æå–å™¨
        
        Args:
            input_dim: è¾“å…¥ç»´åº¦
            config: å› å­æå–å™¨é…ç½®
        """
        super().__init__()
        self.num_factors = config.get('num_factors', 64)
        self.factor_dim = config.get('factor_dim', 128)
        self.num_layers = config.get('num_layers', 3)
        self.activation = config.get('activation', 'relu')
        self.dropout = config.get('dropout', 0.3)
        self.use_batch_norm = config.get('use_batch_norm', True)
        
        # æ„å»ºå› å­æå–ç½‘ç»œ
        layers = []
        current_dim = input_dim
        
        for i in range(self.num_layers):
            # çº¿æ€§å±‚
            if i == self.num_layers - 1:
                # æœ€åä¸€å±‚è¾“å‡ºå› å­
                layers.append(nn.Linear(current_dim, self.num_factors * self.factor_dim))
            else:
                next_dim = max(self.factor_dim, current_dim // 2)
                layers.append(nn.Linear(current_dim, next_dim))
                current_dim = next_dim
            
            # æ‰¹å½’ä¸€åŒ–
            if self.use_batch_norm and i < self.num_layers - 1:
                layers.append(nn.BatchNorm1d(current_dim))
            
            # æ¿€æ´»å‡½æ•°
            if i < self.num_layers - 1:
                if self.activation == 'relu':
                    layers.append(nn.ReLU())
                elif self.activation == 'gelu':
                    layers.append(nn.GELU())
                elif self.activation == 'tanh':
                    layers.append(nn.Tanh())
            
            # Dropout
            if i < self.num_layers - 1:
                layers.append(nn.Dropout(self.dropout))
        
        self.factor_network = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, input_dim]
            
        Returns:
            æå–çš„å› å­ [batch_size, num_factors, factor_dim]
        """
        batch_size = x.shape[0]
        factors = self.factor_network(x)
        factors = factors.view(batch_size, self.num_factors, self.factor_dim)
        return factors


class Classifier(nn.Module):
    """åˆ†ç±»å™¨"""
    
    def __init__(self, input_dim: int, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨
        
        Args:
            input_dim: è¾“å…¥ç»´åº¦
            config: åˆ†ç±»å™¨é…ç½®
        """
        super().__init__()
        self.hidden_dims = config.get('hidden_dims', [512, 256, 128])
        self.dropout = config.get('dropout', 0.4)
        self.activation = config.get('activation', 'relu')
        self.use_batch_norm = config.get('use_batch_norm', True)
        self.output_activation = config.get('output_activation', 'softmax')
        self.num_classes = config.get('num_classes', 2)
        
        # æ„å»ºåˆ†ç±»ç½‘ç»œ
        layers = []
        current_dim = input_dim
        
        for i, hidden_dim in enumerate(self.hidden_dims):
            layers.append(nn.Linear(current_dim, hidden_dim))
            
            if self.use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            
            if self.activation == 'relu':
                layers.append(nn.ReLU())
            elif self.activation == 'gelu':
                layers.append(nn.GELU())
            elif self.activation == 'tanh':
                layers.append(nn.Tanh())
            
            layers.append(nn.Dropout(self.dropout))
            current_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(current_dim, self.num_classes))
        
        self.classifier = nn.Sequential(*layers)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            x: è¾“å…¥ç‰¹å¾ [batch_size, input_dim]
            
        Returns:
            åˆ†ç±»æ¦‚ç‡ [batch_size, num_classes]
        """
        logits = self.classifier(x)
        
        if self.output_activation == 'softmax':
            return F.softmax(logits, dim=1)
        elif self.output_activation == 'sigmoid':
            return torch.sigmoid(logits)
        else:
            return logits


class MixedFrequencyFactorModel(nn.Module):
    """æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹"""
    
    def __init__(self, config: ModelConfig):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            config: æ¨¡å‹é…ç½®
        """
        super().__init__()
        self.config = config
        model_config = config.get_model_config()
        
        self.model_name = model_config.get('name', 'mixed_frequency_factor_model')
        self.model_type = model_config.get('type', 'classification')
        self.num_classes = model_config.get('num_classes', 2)
        
        # å¤šé¢‘ç‡ç¼–ç å™¨
        encoder_config = model_config.get('multi_frequency_encoder', {})
        self.multi_freq_encoder = MultiFrequencyEncoder(encoder_config)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_config = model_config.get('attention', {})
        attention_config['hidden_dim'] = self.multi_freq_encoder.output_dim
        self.attention = MultiHeadAttention(attention_config)
        
        # å› å­æå–å™¨
        factor_config = model_config.get('factor_extractor', {})
        self.factor_extractor = FactorExtractor(self.multi_freq_encoder.output_dim, factor_config)
        
        # åˆ†ç±»å™¨
        classifier_config = model_config.get('classifier', {})
        classifier_config['num_classes'] = self.num_classes
        factor_output_dim = factor_config.get('num_factors', 64) * factor_config.get('factor_dim', 128)
        self.classifier = Classifier(factor_output_dim, classifier_config)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.LSTM):
                for name, param in module.named_parameters():
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in name:
                        nn.init.constant_(param, 0)
    
    def forward(self, weekly_data: torch.Tensor, daily_data: torch.Tensor, 
                intraday_data: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            weekly_data: å‘¨é¢‘æ•°æ®
            daily_data: æ—¥é¢‘æ•°æ®
            intraday_data: æ—¥å†…æ•°æ®
            
        Returns:
            åˆ†ç±»æ¦‚ç‡
        """
        # å¤šé¢‘ç‡ç¼–ç 
        encoded_features = self.multi_freq_encoder(weekly_data, daily_data, intraday_data)
        
        # æ·»åŠ åºåˆ—ç»´åº¦ç”¨äºæ³¨æ„åŠ›è®¡ç®—
        if len(encoded_features.shape) == 2:
            encoded_features = encoded_features.unsqueeze(1)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attended_features = self.attention(encoded_features)
        attended_features = attended_features.squeeze(1)
        
        # å› å­æå–
        factors = self.factor_extractor(attended_features)
        factors_flat = factors.view(factors.shape[0], -1)
        
        # åˆ†ç±»
        output = self.classifier(factors_flat)
        
        return output
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': self.model_name,
            'model_type': self.model_type,
            'num_classes': self.num_classes,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'encoder_output_dim': self.multi_freq_encoder.output_dim,
        }


def create_model(config_path: str = "config.yaml") -> MixedFrequencyFactorModel:
    """
    åˆ›å»ºæ¨¡å‹å®ä¾‹
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ¨¡å‹å®ä¾‹
    """
    try:
        config = ModelConfig(config_path)
        model = MixedFrequencyFactorModel(config)
        return model
    except Exception as e:
        raise RuntimeError(f"åˆ›å»ºæ¨¡å‹å¤±è´¥: {e}")


def load_model(model_path: str, config_path: str = "config.yaml") -> MixedFrequencyFactorModel:
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŠ è½½çš„æ¨¡å‹
    """
    try:
        model = create_model(config_path)
        checkpoint = torch.load(model_path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        return model
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ model.py å¿«é€ŸéªŒè¯...")
    
    try:
        # 1. æµ‹è¯•é…ç½®åŠ è½½
        print("ğŸ“‹ æµ‹è¯•é…ç½®åŠ è½½...")
        config_path = "config.yaml"
        if not os.path.exists(config_path):
            print(f"âš ï¸  é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡é…ç½®æµ‹è¯•")
            # åˆ›å»ºæœ€å°é…ç½®ç”¨äºæµ‹è¯•
            config = ModelConfig.__new__(ModelConfig)
            config.config = {
                'model': {
                    'name': 'test_model',
                    'type': 'classification',
                    'num_classes': 2,
                    'multi_frequency_encoder': {
                        'weekly': {'input_dim': 20, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.1, 'bidirectional': True},
                        'daily': {'input_dim': 30, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.1, 'bidirectional': True},
                        'intraday': {'input_dim': 40, 'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.1, 'bidirectional': True}
                    },
                    'attention': {'num_heads': 4, 'hidden_dim': 384, 'dropout': 0.1, 'use_residual': True},
                    'factor_extractor': {'num_factors': 32, 'factor_dim': 64, 'num_layers': 2, 'activation': 'relu', 'dropout': 0.2, 'use_batch_norm': True},
                    'classifier': {'hidden_dims': [256, 128], 'dropout': 0.3, 'activation': 'relu', 'use_batch_norm': True, 'output_activation': 'softmax'}
                }
            }
        else:
            config = ModelConfig(config_path)
        print("âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # 2. æµ‹è¯•æ¨¡å‹åˆ›å»º
        print("ğŸ—ï¸  æµ‹è¯•æ¨¡å‹åˆ›å»º...")
        model = MixedFrequencyFactorModel(config)
        print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
        # 3. æµ‹è¯•è®¾å¤‡é€‚é…
        print("ğŸ–¥ï¸  æµ‹è¯•è®¾å¤‡é€‚é…...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = model.to(device)
        print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
        
        # 4. æµ‹è¯•å‰å‘ä¼ æ’­
        print("âš¡ æµ‹è¯•å‰å‘ä¼ æ’­...")
        batch_size = 4
        seq_len = 10
        
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        model_config = config.get_model_config()
        encoder_config = model_config.get('multi_frequency_encoder', {})
        
        weekly_dim = encoder_config.get('weekly', {}).get('input_dim', 20)
        daily_dim = encoder_config.get('daily', {}).get('input_dim', 30)
        intraday_dim = encoder_config.get('intraday', {}).get('input_dim', 40)
        
        weekly_data = torch.randn(batch_size, seq_len, weekly_dim).to(device)
        daily_data = torch.randn(batch_size, seq_len, daily_dim).to(device)
        intraday_data = torch.randn(batch_size, seq_len, intraday_dim).to(device)
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = model(weekly_data, daily_data, intraday_data)
        
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"ğŸ“Š è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
        
        # 5. æ‰“å°æ¨¡å‹ä¿¡æ¯
        print("ğŸ“ˆ æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯:")
        model_info = model.get_model_info()
        for key, value in model_info.items():
            print(f"   {key}: {value}")
        
        # 6. æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
        print("ğŸ’¾ æµ‹è¯•æ¨¡å‹ä¿å­˜...")
        temp_model_path = "temp_model.pth"
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config.config
        }, temp_model_path)
        print("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_model_path):
            os.remove(temp_model_path)
            print("ğŸ—‘ï¸  ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        
        print("âœ… model.py éªŒè¯é€šè¿‡!")
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å‡æˆåŠŸå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)