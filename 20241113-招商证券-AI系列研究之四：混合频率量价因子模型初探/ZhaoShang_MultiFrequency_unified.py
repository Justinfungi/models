#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
20241113_æ‹›å•†è¯åˆ¸_AIç³»åˆ—ç ”ç©¶ä¹‹å››_æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹åˆæ¢_unified.py - ç»Ÿä¸€æ¨¡å‹å®ç°
åŸºäºæ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹çš„ç»Ÿä¸€å®ç°ï¼Œæ”¯æŒå¤šé¢‘ç‡æ•°æ®èåˆå’ŒOptunaè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class MultiFrequencyConfig:
    """æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹é…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "MultiFrequencyModel"
    model_type: str = "classification"
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.001
    batch_size: int = 64
    epochs: int = 100
    weight_decay: float = 1e-4
    dropout: float = 0.2
    
    # æ¨¡å‹æ¶æ„é…ç½®
    hidden_dims: List[int] = None
    frequency_dims: Dict[str, int] = None
    attention_heads: int = 8
    attention_dim: int = 64
    
    # æ•°æ®é…ç½®
    sequence_length: int = 20
    feature_dim: int = 10
    num_classes: int = 2
    
    # è®¾å¤‡å’Œä¼˜åŒ–é…ç½®
    device: str = "auto"
    mixed_precision: bool = True
    gradient_clip: float = 1.0
    
    # æ—¥å¿—å’Œä¿å­˜é…ç½®
    log_dir: str = "./logs"
    checkpoint_dir: str = "./checkpoints"
    save_best_only: bool = True
    early_stopping_patience: int = 10
    
    # éªŒè¯é…ç½®
    validation_split: float = 0.2
    test_split: float = 0.1
    
    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [256, 128, 64]
        if self.frequency_dims is None:
            self.frequency_dims = {
                "daily": 64,
                "weekly": 32,
                "monthly": 16
            }

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    if device_choice == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
            print(f"ğŸš€ ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
        else:
            device = torch.device("cpu")
            print("ğŸ’» ä½¿ç”¨CPU")
    else:
        device = torch.device(device_choice)
        print(f"ğŸ¯ æŒ‡å®šè®¾å¤‡: {device}")
    
    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    os.makedirs(log_dir, exist_ok=True)
    
    if log_filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_filename = f"{prefix}_{timestamp}.log"
    
    log_path = os.path.join(log_dir, log_filename)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_path}")
    return logger

def create_directories(config: MultiFrequencyConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    os.makedirs(config.log_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)

# =============================================================================
# æ•°æ®å¤„ç†ç±»
# =============================================================================

class MultiFrequencyDataLoaderManager:
    """å¤šé¢‘ç‡æ•°æ®åŠ è½½ç®¡ç†å™¨"""
    
    def __init__(self, data_config_path: str, config: MultiFrequencyConfig):
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.scaler = StandardScaler()
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âš ï¸ æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_data_config()
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®é…ç½®å¤±è´¥: {e}")
            return self._get_default_data_config()
    
    def _get_default_data_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤æ•°æ®é…ç½®"""
        return {
            "data_source": "synthetic",
            "features": {
                "daily": ["price", "volume", "return"],
                "weekly": ["price_avg", "volume_avg"],
                "monthly": ["trend", "volatility"]
            },
            "target": "label",
            "sequence_length": 20
        }
    
    def get_data_loaders(self) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """è·å–è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®åŠ è½½å™¨"""
        train_df, val_df, test_df = self._load_real_dataframes()
        
        train_loader = self._create_dataloader(train_df, shuffle=True, batch_size=self.config.batch_size)
        val_loader = self._create_dataloader(val_df, shuffle=False, batch_size=self.config.batch_size)
        test_loader = self._create_dataloader(test_df, shuffle=False, batch_size=self.config.batch_size)
        
        return train_loader, val_loader, test_loader
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_config.get("data_split", {})
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=2,
            pin_memory=True if torch.cuda.is_available() else False,
            drop_last=True if shuffle else False
        )
    
    def _load_real_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """åŠ è½½çœŸå®æ•°æ®"""
        try:
            # å°è¯•åŠ è½½çœŸå®æ•°æ®
            data_path = self.data_config.get("data_path", "./data")
            if os.path.exists(data_path):
                # è¿™é‡Œåº”è¯¥å®ç°çœŸå®çš„æ•°æ®åŠ è½½é€»è¾‘
                print("ğŸ“Š åŠ è½½çœŸå®æ•°æ®...")
                return self._generate_synthetic_data()
            else:
                print("âš ï¸ æ•°æ®è·¯å¾„ä¸å­˜åœ¨ï¼Œç”Ÿæˆåˆæˆæ•°æ®")
                return self._generate_synthetic_data()
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨åˆæˆæ•°æ®")
            return self._generate_synthetic_data()
    
    def _generate_synthetic_data(self) -> Tuple[TensorDataset, TensorDataset, TensorDataset]:
        """ç”Ÿæˆåˆæˆæ•°æ®ç”¨äºæµ‹è¯•"""
        print("ğŸ² ç”Ÿæˆåˆæˆå¤šé¢‘ç‡æ•°æ®...")
        
        # ç”Ÿæˆå¤šé¢‘ç‡ç‰¹å¾æ•°æ®
        n_samples = 10000
        seq_len = self.config.sequence_length
        
        # æ—¥é¢‘æ•°æ®
        daily_features = torch.randn(n_samples, seq_len, self.config.frequency_dims["daily"])
        # å‘¨é¢‘æ•°æ®
        weekly_features = torch.randn(n_samples, seq_len//5, self.config.frequency_dims["weekly"])
        # æœˆé¢‘æ•°æ®
        monthly_features = torch.randn(n_samples, seq_len//20, self.config.frequency_dims["monthly"])
        
        # ç»„åˆç‰¹å¾
        features = {
            "daily": daily_features,
            "weekly": weekly_features,
            "monthly": monthly_features
        }
        
        # ç”Ÿæˆæ ‡ç­¾
        labels = torch.randint(0, self.config.num_classes, (n_samples,))
        
        # æ•°æ®åˆ†å‰²
        train_size = int(0.7 * n_samples)
        val_size = int(0.2 * n_samples)
        
        train_features = {k: v[:train_size] for k, v in features.items()}
        val_features = {k: v[train_size:train_size+val_size] for k, v in features.items()}
        test_features = {k: v[train_size+val_size:] for k, v in features.items()}
        
        train_labels = labels[:train_size]
        val_labels = labels[train_size:train_size+val_size]
        test_labels = labels[train_size+val_size:]
        
        train_dataset = TensorDataset(train_features["daily"], train_features["weekly"], 
                                    train_features["monthly"], train_labels)
        val_dataset = TensorDataset(val_features["daily"], val_features["weekly"], 
                                  val_features["monthly"], val_labels)
        test_dataset = TensorDataset(test_features["daily"], test_features["weekly"], 
                                   test_features["monthly"], test_labels)
        
        return train_dataset, val_dataset, test_dataset

# =============================================================================
# æ¨¡å‹æ¶æ„
# =============================================================================

class BaseModel(nn.Module, ABC):
    """åŸºç¡€æ¨¡å‹æŠ½è±¡ç±»"""
    
    def __init__(self, config: MultiFrequencyConfig):
        super().__init__()
        self.config = config
    
    @abstractmethod
    def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        }

class MultiFrequencyEncoder(nn.Module):
    """å¤šé¢‘ç‡ç¼–ç å™¨"""
    
    def __init__(self, input_dim: int, hidden_dim: int, num_layers: int = 2):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.1)
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)
        self.norm = nn.LayerNorm(hidden_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # LSTMç¼–ç 
        lstm_out, _ = self.lstm(x)
        
        # è‡ªæ³¨æ„åŠ›æœºåˆ¶
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
        output = self.norm(lstm_out + attn_out)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        return output.mean(dim=1)

class MultiFrequencyModel(BaseModel):
    """æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹"""
    
    def __init__(self, config: MultiFrequencyConfig):
        super().__init__(config)
        
        # å¤šé¢‘ç‡ç¼–ç å™¨
        self.daily_encoder = MultiFrequencyEncoder(
            config.frequency_dims["daily"], 
            config.hidden_dims[0]
        )
        self.weekly_encoder = MultiFrequencyEncoder(
            config.frequency_dims["weekly"], 
            config.hidden_dims[1]
        )
        self.monthly_encoder = MultiFrequencyEncoder(
            config.frequency_dims["monthly"], 
            config.hidden_dims[2]
        )
        
        # ç‰¹å¾èåˆå±‚
        total_dim = sum(config.hidden_dims)
        self.fusion_layers = nn.Sequential(
            nn.Linear(total_dim, config.hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.hidden_dims[0], config.hidden_dims[1]),
            nn.ReLU(),
            nn.Dropout(config.dropout)
        )
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(config.hidden_dims[1], config.num_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.LSTM):
                for name, param in m.named_parameters():
                    if 'weight' in name:
                        nn.init.xavier_uniform_(param)
                    elif 'bias' in name:
                        nn.init.zeros_(param)
    
    def forward(self, daily: torch.Tensor, weekly: torch.Tensor, monthly: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # å¤šé¢‘ç‡ç¼–ç 
        daily_features = self.daily_encoder(daily)
        weekly_features = self.weekly_encoder(weekly)
        monthly_features = self.monthly_encoder(monthly)
        
        # ç‰¹å¾èåˆ
        combined_features = torch.cat([daily_features, weekly_features, monthly_features], dim=1)
        fused_features = self.fusion_layers(combined_features)
        
        # åˆ†ç±»
        logits = self.classifier(fused_features)
        
        return logits

# =============================================================================
# è®­ç»ƒå™¨
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, config: MultiFrequencyConfig, device: torch.device):
        self.model = model.to(device)
        self.config = config
        self.device = device
        self.logger = logging.getLogger(__name__)
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = optim.AdamW(
            model.parameters(),
            lr=config.learning_rate,
            weight_decay=config.weight_decay
        )
        
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5, verbose=True
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config.mixed_precision else None
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_loss = float('inf')
        self.best_val_auc = 0.0
        self.patience_counter = 0
        self.training_history = {
            'train_losses': [],
            'val_losses': [],
            'train_accuracies': [],
            'val_accuracies': [],
            'train_aucs': [],
            'val_aucs': []
        }
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        for batch_idx, (daily, weekly, monthly, labels) in enumerate(train_loader):
            daily = daily.to(self.device)
            weekly = weekly.to(self.device)
            monthly = monthly.to(self.device)
            labels = labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            if self.config.mixed_precision and self.scaler:
                with autocast():
                    outputs = self.model(daily, weekly, monthly)
                    loss = self.criterion(outputs, labels)
                
                self.scaler.scale(loss).backward()
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                outputs = self.model(daily, weekly, monthly)
                loss = self.criterion(outputs, labels)
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip)
                self.optimizer.step()
            
            total_loss += loss.item()
            
            # æ”¶é›†é¢„æµ‹ç»“æœ
            preds = torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())
        
        avg_loss = total_loss / len(train_loader)
        accuracy = accuracy_score(all_labels, np.array(all_preds) > 0.5)
        auc = roc_auc_score(all_labels, all_preds)
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'auc': auc
        }
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for daily, weekly, monthly, labels in val_loader:
                daily = daily.to(self.device)
                weekly = weekly.to(self.device)
                monthly = monthly.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(daily, weekly, monthly)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                preds = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()
                all_preds.extend(preds)
                all_labels.extend(labels.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        accuracy = accuracy_score(all_labels, np.array(all_preds) > 0.5)
        auc = roc_auc_score(all_labels, all_preds)
        
        return {
            'loss': avg_loss,
            'accuracy': accuracy,
            'auc': auc,
            'y_true_val': all_labels,
            'y_prob_val': all_preds
        }
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        start_time = time.time()
        
        for epoch in range(self.config.epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_metrics['loss'])
            
            # è®°å½•å†å²
            self.training_history['train_losses'].append(train_metrics['loss'])
            self.training_history['val_losses'].append(val_metrics['loss'])
            self.training_history['train_accuracies'].append(train_metrics['accuracy'])
            self.training_history['val_accuracies'].append(val_metrics['accuracy'])
            self.training_history['train_aucs'].append(train_metrics['auc'])
            self.training_history['val_aucs'].append(val_metrics['auc'])
            
            epoch_time = time.time() - epoch_start
            
            # æ‰“å°è¿›åº¦
            self.logger.info(
                f"Epoch {epoch+1}/{self.config.epochs} - "
                f"Train Loss: {train_metrics['loss']:.4f}, Train AUC: {train_metrics['auc']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}, Val AUC: {val_metrics['auc']:.4f}, "
                f"Time: {epoch_time:.2f}s"
            )
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_metrics['auc'] > self.best_val_auc:
                self.best_val_auc = val_metrics['auc']
                self.best_val_loss = val_metrics['loss']
                self.patience_counter = 0
                self._save_checkpoint(epoch, val_metrics)
                self.logger.info(f"æ–°çš„æœ€ä½³æ¨¡å‹ï¼Val AUC: {self.best_val_auc:.4f}")
            else:
                self.patience_counter += 1
            
            # æ—©åœæ£€æŸ¥
            if self.patience_counter >= self.config.early_stopping_patience:
                self.logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨epoch {epoch+1}")
                break
        
        total_time = time.time() - start_time
        self.logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ€»æ—¶é—´: {total_time:.2f}s")
        
        # æ·»åŠ æœ€ç»ˆéªŒè¯ç»“æœåˆ°å†å²è®°å½•
        final_val_metrics = self.validate_epoch(val_loader)
        self.training_history.update({
            'final_train_acc': train_metrics['accuracy'],
            'final_val_acc': final_val_metrics['accuracy'],
            'train_auc': train_metrics['auc'],
            'val_auc': final_val_metrics['auc'],
            'y_true_val': final_val_metrics['y_true_val'],
            'y_prob_val': final_val_metrics['y_prob_val']
        })
        
        return self.training_history
    
    def _save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_auc': self.best_val_auc,
            'config': asdict(self.config),
            'metrics': metrics
        }
        
        checkpoint_path = os.path.join(self.config.checkpoint_dir, 'best_model.pth')
        torch.save(checkpoint, checkpoint_path)

# =============================================================================
# æ¨ç†å™¨
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, model: nn.Module, config: MultiFrequencyConfig, device: torch.device):
        self.model = model.to(device)
        self.config = config
        self.device = device
        self.logger = logging.getLogger(__name__)
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½å®Œæˆ: {checkpoint_path}")
    
    def predict(self, test_loader: DataLoader) -> Dict[str, Any]:
        """æ¨¡å‹é¢„æµ‹"""
        self.model.eval()
        all_preds = []
        all_probs = []
        all_labels = []
        
        with torch.no_grad():
            for daily, weekly, monthly, labels in test_loader:
                daily = daily.to(self.device)
                weekly = weekly.to(self.device)
                monthly = monthly.to(self.device)
                
                outputs = self.model(daily, weekly, monthly)
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_probs.extend(probs[:, 1].cpu().numpy())  # æ­£ç±»æ¦‚ç‡
                all_labels.extend(labels.numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_preds)
        auc = roc_auc_score(all_labels, all_probs)
        precision = precision_score(all_labels, all_preds, average='weighted')
        recall = recall_score(all_labels, all_preds, average='weighted')
        f1 = f1_score(all_labels, all_preds, average='weighted')
        
        results = {
            'predictions': all_preds,
            'probabilities': all_probs,
            'labels': all_labels,
            'metrics': {
                'accuracy': accuracy,
                'auc': auc,
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            }
        }
        
        self.logger.info(f"é¢„æµ‹å®Œæˆ - AUC: {auc:.4f}, Accuracy: {accuracy:.4f}")
        return results

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: MultiFrequencyConfig):
        self.config = config
    
    def generate_model_report(self, model: nn.Module, training_history: Dict[str, Any], 
                            save_path: str = "model_report.md") -> str:
        """ç”Ÿæˆæ¨¡å‹æŠ¥å‘Š"""
        model_info = model.get_model_info() if hasattr(model, 'get_model_info') else {}
        
        report = f"""# æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹æŠ¥å‘Š

## æ¨¡å‹æ¦‚è¿°
- **æ¨¡å‹åç§°**: {self.config.model_name}
- **æ¨¡å‹ç±»å‹**: {self.config.model_type}
- **åˆ›å»ºæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ¨¡å‹æ¶æ„
- **æ€»å‚æ•°é‡**: {model_info.get('total_parameters', 'N/A')}
- **å¯è®­ç»ƒå‚æ•°**: {model_info.get('trainable_parameters', 'N/A')}
- **æ¨¡å‹å¤§å°**: {model_info.get('model_size_mb', 'N/A'):.2f} MB

## è®­ç»ƒé…ç½®
- **å­¦ä¹ ç‡**: {self.config.learning_rate}
- **æ‰¹æ¬¡å¤§å°**: {self.config.batch_size}
- **è®­ç»ƒè½®æ•°**: {self.config.epochs}
- **æƒé‡è¡°å‡**: {self.config.weight_decay}
- **Dropout**: {self.config.dropout}

## è®­ç»ƒç»“æœ
- **æœ€ç»ˆè®­ç»ƒAUC**: {training_history.get('train_auc', 'N/A'):.4f}
- **æœ€ç»ˆéªŒè¯AUC**: {training_history.get('val_auc', 'N/A'):.4f}
- **æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡**: {training_history.get('final_train_acc', 'N/A'):.4f}
- **æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡**: {training_history.get('final_val_acc', 'N/A'):.4f}

## æ¨¡å‹ç‰¹ç‚¹
- æ”¯æŒå¤šé¢‘ç‡æ•°æ®èåˆï¼ˆæ—¥é¢‘ã€å‘¨é¢‘ã€æœˆé¢‘ï¼‰
- ä½¿ç”¨LSTMå’Œæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ—¶åºå»ºæ¨¡
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒå’Œæ¢¯åº¦è£å‰ª
- é›†æˆæ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦

## ä½¿ç”¨è¯´æ˜
```python
# è®­ç»ƒæ¨¡å¼
python {os.path.basename(__file__)} --mode train --config config.yaml --data data.yaml

# æ¨ç†æ¨¡å¼
python {os.path.basename(__file__)} --mode inference --config config.yaml --data data.yaml --checkpoint best_model.pth
```
"""
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return report

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               seed: int = 42, device: str = "auto") -> Dict[str, Any]:
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    # è®¾ç½®éšæœºç§å­
    set_all_seeds(seed)
    
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        config = MultiFrequencyConfig(**config_dict.get('model', {}))
    except:
        print("âš ï¸ ä½¿ç”¨é»˜è®¤é…ç½®")
        config = MultiFrequencyConfig()
    
    # è®¾ç½®è®¾å¤‡å’Œæ—¥å¿—
    device = setup_device(device)
    logger = setup_logging(config.log_dir, "multifreq_train")
    create_directories(config)
    
    # æ•°æ®åŠ è½½
    data_manager = MultiFrequencyDataLoaderManager(data_config_path, config)
    train_loader, val_loader, test_loader = data_manager.get_data_loaders()
    
    # æ¨¡å‹åˆå§‹åŒ–
    model = MultiFrequencyModel(config)
    logger.info(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆ: {model.get_model_info()}")
    
    # è®­ç»ƒ
    trainer = UnifiedTrainer(model, config, device)
    training_history = trainer.train(train_loader, val_loader)
    
    # ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config)
    doc_generator.generate_model_report(model, training_history)
    
    logger.info("è®­ç»ƒå®Œæˆï¼")
    return training_history

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "checkpoints/best_model.pth", device: str = "auto") -> Dict[str, Any]:
    """ä¸»æ¨ç†å‡½æ•°"""
    
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        config = MultiFrequencyConfig(**config_dict.get('model', {}))
    except:
        print("âš ï¸ ä½¿ç”¨é»˜è®¤é…ç½®")
        config = MultiFrequencyConfig()
    
    # è®¾ç½®è®¾å¤‡å’Œæ—¥å¿—
    device = setup_device(device)
    logger = setup_logging(config.log_dir, "multifreq_inference")
    
    # æ•°æ®åŠ è½½
    data_manager = MultiFrequencyDataLoaderManager(data_config_path, config)
    _, _, test_loader = data_manager.get_data_loaders()
    
    # æ¨¡å‹å’Œæ¨ç†å™¨
    model = MultiFrequencyModel(config)
    inferencer = UnifiedInferencer(model, config, device)
    inferencer.load_checkpoint(checkpoint_path)
    
    # æ¨ç†
    results = inferencer.predict(test_loader)
    
    logger.info("æ¨ç†å®Œæˆï¼")
    return results

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ç”Ÿæˆæ–‡æ¡£"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
        config = MultiFrequencyConfig(**config_dict.get('model', {}))
    except:
        config = MultiFrequencyConfig()
    
    model = MultiFrequencyModel(config)
    doc_generator = ModelDocumentationGenerator(config)
    
    # ç”ŸæˆåŸºç¡€æ–‡æ¡£
    dummy_history = {
        'train_auc': 0.85,
        'val_auc': 0.82,
        'final_train_acc': 0.78,
        'final_val_acc': 0.75
    }
    
    doc_generator.generate_model_report(model, dummy_history, "MODEL_DOCUMENTATION.md")
    print("ğŸ“š æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå®Œæˆ: MODEL_DOCUMENTATION.md")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹ç»Ÿä¸€å®ç°")
    parser.add_argument("--mode", type=str, default="train", choices=["train", "inference", "docs"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", type=str, default="config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data", type=str, default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", type=str, default="checkpoints/best_model.pth", help="æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡é€‰æ‹©")
    
    args = parser.parse_args()
    
    if args.mode == "train":
        main_train(args.config, args.data, args.seed, args.device)
    elif args.mode == "inference":
        main_inference(args.config, args.data, args.checkpoint, args.device)
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data)
