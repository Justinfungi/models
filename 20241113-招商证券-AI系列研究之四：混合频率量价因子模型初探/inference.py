#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
inference.py - æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹æ¨ç†æ¨¡å—
"""

import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import yaml
import argparse
import os
import sys
import time
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple, Union
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

from model import MixedFrequencyFactorModel, ModelConfig

class InferenceEngine:
    """æ¨ç†å¼•æ“ç±»"""
    
    def __init__(self, config_path: str = "config.yaml", model_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        """
        self.config = ModelConfig(config_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.model_path = model_path
        self.data_config = self._load_data_config()
        
        print(f"ğŸ”§ æ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®"""
        try:
            with open("data.yaml", 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print("âš ï¸ data.yaml æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {
                'data_source': {
                    'train_path': "../../data/feature_set/mrmr_data_task_06_2016-01-01_2021-07-01.pq",
                    'format': 'parquet'
                },
                'data_schema': {
                    'required_columns': ['date', 'instrument', 'label'],
                    'target_column': 'class'
                }
            }
    
    def load_model(self, model_path: Optional[str] = None) -> None:
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
        """
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = MixedFrequencyFactorModel(self.config)
            self.model.to(self.device)
            
            # åŠ è½½æƒé‡
            if model_path is None:
                model_path = self.model_path
                
            if model_path and os.path.exists(model_path):
                checkpoint = torch.load(model_path, map_location=self.device)
                if 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    self.model.load_state_dict(checkpoint)
                print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {model_path}")
            else:
                print("âš ï¸ æ¨¡å‹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
                
            self.model.eval()
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def load_data(self, data_path: Optional[str] = None, quick_test: bool = False) -> pd.DataFrame:
        """
        åŠ è½½æ¨ç†æ•°æ®
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            åŠ è½½çš„æ•°æ®DataFrame
        """
        try:
            if data_path is None:
                data_path = self.data_config['data_source']['train_path']
            
            # å°è¯•å¤šä¸ªå¯èƒ½çš„è·¯å¾„
            possible_paths = [data_path]
            if 'alternative_paths' in self.data_config['data_source']:
                possible_paths.extend(self.data_config['data_source']['alternative_paths'])
            
            data = None
            for path in possible_paths:
                if os.path.exists(path):
                    try:
                        if path.endswith('.pq') or path.endswith('.parquet'):
                            data = pd.read_parquet(path)
                        elif path.endswith('.csv'):
                            data = pd.read_csv(path)
                        else:
                            continue
                        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {path}")
                        break
                    except Exception as e:
                        print(f"âš ï¸ åŠ è½½æ•°æ®å¤±è´¥ {path}: {e}")
                        continue
            
            if data is None:
                # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•
                print("âš ï¸ æ— æ³•åŠ è½½çœŸå®æ•°æ®ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•")
                data = self._generate_mock_data(quick_test)
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼åªä½¿ç”¨å°‘é‡æ•°æ®
            if quick_test:
                data = data.head(10)
                print(f"ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {len(data)} ä¸ªæ ·æœ¬")
            
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {data.shape}")
            return data
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡é€‰
            return self._generate_mock_data(quick_test)
    
    def _generate_mock_data(self, quick_test: bool = False) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        n_samples = 10 if quick_test else 1000
        n_features = 69  # æ ¹æ®æ•°æ®ä¿¡æ¯ä¸­çš„ç‰¹å¾åˆ—æ•°
        
        # ç”Ÿæˆç‰¹å¾æ•°æ®
        features = np.random.randn(n_samples, n_features)
        
        # åˆ›å»ºç‰¹å¾åˆ—åï¼ˆæ¨¡æ‹ŸåŒ…å«@ç¬¦å·çš„ç‰¹å¾åˆ—ï¼‰
        feature_names = [f"feature_{i}@close{i%100}" for i in range(n_features)]
        
        # åˆ›å»ºDataFrame
        data = pd.DataFrame(features, columns=feature_names)
        
        # æ·»åŠ å¿…éœ€åˆ—
        data['date'] = pd.date_range('2020-01-01', periods=n_samples, freq='D')
        data['symbol'] = [f'A{i%10:02d}' for i in range(n_samples)]
        data['time'] = data['date']
        data['class'] = np.random.randint(0, 2, n_samples)  # äºŒåˆ†ç±»æ ‡ç­¾
        
        print(f"ğŸ”§ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {data.shape}")
        return data
    
    def preprocess_data(self, data: pd.DataFrame) -> Tuple[torch.Tensor, List[str]]:
        """
        é¢„å¤„ç†æ¨ç†æ•°æ®
        
        Args:
            data: åŸå§‹æ•°æ®DataFrame
            
        Returns:
            å¤„ç†åçš„å¼ é‡å’Œç‰¹å¾åˆ—ååˆ—è¡¨
        """
        try:
            # è·å–ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
            feature_columns = [col for col in data.columns if '@' in col]
            
            if not feature_columns:
                # å¦‚æœæ²¡æœ‰@ç¬¦å·çš„åˆ—ï¼Œä½¿ç”¨æ•°å€¼åˆ—
                numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
                exclude_cols = ['class', 'label', 'target']
                feature_columns = [col for col in numeric_columns if col not in exclude_cols]
            
            if not feature_columns:
                raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾åˆ—")
            
            print(f"ğŸ“‹ ä½¿ç”¨ç‰¹å¾åˆ—æ•°é‡: {len(feature_columns)}")
            
            # æå–ç‰¹å¾æ•°æ®
            feature_data = data[feature_columns].values
            
            # å¤„ç†ç¼ºå¤±å€¼
            feature_data = np.nan_to_num(feature_data, nan=0.0)
            
            # è½¬æ¢ä¸ºå¼ é‡
            features_tensor = torch.FloatTensor(feature_data).to(self.device)
            
            # å¦‚æœæ˜¯2Dæ•°æ®ï¼Œæ·»åŠ åºåˆ—ç»´åº¦
            if len(features_tensor.shape) == 2:
                features_tensor = features_tensor.unsqueeze(1)  # (batch_size, seq_len=1, features)
            
            print(f"ğŸ”§ é¢„å¤„ç†å®Œæˆï¼Œå¼ é‡å½¢çŠ¶: {features_tensor.shape}")
            return features_tensor, feature_columns
            
        except Exception as e:
            print(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def predict_batch(self, features: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            features: è¾“å…¥ç‰¹å¾å¼ é‡
            
        Returns:
            é¢„æµ‹æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
        """
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
            
            self.model.eval()
            with torch.no_grad():
                # æ¨¡å‹å‰å‘ä¼ æ’­
                outputs = self.model(features)
                
                # è·å–é¢„æµ‹æ¦‚ç‡
                if isinstance(outputs, dict):
                    logits = outputs.get('logits', outputs.get('output', outputs))
                else:
                    logits = outputs
                
                # åº”ç”¨softmaxè·å–æ¦‚ç‡
                probabilities = torch.softmax(logits, dim=-1)
                
                # è·å–é¢„æµ‹ç±»åˆ«
                predictions = torch.argmax(probabilities, dim=-1)
                
                return probabilities, predictions
                
        except Exception as e:
            print(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
            raise
    
    def predict_single(self, features: torch.Tensor) -> Tuple[float, int]:
        """
        å•æ ·æœ¬é¢„æµ‹
        
        Args:
            features: å•ä¸ªæ ·æœ¬çš„ç‰¹å¾å¼ é‡
            
        Returns:
            é¢„æµ‹æ¦‚ç‡å’Œé¢„æµ‹ç±»åˆ«
        """
        try:
            # ç¡®ä¿è¾“å…¥æ˜¯æ‰¹é‡æ ¼å¼
            if len(features.shape) == 2:
                features = features.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
            
            probabilities, predictions = self.predict_batch(features)
            
            # è¿”å›ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç»“æœ
            prob = probabilities[0].cpu().numpy()
            pred = predictions[0].cpu().item()
            
            return prob, pred
            
        except Exception as e:
            print(f"âŒ å•æ ·æœ¬é¢„æµ‹å¤±è´¥: {e}")
            raise
    
    def run_inference(self, data_path: Optional[str] = None, 
                     output_path: Optional[str] = None,
                     quick_test: bool = False) -> pd.DataFrame:
        """
        è¿è¡Œå®Œæ•´æ¨ç†æµç¨‹
        
        Args:
            data_path: è¾“å…¥æ•°æ®è·¯å¾„
            output_path: è¾“å‡ºç»“æœè·¯å¾„
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            åŒ…å«é¢„æµ‹ç»“æœçš„DataFrame
        """
        try:
            print("ğŸš€ å¼€å§‹æ¨ç†æµç¨‹...")
            start_time = time.time()
            
            # åŠ è½½æ¨¡å‹
            if self.model is None:
                self.load_model()
            
            # åŠ è½½æ•°æ®
            data = self.load_data(data_path, quick_test)
            
            # é¢„å¤„ç†æ•°æ®
            features, feature_columns = self.preprocess_data(data)
            
            # æ‰¹é‡é¢„æµ‹
            probabilities, predictions = self.predict_batch(features)
            
            # æ•´ç†ç»“æœ
            results = data.copy()
            results['predicted_class'] = predictions.cpu().numpy()
            results['predicted_prob_0'] = probabilities[:, 0].cpu().numpy()
            results['predicted_prob_1'] = probabilities[:, 1].cpu().numpy()
            results['confidence'] = torch.max(probabilities, dim=1)[0].cpu().numpy()
            
            # ä¿å­˜ç»“æœ
            if output_path:
                results.to_csv(output_path, index=False)
                print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
            elapsed_time = time.time() - start_time
            print(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†æ ·æœ¬æ•°: {len(results)}")
            print(f"ğŸ¯ é¢„æµ‹åˆ†å¸ƒ: {np.bincount(predictions.cpu().numpy())}")
            
            return results
            
        except Exception as e:
            print(f"âŒ æ¨ç†æµç¨‹å¤±è´¥: {e}")
            raise

def quick_validation():
    """å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯æ¨ç†æ­£ç¡®æ€§"""
    print("ğŸš€ å¼€å§‹ inference.py å¿«é€ŸéªŒè¯...")
    
    try:
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        engine = InferenceEngine()
        
        # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨éšæœºæƒé‡ï¼‰
        engine.load_model()
        
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = engine._generate_mock_data(quick_test=True)
        print(f"ğŸ“Š æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
        
        # é¢„å¤„ç†æ•°æ®
        features, feature_columns = engine.preprocess_data(test_data)
        print(f"ğŸ”§ ç‰¹å¾å¼ é‡å½¢çŠ¶: {features.shape}")
        
        # æµ‹è¯•æ‰¹é‡æ¨ç†
        print("ğŸ”„ æµ‹è¯•æ‰¹é‡æ¨ç†...")
        probabilities, predictions = engine.predict_batch(features)
        print(f"âœ… æ‰¹é‡æ¨ç†æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {probabilities.shape}, {predictions.shape}")
        
        # æµ‹è¯•å•æ ·æœ¬æ¨ç†
        print("ğŸ”„ æµ‹è¯•å•æ ·æœ¬æ¨ç†...")
        single_features = features[0:1]
        prob, pred = engine.predict_single(single_features)
        print(f"âœ… å•æ ·æœ¬æ¨ç†æˆåŠŸï¼Œé¢„æµ‹ç±»åˆ«: {pred}, æ¦‚ç‡: {prob}")
        
        # æµ‹è¯•å®Œæ•´æ¨ç†æµç¨‹
        print("ğŸ”„ æµ‹è¯•å®Œæ•´æ¨ç†æµç¨‹...")
        results = engine.run_inference(quick_test=True)
        print(f"âœ… å®Œæ•´æ¨ç†æµç¨‹æˆåŠŸï¼Œç»“æœå½¢çŠ¶: {results.shape}")
        
        # éªŒè¯è¾“å‡ºæ ¼å¼
        required_columns = ['predicted_class', 'predicted_prob_0', 'predicted_prob_1', 'confidence']
        for col in required_columns:
            if col not in results.columns:
                raise ValueError(f"ç¼ºå°‘å¿…éœ€çš„è¾“å‡ºåˆ—: {col}")
        
        print("âœ… è¾“å‡ºæ ¼å¼éªŒè¯é€šè¿‡")
        print("ğŸ‰ inference.py å¿«é€ŸéªŒè¯å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸è¿è¡Œ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ··åˆé¢‘ç‡é‡ä»·å› å­æ¨¡å‹æ¨ç†")
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, default=None,
                       help='æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data', type=str, default=None,
                       help='è¾“å…¥æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='inference_results.csv',
                       help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quick-test', action='store_true',
                       help='å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯æ¨ç†æ­£ç¡®æ€§')
    
    args = parser.parse_args()
    
    if args.quick_test:
        # æ‰§è¡Œå¿«é€ŸéªŒè¯
        success = quick_validation()
        sys.exit(0 if success else 1)
    
    try:
        # åˆå§‹åŒ–æ¨ç†å¼•æ“
        engine = InferenceEngine(args.config, args.model)
        
        # è¿è¡Œæ¨ç†
        results = engine.run_inference(
            data_path=args.data,
            output_path=args.output
        )
        
        print("ğŸ‰ æ¨ç†ä»»åŠ¡å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æ¨ç†ä»»åŠ¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()