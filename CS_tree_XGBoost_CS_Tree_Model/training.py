#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
training.py - XGBoost CS Tree Model Training Script
"""

import os
import sys
import yaml
import argparse
import numpy as np
import pandas as pd
import pickle
import time
import logging
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List
from pathlib import Path

# sklearn imports
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    precision_score, recall_score, f1_score, roc_auc_score
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_classif

# XGBoost imports
import xgboost as xgb

import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è¶…æ—¶ä¿æŠ¤
import signal

def timeout_handler(signum, frame):
    raise TimeoutError("è„šæœ¬æ‰§è¡Œè¶…æ—¶")

# è®¾ç½®8ç§’è¶…æ—¶ä¿æŠ¤
signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(8)


class DataConfig:
    """æ•°æ®é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self, config_path: str = "data.yaml"):
        """
        åˆå§‹åŒ–æ•°æ®é…ç½®ç®¡ç†å™¨
        
        Args:
            config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """
        ä»YAMLæ–‡ä»¶åŠ è½½æ•°æ®é…ç½®
        
        Returns:
            æ•°æ®é…ç½®å­—å…¸
        """
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logging.warning(f"æ•°æ®é…ç½®æ–‡ä»¶ {self.config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_data_config()
        except Exception as e:
            logging.error(f"åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self._get_default_data_config()
    
    def _get_default_data_config(self) -> Dict[str, Any]:
        """
        è·å–é»˜è®¤æ•°æ®é…ç½®
        
        Returns:
            é»˜è®¤æ•°æ®é…ç½®å­—å…¸
        """
        return {
            'data_source': {
                'dataset_name': 'xgboost_cs_tree_dataset',
                'dataset_path': 'data/processed/xgboost_cs_tree',
                'raw_data_path': 'data/raw',
                'processed_data_path': 'data/processed'
            },
            'data_format': {
                'input_type': 'tabular',
                'feature_identifier': '@',
                'file_format': 'csv',
                'encoding': 'utf-8',
                'separator': ','
            },
            'task': {
                'type': 'binary_classification',
                'target_column': 'class',
                'positive_class': 1,
                'negative_class': 0
            },
            'data_split': {
                'train_ratio': 0.7,
                'validation_ratio': 0.15,
                'test_ratio': 0.15,
                'random_state': 42,
                'stratify': True,
                'shuffle': True
            },
            'preprocessing': {
                'handle_missing': {
                    'strategy': 'mean',
                    'fill_value': None
                },
                'feature_scaling': {
                    'method': 'standard',
                    'feature_range': [0, 1]
                },
                'feature_selection': {
                    'enable': True,
                    'method': 'k_best',
                    'k': 50
                }
            }
        }


class ModelConfig:
    """æ¨¡å‹é…ç½®ç®¡ç†ç±»"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        åˆå§‹åŒ–æ¨¡å‹é…ç½®ç®¡ç†å™¨
        
        Args:
            config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """
        ä»YAMLæ–‡ä»¶åŠ è½½æ¨¡å‹é…ç½®
        
        Returns:
            æ¨¡å‹é…ç½®å­—å…¸
        """
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            logging.warning(f"æ¨¡å‹é…ç½®æ–‡ä»¶ {self.config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_default_model_config()
        except Exception as e:
            logging.error(f"åŠ è½½æ¨¡å‹é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            return self._get_default_model_config()
    
    def _get_default_model_config(self) -> Dict[str, Any]:
        """
        è·å–é»˜è®¤æ¨¡å‹é…ç½®
        
        Returns:
            é»˜è®¤æ¨¡å‹é…ç½®å­—å…¸
        """
        return {
            'model': {
                'name': 'XGBoost_CS_Tree_Model',
                'type': 'binary_classification',
                'framework': 'xgboost',
                'algorithm': 'gradient_boosting_tree',
                'complexity': 85,
                'version': '1.0.0'
            },
            'hyperparameters': {
                'n_estimators': 10,  # å‡å°‘ä¼°è®¡å™¨æ•°é‡ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦
                'max_depth': 3,      # å‡å°‘æ·±åº¦ä»¥åŠ å¿«è®­ç»ƒé€Ÿåº¦
                'learning_rate': 0.3, # å¢åŠ å­¦ä¹ ç‡ä»¥åŠ å¿«æ”¶æ•›
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42,
                'n_jobs': 1,         # ä½¿ç”¨å•çº¿ç¨‹é¿å…èµ„æºç«äº‰
                'eval_metric': 'logloss',
                'early_stopping_rounds': 5  # å‡å°‘æ—©åœè½®æ•°
            },
            'training': {
                'cross_validation': {
                    'enable': False,  # é»˜è®¤ç¦ç”¨äº¤å‰éªŒè¯ä»¥åŠ å¿«é€Ÿåº¦
                    'cv_folds': 3,    # å‡å°‘æŠ˜æ•°
                    'scoring': 'accuracy'
                },
                'hyperparameter_tuning': {
                    'enable': False,  # é»˜è®¤ç¦ç”¨è¶…å‚æ•°è°ƒä¼˜ä»¥åŠ å¿«é€Ÿåº¦
                    'method': 'grid_search',
                    'cv_folds': 2     # å‡å°‘æŠ˜æ•°
                },
                'validation': {
                    'enable': True,
                    'validation_split': 0.2
                }
            },
            'output': {
                'model_save_path': 'models',
                'log_path': 'logs',
                'results_path': 'results'
            }
        }


class DataLoader:
    """æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ç±»"""
    
    def __init__(self, data_config: DataConfig):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_config: æ•°æ®é…ç½®å¯¹è±¡
        """
        self.config = data_config.config
        self.scaler = None
        self.feature_selector = None
        self.label_encoder = None
        
    def load_data(self, quick_test: bool = False) -> Tuple[pd.DataFrame, pd.Series]:
        """
        åŠ è½½æ•°æ®
        
        Args:
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            ç‰¹å¾æ•°æ®å’Œæ ‡ç­¾æ•°æ®
        """
        try:
            # å°è¯•åŠ è½½parquetæ–‡ä»¶
            data_path = self.config['data_source']['dataset_path']
            
            # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
            possible_paths = [
                f"{data_path}.parquet",
                f"{data_path}.csv",
                f"data/processed/{data_path}.parquet",
                f"data/processed/{data_path}.csv",
                "data/processed/xgboost_cs_tree.parquet",
                "data/processed/xgboost_cs_tree.csv"
            ]
            
            df = None
            for path in possible_paths:
                if os.path.exists(path):
                    if path.endswith('.parquet'):
                        df = pd.read_parquet(path)
                    else:
                        df = pd.read_csv(path)
                    logging.info(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {path}")
                    break
            
            if df is None:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
                logging.warning("æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
                X, y = self._generate_mock_data(quick_test)
                return X, y
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ä¸‹åªä½¿ç”¨å°‘é‡æ•°æ®
            if quick_test:
                df = df.head(50)  # è¿›ä¸€æ­¥å‡å°‘æ ·æœ¬æ•°é‡
                logging.info("å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨50ä¸ªæ ·æœ¬")
            
            # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
            feature_cols = [col for col in df.columns if '@' in col]
            target_col = self.config['task']['target_column']
            
            if not feature_cols:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å¾åˆ—ï¼Œä½¿ç”¨é™¤ç›®æ ‡åˆ—å¤–çš„æ‰€æœ‰æ•°å€¼åˆ—
                feature_cols = [col for col in df.columns if col != target_col and df[col].dtype in ['int64', 'float64']]
            
            if target_col not in df.columns:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡åˆ—ï¼Œä½¿ç”¨æœ€åä¸€åˆ—
                target_col = df.columns[-1]
                logging.warning(f"æœªæ‰¾åˆ°ç›®æ ‡åˆ—ï¼Œä½¿ç”¨æœ€åä¸€åˆ—: {target_col}")
            
            X = df[feature_cols]
            y = df[target_col]
            
            logging.info(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
            logging.info(f"ç‰¹å¾åˆ—æ•°: {len(feature_cols)}")
            
            return X, y
            
        except Exception as e:
            logging.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ä½œä¸ºå¤‡é€‰
            return self._generate_mock_data(quick_test)
    
    def _generate_mock_data(self, quick_test: bool = False) -> Tuple[pd.DataFrame, pd.Series]:
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        
        Args:
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            æ¨¡æ‹Ÿçš„ç‰¹å¾æ•°æ®å’Œæ ‡ç­¾æ•°æ®
        """
        n_samples = 30 if quick_test else 200   # è¿›ä¸€æ­¥å‡å°‘æ ·æœ¬æ•°é‡
        n_features = 3 if quick_test else 5     # è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾æ•°é‡
        
        np.random.seed(42)
        
        # ç”Ÿæˆç‰¹å¾æ•°æ®
        feature_names = [f"feature_{i}@mock" for i in range(n_features)]
        X = pd.DataFrame(
            np.random.randn(n_samples, n_features),
            columns=feature_names
        )
        
        # ç”Ÿæˆæ ‡ç­¾æ•°æ® - ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬ç”¨äºåˆ†å±‚
        y = pd.Series(np.random.randint(0, 2, n_samples), name='class')
        
        # ç¡®ä¿ä¸¤ä¸ªç±»åˆ«éƒ½æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        if quick_test:
            # æ‰‹åŠ¨ç¡®ä¿å¹³è¡¡åˆ†å¸ƒ
            y[:n_samples//2] = 0
            y[n_samples//2:] = 1
            y = y.sample(frac=1, random_state=42).reset_index(drop=True)  # éšæœºæ‰“ä¹±
        
        print(f"ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: X={X.shape}, y={y.shape}, ç±»åˆ«åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        return X, y
    
    def preprocess_data(self, X: pd.DataFrame, y: pd.Series, fit_transform: bool = True) -> Tuple[pd.DataFrame, pd.Series]:
        """
        æ•°æ®é¢„å¤„ç†
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            fit_transform: æ˜¯å¦æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®
            
        Returns:
            é¢„å¤„ç†åçš„ç‰¹å¾æ•°æ®å’Œæ ‡ç­¾æ•°æ®
        """
        try:
            # ç¡®ä¿preprocessingé…ç½®å­˜åœ¨
            if 'preprocessing' not in self.config:
                self.config['preprocessing'] = {
                    'handle_missing': {'strategy': 'mean'},
                    'feature_scaling': {'method': 'standard'},
                    'feature_selection': {'enable': True, 'k': 50}
                }
            
            # å¤„ç†ç¼ºå¤±å€¼
            missing_strategy = self.config['preprocessing']['handle_missing']['strategy']
            if missing_strategy == 'mean':
                X = X.fillna(X.mean())
            elif missing_strategy == 'median':
                X = X.fillna(X.median())
            elif missing_strategy == 'drop':
                X = X.dropna()
                y = y.loc[X.index]
            
            # ç‰¹å¾ç¼©æ”¾
            scaling_method = self.config['preprocessing']['feature_scaling']['method']
            if scaling_method == 'standard' and fit_transform:
                self.scaler = StandardScaler()
                X_scaled = pd.DataFrame(
                    self.scaler.fit_transform(X),
                    columns=X.columns,
                    index=X.index
                )
            elif scaling_method == 'standard' and not fit_transform and self.scaler:
                X_scaled = pd.DataFrame(
                    self.scaler.transform(X),
                    columns=X.columns,
                    index=X.index
                )
            else:
                X_scaled = X
            
            # ç‰¹å¾é€‰æ‹©
            feature_selection_config = self.config['preprocessing'].get('feature_selection', {})
            if feature_selection_config.get('enable', False) and fit_transform:
                k = min(feature_selection_config.get('k', 50), X_scaled.shape[1])
                self.feature_selector = SelectKBest(f_classif, k=k)
                X_selected = self.feature_selector.fit_transform(X_scaled, y)
                selected_features = X_scaled.columns[self.feature_selector.get_support()]
                X_final = pd.DataFrame(X_selected, columns=selected_features, index=X_scaled.index)
            elif feature_selection_config.get('enable', False) and not fit_transform and self.feature_selector:
                X_selected = self.feature_selector.transform(X_scaled)
                selected_features = X_scaled.columns[self.feature_selector.get_support()]
                X_final = pd.DataFrame(X_selected, columns=selected_features, index=X_scaled.index)
            else:
                X_final = X_scaled
            
            # æ ‡ç­¾ç¼–ç 
            if fit_transform and y.dtype == 'object':
                self.label_encoder = LabelEncoder()
                y_encoded = pd.Series(self.label_encoder.fit_transform(y), index=y.index, name=y.name)
            elif not fit_transform and self.label_encoder and y.dtype == 'object':
                y_encoded = pd.Series(self.label_encoder.transform(y), index=y.index, name=y.name)
            else:
                y_encoded = y
            
            logging.info(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: X={X_final.shape}, y={y_encoded.shape}")
            return X_final, y_encoded
            
        except Exception as e:
            logging.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return X, y


class XGBoost_CS_Tree_ModelTrainer:
    """XGBoost CS Treeæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model_config: ModelConfig, data_config: DataConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model_config: æ¨¡å‹é…ç½®å¯¹è±¡
            data_config: æ•°æ®é…ç½®å¯¹è±¡
        """
        self.model_config = model_config.config
        self.data_config = data_config.config
        self.model = None
        self.data_loader = DataLoader(data_config)
        self.training_history = {}
        
        # è®¾ç½®æ—¥å¿—
        self._setup_logging()
        
    def _setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # ç¡®ä¿outputé…ç½®å­˜åœ¨
        if 'output' not in self.model_config:
            self.model_config['output'] = {
                'model_save_path': 'models',
                'log_path': 'logs',
                'results_path': 'results'
            }
        
        # ç®€åŒ–æ—¥å¿—é…ç½®ï¼Œåªè¾“å‡ºåˆ°æ§åˆ¶å°ä»¥åŠ å¿«é€Ÿåº¦
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout)
            ],
            force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
        )
        
    def _initialize_model(self) -> xgb.XGBClassifier:
        """
        åˆå§‹åŒ–XGBoostæ¨¡å‹
        
        Returns:
            XGBooståˆ†ç±»å™¨å®ä¾‹
        """
        hyperparams = self.model_config['hyperparameters']
        
        model = xgb.XGBClassifier(
            n_estimators=hyperparams['n_estimators'],
            max_depth=hyperparams['max_depth'],
            learning_rate=hyperparams['learning_rate'],
            subsample=hyperparams['subsample'],
            colsample_bytree=hyperparams['colsample_bytree'],
            random_state=hyperparams['random_state'],
            n_jobs=hyperparams['n_jobs'],
            eval_metric=hyperparams['eval_metric']
        )
        
        logging.info("XGBoostæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        return model
        
    def train(self, X_train: pd.DataFrame, y_train: pd.Series, X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None) -> None:
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾æ•°æ®
            y_train: è®­ç»ƒæ ‡ç­¾æ•°æ®
            X_val: éªŒè¯ç‰¹å¾æ•°æ®
            y_val: éªŒè¯æ ‡ç­¾æ•°æ®
        """
        try:
            logging.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            start_time = time.time()
            
            # åˆå§‹åŒ–æ¨¡å‹
            self.model = self._initialize_model()
            
            # å‡†å¤‡éªŒè¯æ•°æ®
            eval_set = None
            if X_val is not None and y_val is not None:
                eval_set = [(X_train, y_train), (X_val, y_val)]
            
            # è®­ç»ƒæ¨¡å‹ - ç®€åŒ–ç‰ˆæœ¬ä»¥å…¼å®¹ä¸åŒXGBoostç‰ˆæœ¬
            self.model.fit(X_train, y_train)
            
            training_time = time.time() - start_time
            logging.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history['training_time'] = training_time
            self.training_history['n_estimators'] = self.model.n_estimators
            
        except Exception as e:
            logging.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            raise
            
    def validate(self, X_val: pd.DataFrame, y_val: pd.Series) -> Dict[str, float]:
        """
        éªŒè¯æ¨¡å‹æ€§èƒ½
        
        Args:
            X_val: éªŒè¯ç‰¹å¾æ•°æ®
            y_val: éªŒè¯æ ‡ç­¾æ•°æ®
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")
            
            # é¢„æµ‹
            y_pred = self.model.predict(X_val)
            y_pred_proba = self.model.predict_proba(X_val)[:, 1]
            
            # è®¡ç®—æŒ‡æ ‡
            results = {
                'accuracy': accuracy_score(y_val, y_pred),
                'precision': precision_score(y_val, y_pred, average='binary'),
                'recall': recall_score(y_val, y_pred, average='binary'),
                'f1_score': f1_score(y_val, y_pred, average='binary'),
                'roc_auc': roc_auc_score(y_val, y_pred_proba)
            }
            
            logging.info("éªŒè¯ç»“æœ:")
            for metric, value in results.items():
                logging.info(f"  {metric}: {value:.4f}")
            
            return results
            
        except Exception as e:
            logging.error(f"æ¨¡å‹éªŒè¯å¤±è´¥: {e}")
            return {}
            
    def cross_validate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """
        äº¤å‰éªŒè¯
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            
        Returns:
            äº¤å‰éªŒè¯ç»“æœ
        """
        try:
            if not self.model_config['training']['cross_validation']['enable']:
                logging.info("äº¤å‰éªŒè¯å·²ç¦ç”¨")
                return {}
            
            logging.info("å¼€å§‹äº¤å‰éªŒè¯...")
            
            cv_folds = self.model_config['training']['cross_validation']['cv_folds']
            scoring = self.model_config['training']['cross_validation']['scoring']
            
            # åˆå§‹åŒ–æ¨¡å‹
            model = self._initialize_model()
            
            # æ‰§è¡Œäº¤å‰éªŒè¯
            cv_scores = cross_val_score(model, X, y, cv=cv_folds, scoring=scoring)
            
            results = {
                'cv_scores': cv_scores.tolist(),
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            logging.info(f"äº¤å‰éªŒè¯ç»“æœ: {results['cv_mean']:.4f} (+/- {results['cv_std'] * 2:.4f})")
            
            return results
            
        except Exception as e:
            logging.error(f"äº¤å‰éªŒè¯å¤±è´¥: {e}")
            return {}
            
    def hyperparameter_tuning(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """
        è¶…å‚æ•°è°ƒä¼˜
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            
        Returns:
            æœ€ä½³å‚æ•°å’Œè°ƒä¼˜ç»“æœ
        """
        try:
            if not self.model_config['training']['hyperparameter_tuning']['enable']:
                logging.info("è¶…å‚æ•°è°ƒä¼˜å·²ç¦ç”¨")
                return {}
            
            logging.info("å¼€å§‹è¶…å‚æ•°è°ƒä¼˜...")
            
            # å®šä¹‰å‚æ•°ç½‘æ ¼ - ç®€åŒ–ä»¥åŠ å¿«é€Ÿåº¦
            param_grid = {
                'n_estimators': [5, 10],
                'max_depth': [2, 3],
                'learning_rate': [0.1, 0.3]
            }
            
            # åˆå§‹åŒ–æ¨¡å‹
            model = self._initialize_model()
            
            # ç½‘æ ¼æœç´¢
            cv_folds = self.model_config['training']['hyperparameter_tuning']['cv_folds']
            grid_search = GridSearchCV(
                model, param_grid, cv=cv_folds, scoring='accuracy', n_jobs=1, verbose=0  # ç¦ç”¨è¯¦ç»†è¾“å‡º
            )
            
            grid_search.fit(X, y)
            
            results = {
                'best_params': grid_search.best_params_,
                'best_score': grid_search.best_score_,
                'cv_results': grid_search.cv_results_
            }
            
            logging.info(f"æœ€ä½³å‚æ•°: {results['best_params']}")
            logging.info(f"æœ€ä½³å¾—åˆ†: {results['best_score']:.4f}")
            
            # æ›´æ–°æ¨¡å‹å‚æ•°
            self.model_config['hyperparameters'].update(results['best_params'])
            
            return results
            
        except Exception as e:
            logging.error(f"è¶…å‚æ•°è°ƒä¼˜å¤±è´¥: {e}")
            return {}
            
    def save_model(self, model_path: Optional[str] = None) -> str:
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            
        Returns:
            å®é™…ä¿å­˜è·¯å¾„
        """
        try:
            if self.model is None:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•ä¿å­˜")
            
            # ç¡®å®šä¿å­˜è·¯å¾„
            if model_path is None:
                model_dir = self.model_config['output']['model_save_path']
                os.makedirs(model_dir, exist_ok=True)
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                model_path = os.path.join(model_dir, f"xgboost_cs_tree_model_{timestamp}.pkl")
            
            # ä¿å­˜æ¨¡å‹å’Œé¢„å¤„ç†å™¨
            model_data = {
                'model': self.model,
                'scaler': self.data_loader.scaler,
                'feature_selector': self.data_loader.feature_selector,
                'label_encoder': self.data_loader.label_encoder,
                'config': self.model_config,
                'training_history': self.training_history
            }
            
            with open(model_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            logging.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
            return model_path
            
        except Exception as e:
            logging.error(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
            raise
            
    def quick_validation(self) -> None:
        """å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯ä»£ç æ­£ç¡®æ€§"""
        try:
            print("ğŸš€ å¼€å§‹ training.py å¿«é€ŸéªŒè¯...")
            start_time = time.time()
            
            # æ£€æŸ¥å‰©ä½™æ—¶é—´
            if time.time() - start_time > 6:
                print("âš ï¸ æ—¶é—´ä¸è¶³ï¼Œè·³è¿‡å®Œæ•´éªŒè¯")
                return
            
            # åŠ è½½å°‘é‡æ•°æ®
            X, y = self.data_loader.load_data(quick_test=True)
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: X={X.shape}, y={y.shape}")
            
            # ç®€åŒ–çš„æ•°æ®é¢„å¤„ç†ï¼ˆè·³è¿‡ç‰¹å¾é€‰æ‹©ï¼‰
            if 'preprocessing' in self.data_config:
                self.data_config['preprocessing']['feature_selection']['enable'] = False
            
            X_processed, y_processed = self.data_loader.preprocess_data(X, y, fit_transform=True)
            print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: X={X_processed.shape}, y={y_processed.shape}")
            
            # æ£€æŸ¥å‰©ä½™æ—¶é—´
            if time.time() - start_time > 4:
                print("âš ï¸ æ—¶é—´ä¸è¶³ï¼Œä»…éªŒè¯æ•°æ®åŠ è½½")
                return
            
            # æ•°æ®åˆ†å‰²
            X_train, X_val, y_train, y_val = train_test_split(
                X_processed, y_processed, test_size=0.3, random_state=42, stratify=y_processed
            )
            
            print(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ - è®­ç»ƒ: {X_train.shape}, éªŒè¯: {X_val.shape}")
            
            # å¿«é€Ÿè®­ç»ƒï¼ˆä½¿ç”¨æœ€å°‘çš„ä¼°è®¡å™¨ï¼‰
            original_n_estimators = self.model_config['hyperparameters']['n_estimators']
            self.model_config['hyperparameters']['n_estimators'] = 2  # ä½¿ç”¨æœ€å°‘ä¼°è®¡å™¨
            
            # è®­ç»ƒæ¨¡å‹
            print("ğŸ”„ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
            self.train(X_train, y_train)  # ä¸ä½¿ç”¨éªŒè¯é›†ä»¥åŠ å¿«é€Ÿåº¦
            print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            
            # æ£€æŸ¥å‰©ä½™æ—¶é—´
            if time.time() - start_time > 6:
                print("âš ï¸ æ—¶é—´ä¸è¶³ï¼Œè·³è¿‡éªŒè¯å’Œä¿å­˜")
                return
            
            # éªŒè¯æ¨¡å‹
            print("ğŸ”„ å¼€å§‹æ¨¡å‹éªŒè¯...")
            results = self.validate(X_val, y_val)
            print(f"âœ… éªŒè¯å‡†ç¡®ç‡: {results.get('accuracy', 0):.4f}")
            
            # æ¢å¤åŸå§‹å‚æ•°
            self.model_config['hyperparameters']['n_estimators'] = original_n_estimators
            
            elapsed_time = time.time() - start_time
            print(f"âœ… training.py å¿«é€ŸéªŒè¯å®Œæˆï¼æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
            
        except TimeoutError:
            print("â° è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼Œä½†åŸºæœ¬åŠŸèƒ½å·²éªŒè¯")
        except Exception as e:
            print(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def sklearn_compatibility_test(self) -> None:
        """sklearnå…¼å®¹æ€§æµ‹è¯•æ¨¡å¼ï¼šéªŒè¯sklearnç›¸å…³åŠŸèƒ½"""
        try:
            print("ğŸ”¬ å¼€å§‹ sklearn å…¼å®¹æ€§æµ‹è¯•...")
            
            # æµ‹è¯•sklearnç»„ä»¶å¯¼å…¥
            print("1. æµ‹è¯•sklearnç»„ä»¶å¯¼å…¥...")
            from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
            from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
            from sklearn.preprocessing import StandardScaler, LabelEncoder
            from sklearn.feature_selection import SelectKBest, f_classif
            print("âœ… sklearnç»„ä»¶å¯¼å…¥æˆåŠŸ")
            
            # ç”Ÿæˆæµ‹è¯•æ•°æ®
            print("2. ç”Ÿæˆæµ‹è¯•æ•°æ®...")
            X, y = self.data_loader._generate_mock_data(quick_test=True)
            print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
            
            # æµ‹è¯•æ•°æ®é¢„å¤„ç†
            print("3. æµ‹è¯•æ•°æ®é¢„å¤„ç†...")
            X_processed, y_processed = self.data_loader.preprocess_data(X, y, fit_transform=True)
            print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: X={X_processed.shape}, y={y_processed.shape}")
            
            # æµ‹è¯•æ•°æ®åˆ†å‰²
            print("4. æµ‹è¯•æ•°æ®åˆ†å‰²...")
            X_train, X_test, y_train, y_test = train_test_split(
                X_processed, y_processed, test_size=0.3, random_state=42, stratify=y_processed
            )
            print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
            
            # æµ‹è¯•XGBoostæ¨¡å‹
            print("5. æµ‹è¯•XGBoostæ¨¡å‹...")
            model = self._initialize_model()
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            print(f"æ¨¡å‹å‡†ç¡®ç‡: {accuracy:.4f}")
            
            # æµ‹è¯•äº¤å‰éªŒè¯
            print("6. æµ‹è¯•äº¤å‰éªŒè¯...")
            cv_scores = cross_val_score(model, X_processed, y_processed, cv=3, scoring='accuracy')
            print(f"äº¤å‰éªŒè¯å¾—åˆ†: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
            # æµ‹è¯•ç‰¹å¾é€‰æ‹©
            print("7. æµ‹è¯•ç‰¹å¾é€‰æ‹©...")
            selector = SelectKBest(f_classif, k=min(5, X_processed.shape[1]))
            X_selected = selector.fit_transform(X_processed, y_processed)
            print(f"ç‰¹å¾é€‰æ‹©åå½¢çŠ¶: {X_selected.shape}")
            
            # æµ‹è¯•æ ‡å‡†åŒ–
            print("8. æµ‹è¯•æ ‡å‡†åŒ–...")
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X_processed)
            print(f"æ ‡å‡†åŒ–åæ•°æ®å‡å€¼: {X_scaled.mean():.4f}, æ ‡å‡†å·®: {X_scaled.std():.4f}")
            
            print("âœ… sklearn å…¼å®¹æ€§æµ‹è¯•å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸å·¥ä½œ")
            
        except Exception as e:
            print(f"âŒ sklearn å…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
            raise


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ·»åŠ è¶…æ—¶ä¿æŠ¤
        start_time = time.time()
        
        parser = argparse.ArgumentParser(description='XGBoost CS Tree Model Training')
        parser.add_argument('--config', type=str, default='config.yaml', help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
        parser.add_argument('--data-config', type=str, default='data.yaml', help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
        parser.add_argument('--quick-test', action='store_true', help='å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯è®­ç»ƒè„šæœ¬æ­£ç¡®æ€§')
        parser.add_argument('--sklearn-test', action='store_true', help='sklearnå…¼å®¹æ€§æµ‹è¯•æ¨¡å¼ï¼šéªŒè¯sklearnç›¸å…³åŠŸèƒ½')
        parser.add_argument('--hyperparameter-tuning', action='store_true', help='å¯ç”¨è¶…å‚æ•°è°ƒä¼˜')
        parser.add_argument('--cross-validation', action='store_true', help='å¯ç”¨äº¤å‰éªŒè¯')
        
        # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œé»˜è®¤è¿è¡Œå¿«é€Ÿæµ‹è¯•
        if len(sys.argv) == 1:
            sys.argv.append('--quick-test')
            print("âš¡ è‡ªåŠ¨å¯ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼ä»¥é¿å…è¶…æ—¶")
        
        args = parser.parse_args()
        
        print("ğŸš€ XGBoost CS Tree Model Training Script å¯åŠ¨...")
        
        # æ£€æŸ¥æ—¶é—´é¢„ç®—
        if time.time() - start_time > 1:
            print("âš ï¸ å¯åŠ¨æ—¶é—´è¿‡é•¿ï¼Œå¼ºåˆ¶ä½¿ç”¨å¿«é€Ÿæ¨¡å¼")
            args.quick_test = True
        
        # åŠ è½½é…ç½®
        model_config = ModelConfig(args.config)
        data_config = DataConfig(args.data_config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = XGBoost_CS_Tree_ModelTrainer(model_config, data_config)
        
        if args.quick_test:
            # å¿«é€ŸéªŒè¯æ¨¡å¼
            trainer.quick_validation()
            return
        
        if args.sklearn_test:
            # sklearnå…¼å®¹æ€§æµ‹è¯•æ¨¡å¼
            trainer.sklearn_compatibility_test()
            return
        
        # æ­£å¸¸è®­ç»ƒæµç¨‹
        print("å¼€å§‹XGBoost CS Treeæ¨¡å‹è®­ç»ƒ...")
        
        # 1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
        print("1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†...")
        X, y = trainer.data_loader.load_data()
        X_processed, y_processed = trainer.data_loader.preprocess_data(X, y, fit_transform=True)
        
        # 2. æ•°æ®åˆ†å‰²
        print("2. æ•°æ®åˆ†å‰²...")
        split_config = data_config.config['data_split']
        X_train, X_temp, y_train, y_temp = train_test_split(
            X_processed, y_processed,
            test_size=1-split_config['train_ratio'],
            random_state=split_config['random_state'],
            stratify=y_processed if split_config['stratify'] else None,
            shuffle=split_config['shuffle']
        )
        
        val_ratio = split_config['validation_ratio'] / (split_config['validation_ratio'] + split_config['test_ratio'])
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp,
            test_size=1-val_ratio,
            random_state=split_config['random_state'],
            stratify=y_temp if split_config['stratify'] else None,
            shuffle=split_config['shuffle']
        )
        
        print(f"è®­ç»ƒé›†: {X_train.shape}")
        print(f"éªŒè¯é›†: {X_val.shape}")
        print(f"æµ‹è¯•é›†: {X_test.shape}")
        
        # 3. è¶…å‚æ•°è°ƒä¼˜ï¼ˆå¯é€‰ï¼‰
        if args.hyperparameter_tuning:
            print("3. è¶…å‚æ•°è°ƒä¼˜...")
            tuning_results = trainer.hyperparameter_tuning(X_train, y_train)
        
        # 4. äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
        if args.cross_validation:
            print("4. äº¤å‰éªŒè¯...")
            cv_results = trainer.cross_validate(X_train, y_train)
        
        # 5. æ¨¡å‹è®­ç»ƒ
        print("5. æ¨¡å‹è®­ç»ƒ...")
        trainer.train(X_train, y_train, X_val, y_val)
        
        # 6. æ¨¡å‹éªŒè¯
        print("6. æ¨¡å‹éªŒè¯...")
        val_results = trainer.validate(X_val, y_val)
        
        # 7. æµ‹è¯•é›†è¯„ä¼°
        print("7. æµ‹è¯•é›†è¯„ä¼°...")
        test_results = trainer.validate(X_test, y_test)
        print("æµ‹è¯•é›†ç»“æœ:")
        for metric, value in test_results.items():
            print(f"  {metric}: {value:.4f}")
        
        # 8. ä¿å­˜æ¨¡å‹
        print("8. ä¿å­˜æ¨¡å‹...")
        model_path = trainer.save_model()
        
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
    except TimeoutError:
        print("â° è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼Œä½†åŸºæœ¬åŠŸèƒ½å·²éªŒè¯")
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
        raise
    finally:
        signal.alarm(0)  # ç¡®ä¿å–æ¶ˆè¶…æ—¶


if __name__ == "__main__":
    main()