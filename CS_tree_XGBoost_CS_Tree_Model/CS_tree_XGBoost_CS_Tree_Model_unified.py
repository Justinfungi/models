#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CS_tree_XGBoost_CS_Tree_Model_unified.py - ç»Ÿä¸€æ¨¡å‹å®ç°
æ•´åˆè®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½çš„ç»Ÿä¸€æ¨¡æ¿ï¼Œæ”¯æŒXGBoost CS Treeæ¨¡å‹æ¶æ„
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.feature_selection import SelectKBest, f_classif
import xgboost as xgb

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class CS_tree_XGBoost_CS_Tree_ModelConfig:
    """CS_tree_XGBoost_CS_Tree_Modelé…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "CS_tree_XGBoost_CS_Tree_Model"
    model_type: str = "classification"  # classification, regression, time_series
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.1
    batch_size: int = 32  # ç¡®ä¿batch sizeè¶³å¤Ÿå¤§é¿å…BatchNormé—®é¢˜
    epochs: int = 100
    
    # XGBoostç‰¹å®šé…ç½®
    n_estimators: int = 100
    max_depth: int = 6
    subsample: float = 0.8
    colsample_bytree: float = 0.8
    colsample_bylevel: float = 1.0
    colsample_bynode: float = 1.0
    reg_alpha: float = 0.0
    reg_lambda: float = 1.0
    gamma: float = 0.0
    min_child_weight: int = 1
    random_state: int = 42
    n_jobs: int = -1
    objective: str = "binary:logistic"
    eval_metric: str = "logloss"
    tree_method: str = "auto"
    verbosity: int = 1
    early_stopping_rounds: int = 10
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True  # æ··åˆç²¾åº¦è®­ç»ƒ
    dataloader_num_workers: int = 4  # æ•°æ®åŠ è½½å¹¶å‘æ•°
    pin_memory: bool = True  # å›ºå®šå†…å­˜
    gradient_clip_value: float = 1.0  # æ¢¯åº¦è£å‰ª
    
    # æ•°æ®é…ç½®
    seq_len: int = 96
    pred_len: int = 24
    input_dim: Optional[int] = None  # åŠ¨æ€è®¾ç½®
    output_dim: int = 2  # äºŒåˆ†ç±»
    hidden_dim: int = 128
    dropout: float = 0.2
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    seed: int = 42
    
    # è·¯å¾„é…ç½®
    data_path: str = "./data"
    checkpoint_dir: str = "./checkpoints"
    results_dir: str = "./results"
    logs_dir: str = "./logs"

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    import logging
    logger = logging.getLogger(__name__)

    if device_choice == "cpu":
        device = torch.device('cpu')
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
        logger.info("è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨CPU")
    elif device_choice == "cuda":
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            logger.info(f"è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨GPU - {gpu_name} ({gpu_memory:.1f}GB)")
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            cached_memory = torch.cuda.memory_reserved() / 1024**3

            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU: {gpu_name}")
            print(f"   ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f}GB æ€»é‡")
            print(f"   ğŸ“Š GPUæ•°é‡: {gpu_count}")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨: {current_memory:.2f}GB")
            print(f"   ğŸ“Š ç¼“å­˜ä½¿ç”¨: {cached_memory:.2f}GB")

            logger.info(f"è®¾å¤‡é…ç½®: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU")
            logger.info(f"GPUä¿¡æ¯: {gpu_name}, {gpu_memory:.1f}GBæ€»å†…å­˜, {gpu_count}ä¸ªGPU")
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {current_memory:.2f}GBå·²ç”¨, {cached_memory:.2f}GBç¼“å­˜")

            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            logger.info("è®¾å¤‡é…ç½®: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•è·¯å¾„
        prefix: æ—¥å¿—æ–‡ä»¶å‰ç¼€ï¼ˆå½“log_filenameä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
        log_filename: å›ºå®šçš„æ—¥å¿—æ–‡ä»¶åï¼ˆå¦‚æœæä¾›ï¼Œå°†å¿½ç•¥prefixå’Œæ—¶é—´æˆ³ï¼‰
    """
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    if log_filename:
        # ä½¿ç”¨å›ºå®šçš„æ—¥å¿—æ–‡ä»¶å
        log_file = log_dir / log_filename
    else:
        # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶åï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{prefix}_{timestamp}.log"
    
    # æ¸…é™¤ä¹‹å‰çš„handlersï¼Œé¿å…é‡å¤æ—¥å¿—
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger

def create_directories(config: CS_tree_XGBoost_CS_Tree_ModelConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    dirs = [config.checkpoint_dir, config.results_dir, config.logs_dir]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

# =============================================================================
# æ•°æ®å¤„ç†åŸºç±»
# =============================================================================

class CS_tree_XGBoost_CS_Tree_ModelDataLoaderManager:
    """CS_tree_XGBoost_CS_Tree_Modelæ•°æ®åŠ è½½å™¨ç®¡ç†ç±»"""
    
    def __init__(self, data_config_path: str, config: CS_tree_XGBoost_CS_Tree_ModelConfig):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
        
        Args:
            data_config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«data_splitä¿¡æ¯
            config: æ¨¡å‹é…ç½®
        """
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.data_split_info = self.data_config.get('data_split', {})
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ï¼Œé»˜è®¤ä½¿ç”¨Phase 1é…ç½®"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰phase_1é…ç½®ï¼Œä½¿ç”¨å®ƒä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²ä¿¡æ¯
            if 'phase_1' in config:
                phase_1_config = config['phase_1']
                # æ„å»ºæ ‡å‡†çš„data_splitæ ¼å¼
                config['data_split'] = {
                    "workflow_type": "time_series",
                    "train": {
                        "start_date": phase_1_config['train_data']['start_date'],
                        "end_date": phase_1_config['train_data']['end_date'],
                        "samples": phase_1_config['train_data']['samples'],
                        "duration_years": phase_1_config['train_data']['duration_years'],
                        "file": phase_1_config['train_data']['file']
                    },
                    "validation": {
                        "start_date": phase_1_config['valid_data']['start_date'],
                        "end_date": phase_1_config['valid_data']['end_date'],
                        "samples": phase_1_config['valid_data']['samples'],
                        "duration_years": phase_1_config['valid_data']['duration_years'],
                        "file": phase_1_config['valid_data']['file']
                    },
                    "test": {
                        "start_date": phase_1_config['test_data']['start_date'],
                        "end_date": phase_1_config['test_data']['end_date'],
                        "samples": phase_1_config['test_data']['samples'],
                        "duration_years": phase_1_config['test_data']['duration_years'],
                        "file": phase_1_config['test_data']['file']
                    }
                }
                print(f"âœ… ä½¿ç”¨Phase 1é…ç½®ä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²")
                print(f"  è®­ç»ƒæœŸ: {config['data_split']['train']['start_date']} åˆ° {config['data_split']['train']['end_date']}")
                print(f"  éªŒè¯æœŸ: {config['data_split']['validation']['start_date']} åˆ° {config['data_split']['validation']['end_date']}")
                print(f"  æµ‹è¯•æœŸ: {config['data_split']['test']['start_date']} åˆ° {config['data_split']['test']['end_date']}")
            
            return config
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path}: {e}")
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_split_info
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨ï¼Œç¡®ä¿batch sizeä¸ä¸º1ä»¥é¿å…BatchNormé—®é¢˜"""
        # ç¡®ä¿batch sizeè‡³å°‘ä¸º2ï¼Œé¿å…BatchNormåœ¨å•æ ·æœ¬æ—¶å‡ºé”™
        actual_batch_size = max(batch_size, 2)
        
        # å¦‚æœæ•°æ®é›†å¤§å°å°äºbatch_sizeï¼Œè°ƒæ•´batch_size
        if len(dataset) < actual_batch_size:
            actual_batch_size = max(len(dataset), 1)
            print(f"âš ï¸ æ•°æ®é›†å¤§å°({len(dataset)})å°äºæ‰¹æ¬¡å¤§å°ï¼Œè°ƒæ•´ä¸º: {actual_batch_size}")
        
        dataloader = DataLoader(
            dataset,
            batch_size=actual_batch_size,
            shuffle=shuffle,
            num_workers=4,  # æ•°æ®åŠ è½½ä¼˜åŒ–
            pin_memory=True,  # GPUå†…å­˜ä¼˜åŒ–
            persistent_workers=True,  # æ•°æ®åŠ è½½ä¼˜åŒ–
            prefetch_factor=2,  # æ•°æ®åŠ è½½ä¼˜åŒ–
            drop_last=False  # ä¸ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
        )
        return dataloader

    def _load_parquet_data(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½parquetæ•°æ®æ–‡ä»¶"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        df = pd.read_parquet(file_path)
        print(f"ğŸ“Š åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
        print(f"   æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"   ç‰¹å¾åˆ—æ•°: {df.shape[1] - 1}")  # å‡å»æ ‡ç­¾åˆ—
        
        return df

    def get_input_dim_from_dataframe(self, df: pd.DataFrame) -> int:
        """ä»DataFrameè·å–ç‰¹å¾ç»´åº¦ï¼ˆæ’é™¤æ ‡ç­¾åˆ—å’Œéæ•°å€¼åˆ—ï¼‰"""
        # æ’é™¤éæ•°å€¼åˆ—å’Œæ ‡ç­¾åˆ—
        exclude_cols = ['symbol', 'date', 'time', 'code', 'fut_code', 'exchange', 'industry_name', 'class']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        features = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {features}")
        return features

    def validate_input_dimensions(self, config: CS_tree_XGBoost_CS_Tree_ModelConfig, actual_input_dim: int) -> int:
        """éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„è¾“å…¥ç»´åº¦ä¸å®é™…æ•°æ®æ˜¯å¦ä¸€è‡´"""
        config_input_dim = getattr(config, 'input_dim', None)
        
        if config_input_dim is None:
            print(f"ğŸ“‹ é…ç½®æ–‡ä»¶ä¸­æœªæŒ‡å®šinput_dimï¼Œä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        if config_input_dim != actual_input_dim:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…ï¼é…ç½®: {config_input_dim}, å®é™…: {actual_input_dim}")
            print(f"ğŸ”§ ä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        print(f"âœ… è¾“å…¥ç»´åº¦éªŒè¯é€šè¿‡: {actual_input_dim}")
        return actual_input_dim

    def load_data_loaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        åŠ è½½çœŸå®æ•°æ®å¹¶åˆ›å»ºæ•°æ®åŠ è½½å™¨

        Args:
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            train_loader, validation_loader, test_loader
        """
        # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
        data_split = self.get_data_split_info()
        
        # æ„å»ºæ•°æ®æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        base_data_path = "/home/feng.hao.jie/deployment/model_explorer/b_model_reproduction_agent/data/feature_set"
        
        train_file = os.path.join(base_data_path, data_split['train']['file'])
        valid_file = os.path.join(base_data_path, data_split['validation']['file'])
        test_file = os.path.join(base_data_path, data_split['test']['file'])
        
        # åŠ è½½æ•°æ®
        train_df = self._load_parquet_data(train_file)
        valid_df = self._load_parquet_data(valid_file)
        test_df = self._load_parquet_data(test_file)
        
        # åŠ¨æ€æ£€æµ‹ç‰¹å¾ç»´åº¦
        input_dim = self.get_input_dim_from_dataframe(train_df)
        self.validate_input_dimensions(self.config, input_dim)
        
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾ - åªä½¿ç”¨æ•°å€¼åˆ—ï¼Œæ’é™¤æ ‡ç­¾åˆ—
        exclude_cols = ['symbol', 'date', 'time', 'code', 'fut_code', 'exchange', 'industry_name', 'class']
        feature_cols = [col for col in train_df.columns if col not in exclude_cols]
        print(f"ğŸ” ç‰¹å¾åˆ—: {len(feature_cols)} ä¸ª")
        print(f"ğŸ” æ’é™¤åˆ—: {exclude_cols}")
        
        X_train = train_df[feature_cols].values
        y_train = train_df['class'].values
        
        X_valid = valid_df[feature_cols].values
        y_valid = valid_df['class'].values
        
        X_test = test_df[feature_cols].values
        y_test = test_df['class'].values
        
        # æ•°æ®é¢„å¤„ç† - æ·»åŠ NaNæ£€æŸ¥å’Œå¤„ç†
        print(f"ğŸ” æ•°æ®é¢„å¤„ç†å‰æ£€æŸ¥:")
        print(f"   è®­ç»ƒé›†NaNæ•°é‡: {np.isnan(X_train).sum()}")
        print(f"   éªŒè¯é›†NaNæ•°é‡: {np.isnan(X_valid).sum()}")
        print(f"   æµ‹è¯•é›†NaNæ•°é‡: {np.isnan(X_test).sum()}")
        
        # å¤„ç†NaNå€¼ - ç”¨0å¡«å……
        X_train = np.nan_to_num(X_train, nan=0.0, posinf=0.0, neginf=0.0)
        X_valid = np.nan_to_num(X_valid, nan=0.0, posinf=0.0, neginf=0.0)
        X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_valid_scaled = self.scaler.transform(X_valid)
        X_test_scaled = self.scaler.transform(X_test)
        
        # æ•°æ®é¢„å¤„ç†åæ£€æŸ¥
        print(f"ğŸ” æ•°æ®é¢„å¤„ç†åæ£€æŸ¥:")
        print(f"   è®­ç»ƒé›†NaNæ•°é‡: {np.isnan(X_train_scaled).sum()}")
        print(f"   éªŒè¯é›†NaNæ•°é‡: {np.isnan(X_valid_scaled).sum()}")
        print(f"   æµ‹è¯•é›†NaNæ•°é‡: {np.isnan(X_test_scaled).sum()}")
        
        # å†æ¬¡å¤„ç†å¯èƒ½çš„NaNå€¼
        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        X_valid_scaled = np.nan_to_num(X_valid_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train_scaled)
        y_train_tensor = torch.LongTensor(y_train)
        
        X_valid_tensor = torch.FloatTensor(X_valid_scaled)
        y_valid_tensor = torch.LongTensor(y_valid)
        
        X_test_tensor = torch.FloatTensor(X_test_scaled)
        y_test_tensor = torch.LongTensor(y_test)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self._create_dataloader(train_dataset, shuffle=True, batch_size=batch_size)
        valid_loader = self._create_dataloader(valid_dataset, shuffle=False, batch_size=batch_size)
        test_loader = self._create_dataloader(test_dataset, shuffle=False, batch_size=batch_size)
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ")
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(valid_dataset)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_loader, valid_loader, test_loader
    
    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç›¸å…³ä¿¡æ¯"""
        return {
            "workflow_type": self.data_split_info.get("workflow_type", "unknown"),
            "train_period": self.data_split_info.get("train", {}),
            "validation_period": self.data_split_info.get("validation", {}),
            "test_period": self.data_split_info.get("test", {}),
            "data_config": self.data_config
        }

# =============================================================================
# æ¨¡å‹åŸºç±»
# =============================================================================

class BaseModel(nn.Module, ABC):
    """æ¨¡å‹åŸºç±»"""
    
    def __init__(self, config: CS_tree_XGBoost_CS_Tree_ModelConfig):
        super().__init__()
        self.config = config
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024)  # å‡è®¾float32
        }

# =============================================================================
# CS_tree_XGBoost_CS_Tree_Modelæ¨¡å‹å®ç°
# =============================================================================

class CS_tree_XGBoost_CS_Tree_ModelModel(BaseModel):
    """CS_tree_XGBoost_CS_Tree_Modelæ¨¡å‹å®ç°"""
    
    def __init__(self, config: CS_tree_XGBoost_CS_Tree_ModelConfig):
        super().__init__(config)
        
        # è¾“å…¥ç»´åº¦å¿…é¡»ä»é…ç½®ä¸­è·å–ï¼Œä¸èƒ½ç¡¬ç¼–ç 
        self.input_dim = getattr(config, 'input_dim', None)
        
        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰input_dimï¼Œå¿…é¡»åœ¨è®­ç»ƒæ—¶åŠ¨æ€è®¾ç½®
        if self.input_dim is None:
            print("âš ï¸ é…ç½®ä¸­æœªæŒ‡å®šinput_dimï¼Œå°†åœ¨æ•°æ®åŠ è½½æ—¶è‡ªåŠ¨æ£€æµ‹")
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç½‘ç»œå±‚ï¼Œç­‰å¾…input_dimç¡®å®š
        self.layers = None
        
        # XGBoostæ¨¡å‹ï¼ˆç”¨äºæ··åˆæ¶æ„ï¼‰
        self.xgb_model = None
        
    def _build_layers(self, input_dim: int):
        """ä½¿ç”¨åŠ¨æ€è¾“å…¥ç»´åº¦æ„å»ºç½‘ç»œï¼Œç»å¯¹ä¸èƒ½ç¡¬ç¼–ç """
        print(f"ğŸ”§ æ„å»ºç½‘ç»œå±‚ï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
        return nn.Sequential(
            nn.Linear(input_dim, self.config.hidden_dim),  # åŠ¨æ€input_dim
            nn.LayerNorm(self.config.hidden_dim),  # ä½¿ç”¨LayerNormæ›¿ä»£BatchNormé¿å…batch size=1çš„é—®é¢˜
            nn.ReLU(),
            nn.Dropout(self.config.dropout),
            nn.Linear(self.config.hidden_dim, self.config.hidden_dim // 2),
            nn.LayerNorm(self.config.hidden_dim // 2),  # ä½¿ç”¨LayerNormæ›¿ä»£BatchNorm
            nn.ReLU(),
            nn.Dropout(self.config.dropout),
            nn.Linear(self.config.hidden_dim // 2, self.config.output_dim)
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºç½‘ç»œ
        if self.layers is None:
            if self.input_dim is None:
                self.input_dim = x.shape[-1]  # ä»è¾“å…¥æ•°æ®è‡ªåŠ¨æ£€æµ‹
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç»´åº¦: {self.input_dim}")
            self.layers = self._build_layers(self.input_dim)
            # ç¡®ä¿å±‚åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.layers = self.layers.to(x.device)
        
        # æ·»åŠ è¾“å…¥æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(x).any() or torch.isinf(x).any():
            print(f"âš ï¸ æ£€æµ‹åˆ°è¾“å…¥åŒ…å«NaNæˆ–Infå€¼ï¼Œè¿›è¡Œä¿®å¤")
            x = torch.nan_to_num(x, nan=0.0, posinf=1e6, neginf=-1e6)
        
        # æ£€æŸ¥batch sizeï¼Œå¦‚æœä¸º1ä¸”æ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼ï¼Œåˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼è¿›è¡Œå‰å‘ä¼ æ’­
        original_training = self.training
        if x.shape[0] == 1 and self.training:
            print(f"âš ï¸ æ£€æµ‹åˆ°batch sizeä¸º1ï¼Œä¸´æ—¶åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼é¿å…LayerNormé—®é¢˜")
            self.eval()
        
        try:
            output = self.layers(x)
        finally:
            # æ¢å¤åŸå§‹è®­ç»ƒæ¨¡å¼
            if original_training and not self.training:
                self.train()
        
        # æ·»åŠ è¾“å‡ºæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(output).any() or torch.isinf(output).any():
            print(f"âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºåŒ…å«NaNæˆ–Infå€¼ï¼Œè¿›è¡Œä¿®å¤")
            output = torch.nan_to_num(output, nan=0.0, posinf=1e6, neginf=-1e6)
        
        return output
    
    def init_xgboost_model(self):
        """åˆå§‹åŒ–XGBoostæ¨¡å‹"""
        self.xgb_model = xgb.XGBClassifier(
            n_estimators=self.config.n_estimators,
            max_depth=self.config.max_depth,
            learning_rate=self.config.learning_rate,
            subsample=self.config.subsample,
            colsample_bytree=self.config.colsample_bytree,
            colsample_bylevel=self.config.colsample_bylevel,
            colsample_bynode=self.config.colsample_bynode,
            reg_alpha=self.config.reg_alpha,
            reg_lambda=self.config.reg_lambda,
            gamma=self.config.gamma,
            min_child_weight=self.config.min_child_weight,
            objective=self.config.objective,
            eval_metric=self.config.eval_metric,
            tree_method=self.config.tree_method,
            random_state=self.config.random_state,
            n_jobs=self.config.n_jobs,
            verbosity=self.config.verbosity
        )

# =============================================================================
# è®­ç»ƒå™¨ç±»
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config: CS_tree_XGBoost_CS_Tree_ModelConfig, model: BaseModel, 
                 data_loader_manager: CS_tree_XGBoost_CS_Tree_ModelDataLoaderManager, 
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir
        
        # è®¾å¤‡è®¾ç½®
        self.device = setup_device(getattr(config, 'device', 'auto'))
        self.rank = 0
        self.world_size = 1
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logging(config.logs_dir, "training")
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config.use_mixed_precision and torch.cuda.is_available() else None
        self.use_amp = config.use_mixed_precision and torch.cuda.is_available()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_score = 0.0
        self.train_history = []
        
        # Checkpointè·Ÿè¸ªå™¨
        self.checkpoint_tracker = {
            'global_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'early_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'mid_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'late_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None}
        }
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(config.seed)
        
        # åˆ›å»ºç›®å½•
        create_directories(config)
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡å¹¶è®°å½•GPUä¿¡æ¯
        self.model.to(self.device)
        self._log_device_info()
        
        # è®°å½•æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if self.use_amp:
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_info = self.data_loader_manager.get_data_info()
        self.logger.info(f"æ•°æ®åˆ†å‰²ä¿¡æ¯: {data_info['workflow_type']}")
        self.logger.info(f"è®­ç»ƒæœŸé—´: {data_info['train_period']}")
        self.logger.info(f"éªŒè¯æœŸé—´: {data_info['validation_period']}")
        self.logger.info(f"æµ‹è¯•æœŸé—´: {data_info['test_period']}")
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡å’ŒGPUä½¿ç”¨ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            
            self.logger.info(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name}")
            self.logger.info(f"ğŸ“Š GPUæ€»å†…å­˜: {gpu_memory:.1f}GB")
            self.logger.info(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {current_memory:.2f}GB")
            self.logger.info(f"ğŸ“Š å¯ç”¨å†…å­˜: {gpu_memory - current_memory:.2f}GB")
        else:
            self.logger.info("ğŸ–¥ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            
        # è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f}MB")
    
    def setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # ç¡®ä¿æ¨¡å‹å·²ç»åˆå§‹åŒ–
        if hasattr(self.model, 'layers') and self.model.layers is None:
            # å¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰åˆå§‹åŒ–å±‚ï¼Œå…ˆç”¨ä¸€ä¸ªdummyè¾“å…¥æ¥åˆå§‹åŒ–
            if self.config.input_dim:
                dummy_input = torch.randn(1, self.config.input_dim).to(self.device)
                _ = self.model(dummy_input)  # è§¦å‘å±‚çš„åˆå§‹åŒ–
        
        # ä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        safe_lr = min(self.config.learning_rate, 0.001)  # é™åˆ¶æœ€å¤§å­¦ä¹ ç‡
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=safe_lr,
            eps=1e-8,  # å¢åŠ æ•°å€¼ç¨³å®šæ€§
            weight_decay=1e-5  # æ·»åŠ æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        if safe_lr != self.config.learning_rate:
            self.logger.info(f"ğŸ”§ ä¸ºæ•°å€¼ç¨³å®šæ€§è°ƒæ•´å­¦ä¹ ç‡: {self.config.learning_rate} -> {safe_lr}")
        
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', patience=5, factor=0.5
        )
        
        if self.config.model_type == "classification":
            self.criterion = nn.CrossEntropyLoss()
        else:
            self.criterion = nn.MSELoss()
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with autocast():
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª - ä½¿ç”¨æ›´ä¿å®ˆçš„å€¼
                gradient_clip = getattr(self.config, 'gradient_clip_value', 0.5)
                gradient_clip = min(gradient_clip, 0.5)  # é™åˆ¶æœ€å¤§æ¢¯åº¦è£å‰ªå€¼
                self.scaler.unscale_(self.optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), gradient_clip)
                
                # ä¼˜åŒ–å™¨æ­¥è¿›
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # æ ‡å‡†è®­ç»ƒ
                output = self.model(data)
                loss = self.criterion(output, target)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª - ä½¿ç”¨æ›´ä¿å®ˆçš„å€¼
                gradient_clip = getattr(self.config, 'gradient_clip_value', 0.5)
                gradient_clip = min(gradient_clip, 0.5)  # é™åˆ¶æœ€å¤§æ¢¯åº¦è£å‰ªå€¼
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), gradient_clip)
                
                self.optimizer.step()
            
            total_loss += loss.item()
            
            if self.config.model_type == "classification":
                # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                if torch.isnan(output).any() or torch.isinf(output).any():
                    self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºåŒ…å«NaNæˆ–Infå€¼ï¼Œè¿›è¡Œä¿®å¤")
                    output = torch.nan_to_num(output, nan=0.0, posinf=1e6, neginf=-1e6)
                
                probs = torch.softmax(output, dim=1)
                
                # æ£€æŸ¥æ¦‚ç‡æ˜¯å¦åŒ…å«NaN
                if torch.isnan(probs).any():
                    self.logger.warning(f"âš ï¸ æ£€æµ‹åˆ°softmaxè¾“å‡ºåŒ…å«NaNå€¼ï¼Œè¿›è¡Œä¿®å¤")
                    probs = torch.nan_to_num(probs, nan=0.5)  # ç”¨0.5å¡«å……NaNæ¦‚ç‡
                
                preds = torch.argmax(output, dim=1)
                all_probs.extend(probs.detach().cpu().numpy())
                all_preds.extend(preds.detach().cpu().numpy())
                all_targets.extend(target.detach().cpu().numpy())
        
        metrics = {"loss": total_loss / len(train_loader)}
        
        if self.config.model_type == "classification":
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
            # è®¡ç®—è®­ç»ƒAUC - æ·»åŠ NaNæ£€æŸ¥
            if len(np.unique(all_targets)) == 2:  # äºŒåˆ†ç±»
                all_probs_array = np.array(all_probs)
                if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                    # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
                    prob_positive = all_probs_array[:, 1]
                    targets_array = np.array(all_targets)
                    
                    # è¿‡æ»¤NaNå€¼
                    valid_mask = ~(np.isnan(prob_positive) | np.isnan(targets_array))
                    if valid_mask.sum() > 0:
                        try:
                            metrics["auc"] = roc_auc_score(targets_array[valid_mask], prob_positive[valid_mask])
                        except ValueError as e:
                            self.logger.warning(f"âš ï¸ è®­ç»ƒAUCè®¡ç®—å¤±è´¥: {e}")
                            metrics["auc"] = 0.5  # é»˜è®¤å€¼
                    else:
                        self.logger.warning(f"âš ï¸ æ‰€æœ‰æ¦‚ç‡å€¼éƒ½æ˜¯NaNï¼Œè®¾ç½®AUCä¸ºé»˜è®¤å€¼")
                        metrics["auc"] = 0.5
        
        return metrics
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                # æ··åˆç²¾åº¦æ¨ç†
                if self.use_amp:
                    with autocast():
                        output = self.model(data)
                        loss = self.criterion(output, target)
                else:
                    output = self.model(data)
                    loss = self.criterion(output, target)
                
                total_loss += loss.item()
                
                if self.config.model_type == "classification":
                    # æ·»åŠ æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
                    if torch.isnan(output).any() or torch.isinf(output).any():
                        self.logger.warning(f"âš ï¸ éªŒè¯æ—¶æ£€æµ‹åˆ°æ¨¡å‹è¾“å‡ºåŒ…å«NaNæˆ–Infå€¼ï¼Œè¿›è¡Œä¿®å¤")
                        output = torch.nan_to_num(output, nan=0.0, posinf=1e6, neginf=-1e6)
                    
                    probs = torch.softmax(output, dim=1)
                    
                    # æ£€æŸ¥æ¦‚ç‡æ˜¯å¦åŒ…å«NaN
                    if torch.isnan(probs).any():
                        self.logger.warning(f"âš ï¸ éªŒè¯æ—¶æ£€æµ‹åˆ°softmaxè¾“å‡ºåŒ…å«NaNå€¼ï¼Œè¿›è¡Œä¿®å¤")
                        probs = torch.nan_to_num(probs, nan=0.5)  # ç”¨0.5å¡«å……NaNæ¦‚ç‡
                    
                    preds = torch.argmax(output, dim=1)
                    all_probs.extend(probs.detach().cpu().numpy())
                    all_preds.extend(preds.detach().cpu().numpy())
                    all_targets.extend(target.detach().cpu().numpy())
        
        metrics = {"loss": total_loss / len(val_loader)}
        
        if self.config.model_type == "classification":
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
            metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
            
            # è®¡ç®—AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡è¾“å‡ºï¼‰- æ·»åŠ å¼ºåŒ–çš„NaNæ£€æŸ¥
            if len(all_probs) > 0 and len(np.unique(all_targets)) == 2:
                try:
                    all_probs_array = np.array(all_probs)
                    if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                        # æ£€æŸ¥å¹¶å¤„ç†NaNå€¼
                        prob_positive = all_probs_array[:, 1]
                        targets_array = np.array(all_targets)
                        
                        # è¿‡æ»¤NaNå’ŒInfå€¼
                        valid_mask = ~(np.isnan(prob_positive) | np.isnan(targets_array) | 
                                     np.isinf(prob_positive) | np.isinf(targets_array))
                        
                        if valid_mask.sum() > 0 and len(np.unique(targets_array[valid_mask])) == 2:
                            metrics["auc"] = roc_auc_score(targets_array[valid_mask], prob_positive[valid_mask])
                        else:
                            self.logger.warning(f"âš ï¸ éªŒè¯AUCè®¡ç®—å¤±è´¥ï¼šæœ‰æ•ˆæ ·æœ¬ä¸è¶³æˆ–ç±»åˆ«ä¸è¶³")
                            metrics["auc"] = 0.5  # é»˜è®¤å€¼
                except Exception as e:
                    self.logger.warning(f"âš ï¸ éªŒè¯AUCè®¡ç®—å¼‚å¸¸: {e}")
                    metrics["auc"] = 0.5  # é»˜è®¤å€¼
            
            # æ·»åŠ é¢„æµ‹æ•°æ®åˆ°metricsä¸­ï¼Œä¾›åç»­ä½¿ç”¨
            metrics["y_true"] = all_targets
            metrics["y_prob"] = all_probs
        
        return metrics
    
    def save_checkpoint_if_best(self, epoch: int, val_loss: float):
        """ä¿å­˜æœ€ä½³checkpointçš„ç­–ç•¥"""
        # æ›´æ–°å…¨å±€æœ€ä½³
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        # æ›´æ–°åŒºé—´æœ€ä½³ï¼ˆå®æ—¶æ›´æ–°best_model.pthï¼‰
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        # åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³ä¸ºinterval_best
        if epoch == 29:  # epochä»0å¼€å§‹ï¼Œæ‰€ä»¥29æ˜¯ç¬¬30ä¸ªepoch
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')

    def _save_checkpoint(self, epoch: int, val_loss: float, checkpoint_type: str):
        """ä¿å­˜checkpoint"""
        # åˆ é™¤æ—§checkpoint
        old_path = self.checkpoint_tracker[checkpoint_type]['path']
        if old_path and os.path.exists(old_path):
            os.remove(old_path)
        
        # ä¿å­˜æ–°checkpoint
        checkpoint_name = f"checkpoint_{checkpoint_type}_epoch{epoch:03d}_val{val_loss:.4f}.pt"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': val_loss,
            'checkpoint_type': checkpoint_type
        }, checkpoint_path)
        
        self.checkpoint_tracker[checkpoint_type] = {
            'epoch': epoch,
            'val_loss': val_loss,
            'path': checkpoint_path
        }
        self.logger.info(f"ğŸ’¾ ä¿å­˜{checkpoint_type} checkpoint: epoch {epoch}, val_loss {val_loss:.4f}")

    def _save_interval_best(self, checkpoint_type: str, epoch_range: str):
        """åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³checkpointä¸ºinterval_best"""
        # è·å–å½“å‰åŒºé—´æœ€ä½³çš„checkpointè·¯å¾„
        source_path = self.checkpoint_tracker[checkpoint_type]['path']
        
        if source_path and os.path.exists(source_path):
            # åˆ›å»ºinterval_bestæ–‡ä»¶å
            interval_name = f"interval_best_{epoch_range}.pt"
            interval_path = os.path.join(self.checkpoint_dir, interval_name)
            
            # å¤åˆ¶checkpoint
            shutil.copy2(source_path, interval_path)
            
            epoch = self.checkpoint_tracker[checkpoint_type]['epoch']
            val_loss = self.checkpoint_tracker[checkpoint_type]['val_loss']
            
            self.logger.info(f"ğŸ“¦ å¤åˆ¶åŒºé—´æœ€ä½³ [{epoch_range}]: epoch {epoch}, val_loss {val_loss:.4f}")
            self.logger.info(f"   æºæ–‡ä»¶: {os.path.basename(source_path)}")
            self.logger.info(f"   ç›®æ ‡æ–‡ä»¶: {interval_name}")
        else:
            self.logger.warning(f"âš ï¸ åŒºé—´ [{epoch_range}] æ²¡æœ‰æ‰¾åˆ°æœ€ä½³checkpoint")
    
    def train(self, batch_size: int = None, no_save_model: bool = False) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        # è·å–æ•°æ®åŠ è½½å™¨ï¼Œç¡®ä¿batch sizeè‡³å°‘ä¸º2
        batch_size = batch_size or self.config.batch_size
        batch_size = max(batch_size, 2)  # ç¡®ä¿batch sizeè‡³å°‘ä¸º2
        train_loader, val_loader, test_loader = self.data_loader_manager.load_data_loaders(
            batch_size=batch_size)
        
        self.setup_training_components()
        
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {self.model.get_model_info()}")
        
        start_time = time.time()
        epochs = self.config.epochs
        
        # ç”¨äºä¿å­˜æœ€åä¸€ä¸ªepochçš„éªŒè¯é¢„æµ‹æ•°æ®
        final_val_predictions = None
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)

            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # å¦‚æœæ˜¯æœ€åä¸€ä¸ªepochï¼Œä¿å­˜éªŒè¯é¢„æµ‹æ•°æ®
            if epoch == epochs - 1 and self.config.model_type == "classification":
                final_val_predictions = {
                    "y_true_val": val_metrics.get("y_true", []),
                    "y_prob_val": val_metrics.get("y_prob", [])
                }
            
            # è®°å½•å†å²
            epoch_info = {
                "epoch": epoch + 1,
                "train_loss": train_metrics["loss"],
                "val_loss": val_metrics["loss"],
                "lr": self.optimizer.param_groups[0]['lr'],
                "time": time.time() - epoch_start
            }
            
            if self.config.model_type == "classification":
                epoch_info.update({
                    "train_acc": train_metrics.get("accuracy", 0),
                    "val_acc": val_metrics.get("accuracy", 0),
                    "val_f1": val_metrics.get("f1", 0),
                    "train_auc": train_metrics.get("auc", 0),
                    "val_auc": val_metrics.get("auc", 0)
                })
                current_score = val_metrics.get("accuracy", 0)
            else:
                current_score = -val_metrics["loss"]  # å›å½’ä»»åŠ¡ä½¿ç”¨è´ŸæŸå¤±
            
            self.train_history.append(epoch_info)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(current_score)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_score > self.best_val_score:
                self.best_val_score = current_score
                if not no_save_model:
                    self.save_checkpoint(epoch + 1, is_best=True)
            
            # ä¿å­˜checkpointç­–ç•¥
            if not no_save_model:
                self.save_checkpoint_if_best(epoch, val_metrics["loss"])
            
            # GPUå†…å­˜ç›‘æ§ï¼ˆæ¯10ä¸ªepochè®°å½•ä¸€æ¬¡ï¼‰
            if self.device.type == 'cuda' and (epoch + 1) % 10 == 0:
                current_memory = torch.cuda.memory_allocated() / 1024**3
                max_memory = torch.cuda.max_memory_allocated() / 1024**3
                cached_memory = torch.cuda.memory_reserved() / 1024**3
                self.logger.info(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨ (Epoch {epoch+1}): å½“å‰ {current_memory:.2f}GB, å³°å€¼ {max_memory:.2f}GB, ç¼“å­˜ {cached_memory:.2f}GB")
            
            # æ—¥å¿—è¾“å‡º
            log_msg = (
                f"Epoch {epoch+1}/{epochs} - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}"
            )

            if self.config.model_type == "classification":
                log_msg += f", Val Acc: {val_metrics.get('accuracy', 0):.4f}"
                if 'auc' in val_metrics:
                    log_msg += f", Val AUC: {val_metrics['auc']:.4f}"
                if 'auc' in train_metrics:
                    log_msg += f", Train AUC: {train_metrics['auc']:.4f}"

            log_msg += f", Time: {epoch_info['time']:.2f}s"
            self.logger.info(log_msg)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        result = {
            "best_val_score": self.best_val_score,
            "total_epochs": len(self.train_history),
            "total_time": total_time,
            "train_history": self.train_history
        }
        
        # æ·»åŠ æœ€åä¸€ä¸ªepochçš„éªŒè¯é¢„æµ‹æ•°æ®
        if final_val_predictions:
            result.update(final_val_predictions)
        
        return result
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_score': self.best_val_score,
            'config': asdict(self.config)
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = Path(self.config.checkpoint_dir) / "latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = Path(self.config.checkpoint_dir) / "best_model.pth"
            torch.save(checkpoint, best_path)
            self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = Path(self.config.results_dir) / "training_history.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        self.logger.info(f"è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

# =============================================================================
# æ¨ç†å™¨ç±»
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, config: CS_tree_XGBoost_CS_Tree_ModelConfig, model: BaseModel, 
                 data_loader_manager: CS_tree_XGBoost_CS_Tree_ModelDataLoaderManager):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.device = setup_device()
        self.logger = setup_logging(config.logs_dir, "inference")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)
        self.model.eval()
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        return checkpoint
    
    def predict_batch(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """æ‰¹é‡æ¨ç†"""
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in data_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                if self.config.model_type == "classification":
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(outputs, dim=1)
                    all_probs.extend(probs.detach().cpu().numpy())
                    all_preds.extend(preds.detach().cpu().numpy())
                else:
                    all_preds.extend(outputs.detach().cpu().numpy())
                    all_probs.extend(outputs.detach().cpu().numpy())  # å›å½’ä»»åŠ¡æ¦‚ç‡å³ä¸ºé¢„æµ‹å€¼
        
        return np.array(all_preds), np.array(all_probs)
    
    def predict_single(self, data: np.ndarray) -> Tuple[Union[int, float], np.ndarray]:
        """å•æ ·æœ¬æ¨ç†"""
        data_tensor = torch.FloatTensor(data).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            output = self.model(data_tensor)
            
            if self.config.model_type == "classification":
                probs = torch.softmax(output, dim=1)
                pred = torch.argmax(output, dim=1).item()
                return pred, probs.cpu().numpy()[0]
            else:
                pred = output.item()
                return pred, np.array([pred])
    
    def evaluate(self) -> Dict[str, float]:
        """æ¨¡å‹è¯„ä¼°"""
        # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                if self.config.model_type == "classification":
                    probs = torch.softmax(outputs, dim=1)
                    preds = torch.argmax(outputs, dim=1)
                    all_probs.extend(probs.detach().cpu().numpy())
                    all_preds.extend(preds.detach().cpu().numpy())
                else:
                    all_preds.extend(outputs.detach().cpu().numpy())
                
                all_targets.extend(target.detach().cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        
        if self.config.model_type == "classification":
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
            metrics["precision"] = precision_score(all_targets, all_preds, average='weighted')
            metrics["recall"] = recall_score(all_targets, all_preds, average='weighted')
            metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
            
            # AUCæŒ‡æ ‡è®¡ç®—
            if len(np.unique(all_targets)) == 2:  # äºŒåˆ†ç±»
                all_probs_positive = np.array(all_probs)[:, 1]
                metrics["auc"] = roc_auc_score(all_targets, all_probs_positive)
            elif len(np.unique(all_targets)) > 2:  # å¤šåˆ†ç±»
                try:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs, multi_class='ovr', average='weighted')
                except ValueError:
                    # å¦‚æœæ— æ³•è®¡ç®—å¤šåˆ†ç±»AUCï¼Œè·³è¿‡
                    pass
        else:
            from sklearn.metrics import mean_squared_error, mean_absolute_error
            metrics["mse"] = mean_squared_error(all_targets, all_preds)
            metrics["mae"] = mean_absolute_error(all_targets, all_preds)
            metrics["rmse"] = np.sqrt(metrics["mse"])
        
        return metrics
    
    def generate_predictions(self, start_date: str = None, end_date: str = None, 
                           output_path: str = None, output_format: str = "parquet") -> pd.DataFrame:
        """ç”Ÿæˆé¢„æµ‹æ•°æ®æ–‡ä»¶ç”¨äºå›æµ‹"""
        # 1. è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_split = self.data_loader_manager.get_data_split_info()
        
        # 2. ç¡®å®šä½¿ç”¨çš„æ•°æ®é›†ï¼ˆæµ‹è¯•é›†ï¼‰
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        # 3. æ‰§è¡Œæ‰¹é‡é¢„æµ‹
        predictions, probabilities = self.predict_batch(test_loader)
        
        # 4. ç”Ÿæˆæ—¥æœŸåºåˆ—å¹¶å¯¹é½
        test_start = data_split['test']['start_date']
        test_end = data_split['test']['end_date']
        
        # ç”Ÿæˆæ—¥æœŸèŒƒå›´
        date_range = pd.date_range(start=test_start, end=test_end, freq='D')
        
        # ç¡®ä¿é¢„æµ‹æ•°é‡ä¸æ—¥æœŸæ•°é‡åŒ¹é…
        if len(predictions) != len(date_range):
            # å¦‚æœæ•°é‡ä¸åŒ¹é…ï¼Œæˆªå–æˆ–å¡«å……
            min_len = min(len(predictions), len(date_range))
            predictions = predictions[:min_len]
            probabilities = probabilities[:min_len]
            date_range = date_range[:min_len]
        
        # 5. åˆ›å»ºæ ‡å‡†æ ¼å¼DataFrame
        predictions_df = pd.DataFrame({
            'date': date_range,                    # é¢„æµ‹æ—¥æœŸ
            'prediction': predictions,             # é¢„æµ‹ç±»åˆ« (0æˆ–1)
            'confidence': probabilities[:, 1] if probabilities.ndim > 1 else probabilities,     # é¢„æµ‹ç½®ä¿¡åº¦
            'probability_class_0': probabilities[:, 0] if probabilities.ndim > 1 else 1 - probabilities,  # ç±»åˆ«0çš„æ¦‚ç‡
            'probability_class_1': probabilities[:, 1] if probabilities.ndim > 1 else probabilities,  # ç±»åˆ«1çš„æ¦‚ç‡
            'model_name': self.config.model_name,  # æ¨¡å‹åç§°
            'timestamp': datetime.now().isoformat()  # ç”Ÿæˆæ—¶é—´æˆ³
        })
        
        # 6. ä¿å­˜ä¸ºæŒ‡å®šæ ¼å¼æ–‡ä»¶
        if output_path:
            if output_format == "parquet":
                predictions_df.to_parquet(output_path, index=False)
            elif output_format == "csv":
                predictions_df.to_csv(output_path, index=False)
            
            self.logger.info(f"é¢„æµ‹æ–‡ä»¶å·²ä¿å­˜: {output_path}")
        
        return predictions_df
    
    def save_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, 
                        output_path: str):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        results = {
            "predictions": predictions.tolist(),
            "probabilities": probabilities.tolist(),
            "config": asdict(self.config),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: CS_tree_XGBoost_CS_Tree_ModelConfig, model: BaseModel):
        self.config = config
        self.model = model
    
    def generate_model_md(self, output_path: str = "MODEL.md"):
        """ç”ŸæˆMODEL.mdæ–‡æ¡£"""
        model_info = self.model.get_model_info()
        
        content = f"""# {self.config.model_name} Model

## æ¨¡å‹æ¦‚è¿°

{self.config.model_name} æ˜¯ä¸€ä¸ªåŸºäºXGBoostçš„{self.config.model_type}æ¨¡å‹ï¼Œç»“åˆäº†æ·±åº¦å­¦ä¹ å’Œæ¢¯åº¦æå‡æ ‘çš„ä¼˜åŠ¿ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- **ä»»åŠ¡ç±»å‹**: {self.config.model_type}
- **æ¨¡å‹å‚æ•°**: {model_info['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°**: {model_info['trainable_parameters']:,}
- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.2f} MB
- **XGBoosté›†æˆ**: æ”¯æŒXGBoostæ¢¯åº¦æå‡æ ‘ç®—æ³•

## æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

1. **ç¥ç»ç½‘ç»œç»„ä»¶**: å¤šå±‚æ„ŸçŸ¥æœºç”¨äºç‰¹å¾å­¦ä¹ 
2. **XGBoostç»„ä»¶**: æ¢¯åº¦æå‡æ ‘ç”¨äºæœ€ç»ˆé¢„æµ‹
3. **æ··åˆæ¶æ„**: ç»“åˆæ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ çš„ä¼˜åŠ¿

### ç½‘ç»œç»“æ„

```
è¾“å…¥å±‚ -> éšè—å±‚1({self.config.hidden_dim}) -> Dropout -> éšè—å±‚2({self.config.hidden_dim//2}) -> Dropout -> è¾“å‡ºå±‚({self.config.output_dim})
```

## æŠ€æœ¯åŸç†

### XGBoostå‚æ•°é…ç½®

- **n_estimators**: {self.config.n_estimators}
- **max_depth**: {self.config.max_depth}
- **learning_rate**: {self.config.learning_rate}
- **subsample**: {self.config.subsample}
- **colsample_bytree**: {self.config.colsample_bytree}

## é…ç½®å‚æ•°

### è®­ç»ƒé…ç½®
- å­¦ä¹ ç‡: {self.config.learning_rate}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- è®­ç»ƒè½®æ•°: {self.config.epochs}

### æ•°æ®é…ç½®
- åºåˆ—é•¿åº¦: {self.config.seq_len}
- é¢„æµ‹é•¿åº¦: {self.config.pred_len}
- è¾“å…¥ç»´åº¦: {self.config.input_dim or "åŠ¨æ€æ£€æµ‹"}
- è¾“å‡ºç»´åº¦: {self.config.output_dim}

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```bash
python CS_tree_XGBoost_CS_Tree_Model_unified.py train --config config.yaml --data-config data.yaml
```

### æ¨¡å‹æ¨ç†

```bash
python CS_tree_XGBoost_CS_Tree_Model_unified.py inference --config config.yaml --data-config data.yaml --checkpoint best_model.pth
```

### æ¨¡å‹è¯„ä¼°

```bash
python CS_tree_XGBoost_CS_Tree_Model_unified.py inference --config config.yaml --data-config data.yaml --checkpoint best_model.pth --inference-mode eval
```

### ç”Ÿæˆé¢„æµ‹æ–‡ä»¶

```bash
python CS_tree_XGBoost_CS_Tree_Model_unified.py inference --config config.yaml --data-config data.yaml --checkpoint best_model.pth --inference-mode test --format parquet
```

## æ€§èƒ½æŒ‡æ ‡

æ¨¡å‹æ”¯æŒä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡ï¼š
- å‡†ç¡®ç‡ (Accuracy)
- ç²¾ç¡®ç‡ (Precision)
- å¬å›ç‡ (Recall)
- F1åˆ†æ•° (F1-Score)
- AUC-ROC

## æ³¨æ„äº‹é¡¹

1. **æ•°æ®æ ¼å¼**: æ”¯æŒparquetæ ¼å¼çš„æ—¶é—´åºåˆ—æ•°æ®
2. **ç‰¹å¾ç»´åº¦**: è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
3. **GPUæ”¯æŒ**: è‡ªåŠ¨æ£€æµ‹GPUå¯ç”¨æ€§ï¼Œæ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ
4. **Checkpoint**: æ”¯æŒå¤šç§checkpointä¿å­˜ç­–ç•¥

## æ›´æ–°æ—¥å¿—

- åˆå§‹ç‰ˆæœ¬: {datetime.now().strftime('%Y-%m-%d')}
- æ”¯æŒXGBoosté›†æˆ
- æ”¯æŒåŠ¨æ€ç‰¹å¾ç»´åº¦æ£€æµ‹
- æ”¯æŒOptunaè¶…å‚æ•°ä¼˜åŒ–

"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¨¡å‹æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")

# =============================================================================
# ä¸»è¦æ¥å£å‡½æ•°
# =============================================================================

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               optuna_config_path: str = None, device: str = "auto",
               epochs_override: int = None, no_save_model: bool = False, 
               seed: int = 42, checkpoint_dir: str = "checkpoints"):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§
    import random
    import numpy as np
    import torch
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    # è®¾ç½®å›ºå®šçš„æ—¥å¿—è·¯å¾„
    log_dir = "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/training/logs/CS_tree_XGBoost_CS_Tree_Model_3982978951"
    logger = setup_logging(log_dir=log_dir, log_filename="log.txt")
    
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    logger.info(f"ğŸ’¾ Checkpointä¿å­˜ç›®å½•: {checkpoint_dir}")
    
    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # 1. åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = CS_tree_XGBoost_CS_Tree_ModelConfig()
    
    # ä»é…ç½®æ–‡ä»¶æ›´æ–°é…ç½®
    if 'hyperparameters' in config_dict:
        hp = config_dict['hyperparameters']
        config.n_estimators = hp.get('n_estimators', config.n_estimators)
        config.max_depth = hp.get('max_depth', config.max_depth)
        config.learning_rate = hp.get('learning_rate', config.learning_rate)
        config.subsample = hp.get('subsample', config.subsample)
        config.colsample_bytree = hp.get('colsample_bytree', config.colsample_bytree)
        config.colsample_bylevel = hp.get('colsample_bylevel', config.colsample_bylevel)
        config.colsample_bynode = hp.get('colsample_bynode', config.colsample_bynode)
        config.reg_alpha = hp.get('reg_alpha', config.reg_alpha)
        config.reg_lambda = hp.get('reg_lambda', config.reg_lambda)
        config.gamma = hp.get('gamma', config.gamma)
        config.min_child_weight = hp.get('min_child_weight', config.min_child_weight)
        config.objective = hp.get('objective', config.objective)
        config.eval_metric = hp.get('eval_metric', config.eval_metric)
        config.tree_method = hp.get('tree_method', config.tree_method)
        config.random_state = hp.get('random_state', config.random_state)
        config.n_jobs = hp.get('n_jobs', config.n_jobs)
        config.verbosity = hp.get('verbosity', config.verbosity)
        config.early_stopping_rounds = hp.get('early_stopping_rounds', config.early_stopping_rounds)
    
    if 'training' in config_dict:
        training = config_dict['training']
        config.batch_size = training.get('batch_size', config.batch_size)
        config.epochs = training.get('epochs', config.epochs)
    
    # åº”ç”¨Optunaé…ç½®è¦†ç›–
    if optuna_config_path and os.path.exists(optuna_config_path):
        with open(optuna_config_path, 'r', encoding='utf-8') as f:
            optuna_config = yaml.safe_load(f)
        
        # åº”ç”¨è¶…å‚æ•°è¦†ç›–
        for key, value in optuna_config.items():
            if hasattr(config, key):
                setattr(config, key, value)
                logger.info(f"ğŸ”§ Optunaè¦†ç›–å‚æ•°: {key} = {value}")
    
    # è¦†ç›–é…ç½®
    if device != "auto":
        config.device = device
    if epochs_override:
        config.epochs = epochs_override
    
    config.checkpoint_dir = checkpoint_dir
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = CS_tree_XGBoost_CS_Tree_ModelDataLoaderManager(data_config_path, config)
    
    # 3. åˆ›å»ºæ¨¡å‹
    model = CS_tree_XGBoost_CS_Tree_ModelModel(config)
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨
    trainer = UnifiedTrainer(config, model, data_loader_manager, checkpoint_dir=checkpoint_dir)
    
    # 5. æ‰§è¡Œè®­ç»ƒ
    results = trainer.train(no_save_model=no_save_model)
    
    # 6. è¾“å‡ºJSONæ ¼å¼ç»“æœ
    final_results = {
        "final_results": {
            "best_val_score": results['best_val_score'],
            "total_epochs": results['total_epochs'],
            "total_time": results['total_time'],
            "final_train_acc": results['train_history'][-1].get('train_acc', 0),
            "final_val_acc": results['train_history'][-1].get('val_acc', 0),
            "train_auc": results['train_history'][-1].get('train_auc', 0),
            "val_auc": results['train_history'][-1].get('val_auc', 0),
            "train_losses": [epoch['train_loss'] for epoch in results['train_history']],
            "val_losses": [epoch['val_loss'] for epoch in results['train_history']],
            "train_accuracies": [epoch.get('train_acc', 0) for epoch in results['train_history']],
            "val_accuracies": [epoch.get('val_acc', 0) for epoch in results['train_history']]
        }
    }
    print(json.dumps(final_results, indent=2))
    
    return results

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "best_model.pth", mode: str = "eval",
                  start_date: str = None, end_date: str = None, 
                  output_path: str = None, output_format: str = "parquet"):
    """ä¸»æ¨ç†å‡½æ•°"""
    # 1. åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = CS_tree_XGBoost_CS_Tree_ModelConfig()
    
    # ä»é…ç½®æ–‡ä»¶æ›´æ–°é…ç½®
    if 'hyperparameters' in config_dict:
        hp = config_dict['hyperparameters']
        config.n_estimators = hp.get('n_estimators', config.n_estimators)
        config.max_depth = hp.get('max_depth', config.max_depth)
        config.learning_rate = hp.get('learning_rate', config.learning_rate)
        config.subsample = hp.get('subsample', config.subsample)
        config.colsample_bytree = hp.get('colsample_bytree', config.colsample_bytree)
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = CS_tree_XGBoost_CS_Tree_ModelDataLoaderManager(data_config_path, config)
    
    # 3. åˆ›å»ºæ•°æ®åŠ è½½å™¨ä»¥è·å–è¾“å…¥ç»´åº¦
    _, _, _ = data_loader_manager.load_data_loaders()
    
    # 4. åˆ›å»ºæ¨¡å‹å¹¶åˆå§‹åŒ–
    model = CS_tree_XGBoost_CS_Tree_ModelModel(config)
    
    # 5. åˆå§‹åŒ–æ¨¡å‹å±‚ï¼ˆä½¿ç”¨dummyè¾“å…¥ï¼‰
    if config.input_dim:
        dummy_input = torch.randn(1, config.input_dim)
        _ = model(dummy_input)  # è§¦å‘å±‚çš„åˆå§‹åŒ–
    
    # 6. åˆ›å»ºæ¨ç†å™¨
    inferencer = UnifiedInferencer(config, model, data_loader_manager)
    
    # 7. åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = inferencer.load_checkpoint(checkpoint_path)
    
    # 8. æ ¹æ®modeæ‰§è¡Œä¸åŒæ“ä½œ
    if mode == "eval":
        # ä¼ ç»Ÿæ¨¡å‹è¯„ä¼°
        metrics = inferencer.evaluate()
        print("è¯„ä¼°ç»“æœ:")
        for metric, value in metrics.items():
            print(f"  {metric}: {value:.4f}")
        return metrics
    elif mode == "test":
        # ç”Ÿæˆé¢„æµ‹æ•°æ®æ–‡ä»¶ç”¨äºå›æµ‹
        if not output_path:
            output_path = f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{output_format}"
        
        predictions_df = inferencer.generate_predictions(
            start_date=start_date,
            end_date=end_date,
            output_path=output_path,
            output_format=output_format
        )
        
        print(f"é¢„æµ‹æ–‡ä»¶å·²ç”Ÿæˆ: {output_path}")
        print(f"é¢„æµ‹æ•°æ®å½¢çŠ¶: {predictions_df.shape}")
        return predictions_df

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ä¸»æ–‡æ¡£ç”Ÿæˆå‡½æ•°"""
    # 1. åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = CS_tree_XGBoost_CS_Tree_ModelConfig()
    
    # 2. åˆ›å»ºæ¨¡å‹
    model = CS_tree_XGBoost_CS_Tree_ModelModel(config)
    
    # 3. ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config, model)
    doc_generator.generate_model_md("MODEL.md")
    
    print("æ–‡æ¡£ç”Ÿæˆå®Œæˆ")

# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="CS_tree_XGBoost_CS_Tree_Modelç»Ÿä¸€æ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·")
    parser.add_argument("mode", choices=["train", "inference", "docs"], help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="config.yaml", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-config", default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default="best_model.pth", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--data", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # Optunaä¼˜åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--optuna-config", help="Optunaè¯•éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--no-save-model", action="store_true", help="ä¸ä¿å­˜æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°æ€§ï¼‰")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    
    # æ¨ç†ç›¸å…³å‚æ•°
    parser.add_argument("--inference-mode", choices=["test", "eval"], default="eval", 
                       help="æ¨ç†æ¨¡å¼ï¼štest(ç”Ÿæˆé¢„æµ‹æ–‡ä»¶) æˆ– eval(è¯„ä¼°)")
    parser.add_argument("--start-date", help="æ¨ç†å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end-date", help="æ¨ç†ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--format", choices=["parquet", "csv"], default="parquet", 
                       help="è¾“å‡ºæ ¼å¼")
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    
    if args.mode == "train":
        main_train(
            config_path=args.config,
            data_config_path=args.data_config,
            optuna_config_path=args.optuna_config,
            device=args.device,
            epochs_override=args.epochs,
            no_save_model=args.no_save_model,
            seed=args.seed,
            checkpoint_dir=args.checkpoint_dir
        )
    elif args.mode == "inference":
        main_inference(
            config_path=args.config,
            data_config_path=args.data_config,
            checkpoint_path=args.checkpoint,
            mode=args.inference_mode,
            start_date=args.start_date,
            end_date=args.end_date,
            output_path=args.output,
            output_format=args.format
        )
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data_config)