#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TKAN_unified.py - ç»Ÿä¸€TKANæ¨¡å‹å®ç°
æ•´åˆè®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”ŸæˆåŠŸèƒ½çš„ç»Ÿä¸€æ¨¡æ¿ï¼Œæ”¯æŒOptunaè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod
import math
import random

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class TKANConfig:
    """TKANæ¨¡å‹é…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "TKAN"
    model_type: str = "classification"  # classification, regression, time_series
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 100
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True  # æ··åˆç²¾åº¦è®­ç»ƒ
    dataloader_num_workers: int = 4  # æ•°æ®åŠ è½½å¹¶å‘æ•°
    pin_memory: bool = True  # å›ºå®šå†…å­˜
    gradient_clip_value: float = 1.0  # æ¢¯åº¦è£å‰ª
    
    # TKANç‰¹å®šé…ç½®
    units: int = 128
    return_sequences: bool = False
    stateful: bool = False
    dropout: float = 0.2
    recurrent_dropout: float = 0.1
    use_bias: bool = True
    num_layers: int = 2
    hidden_dims: List[int] = None
    activation: str = 'tanh'
    kan_sublayers: int = 4
    grid_size: int = 5
    spline_order: int = 3
    output_dim: int = 1
    output_activation: str = 'sigmoid'
    
    # æ•°æ®é…ç½®
    input_dim: int = None  # åŠ¨æ€è®¾ç½®
    seq_len: int = 96
    pred_len: int = 24
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    seed: int = 42
    
    # è·¯å¾„é…ç½®
    data_path: str = "./data"
    checkpoint_dir: str = "./checkpoints"
    results_dir: str = "./results"
    logs_dir: str = "./logs"
    
    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [256, 128]
        
        # éªŒè¯hidden_dimsé…ç½®
        if not isinstance(self.hidden_dims, list) or len(self.hidden_dims) == 0:
            raise ValueError("hidden_dimså¿…é¡»æ˜¯éç©ºåˆ—è¡¨")
        
        for i, dim in enumerate(self.hidden_dims):
            if not isinstance(dim, int) or dim <= 0:
                raise ValueError(f"hidden_dims[{i}]å¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå½“å‰å€¼: {dim}")
        
        # éªŒè¯å…¶ä»–å…³é”®å‚æ•°
        if self.output_dim <= 0:
            raise ValueError(f"output_dimå¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå½“å‰å€¼: {self.output_dim}")
        
        if self.grid_size <= 0:
            raise ValueError(f"grid_sizeå¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå½“å‰å€¼: {self.grid_size}")
        
        if self.spline_order <= 0:
            raise ValueError(f"spline_orderå¿…é¡»æ˜¯æ­£æ•´æ•°ï¼Œå½“å‰å€¼: {self.spline_order}")

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    import logging
    logger = logging.getLogger(__name__)

    if device_choice == "cpu":
        device = torch.device('cpu')
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
        logger.info("è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨CPU")
    elif device_choice == "cuda":
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            logger.info(f"è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨GPU - {gpu_name} ({gpu_memory:.1f}GB)")
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            cached_memory = torch.cuda.memory_reserved() / 1024**3

            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU: {gpu_name}")
            print(f"   ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f}GB æ€»é‡")
            print(f"   ğŸ“Š GPUæ•°é‡: {gpu_count}")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨: {current_memory:.2f}GB")
            print(f"   ğŸ“Š ç¼“å­˜ä½¿ç”¨: {cached_memory:.2f}GB")

            logger.info(f"è®¾å¤‡é…ç½®: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU")
            logger.info(f"GPUä¿¡æ¯: {gpu_name}, {gpu_memory:.1f}GBæ€»å†…å­˜, {gpu_count}ä¸ªGPU")
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {current_memory:.2f}GBå·²ç”¨, {cached_memory:.2f}GBç¼“å­˜")

            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            logger.info("è®¾å¤‡é…ç½®: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•è·¯å¾„
        prefix: æ—¥å¿—æ–‡ä»¶å‰ç¼€ï¼ˆå½“log_filenameä¸ºNoneæ—¶ä½¿ç”¨ï¼‰
        log_filename: å›ºå®šçš„æ—¥å¿—æ–‡ä»¶åï¼ˆå¦‚æœæä¾›ï¼Œå°†å¿½ç•¥prefixå’Œæ—¶é—´æˆ³ï¼‰
    """
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    if log_filename:
        # ä½¿ç”¨å›ºå®šçš„æ—¥å¿—æ–‡ä»¶å
        log_file = log_dir / log_filename
    else:
        # ä½¿ç”¨å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—æ–‡ä»¶åï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{prefix}_{timestamp}.log"
    
    # æ¸…é™¤ä¹‹å‰çš„handlersï¼Œé¿å…é‡å¤æ—¥å¿—
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger

def create_directories(config: TKANConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    dirs = [config.checkpoint_dir, config.results_dir, config.logs_dir]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

# =============================================================================
# TKANæ¨¡å‹æ¶æ„å®ç°
# =============================================================================

class KANLinear(nn.Module):
    """Kolmogorov-Arnold Network çº¿æ€§å±‚å®ç°"""
    
    def __init__(
        self,
        in_features: int,
        out_features: int,
        grid_size: int = 5,
        spline_order: int = 3,
        scale_noise: float = 0.1,
        scale_base: float = 1.0,
        scale_spline: float = 1.0,
        enable_standalone_scale_spline: bool = True,
        base_activation: str = 'silu',
        grid_eps: float = 0.02,
        grid_range: Tuple[float, float] = (-1, 1)
    ):
        """KAN çº¿æ€§å±‚åˆå§‹åŒ–"""
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.grid_size = grid_size
        self.spline_order = spline_order
        self.scale_noise = scale_noise
        self.scale_base = scale_base
        self.scale_spline = scale_spline
        self.enable_standalone_scale_spline = enable_standalone_scale_spline
        self.base_activation = base_activation
        self.grid_eps = grid_eps
        self.grid_range = grid_range
        
        # åˆ›å»ºç½‘æ ¼
        grid = torch.linspace(grid_range[0], grid_range[1], grid_size + 1)
        self.register_buffer('grid', grid)
        
        # åˆå§‹åŒ–å‚æ•°
        self.base_weight = nn.Parameter(torch.randn(out_features, in_features) * scale_base)
        self.spline_weight = nn.Parameter(
            torch.randn(out_features, in_features, grid_size + spline_order) * scale_spline
        )
        
        if enable_standalone_scale_spline:
            self.spline_scaler = nn.Parameter(torch.randn(out_features, in_features) * scale_spline)
        
        # åŸºç¡€æ¿€æ´»å‡½æ•°
        if base_activation == 'silu':
            self.base_activation_fn = F.silu
        elif base_activation == 'relu':
            self.base_activation_fn = F.relu
        elif base_activation == 'tanh':
            self.base_activation_fn = torch.tanh
        else:
            self.base_activation_fn = F.silu
    
    def b_splines(self, x: torch.Tensor, grid: torch.Tensor) -> torch.Tensor:
        """B-spline åŸºå‡½æ•°è®¡ç®—"""
        assert x.dim() == 2 and x.size(1) == self.in_features
        
        # ç¡®ä¿gridå’Œxåœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = x.device
        if grid.device != device:
            grid = grid.to(device)
        
        batch_size = x.size(0)
        
        # è®¡ç®—æ¯ä¸ªè¾“å…¥ç‚¹åœ¨ç½‘æ ¼ä¸Šçš„ä½ç½®
        x_expanded = x.unsqueeze(-1)  # [batch_size, in_features, 1]
        grid_expanded = grid.unsqueeze(0).unsqueeze(0).expand(batch_size, self.in_features, -1)  # [batch_size, in_features, grid_size+1]
        
        # åˆå§‹åŒ–B-splineåŸºå‡½æ•° (0é˜¶)
        bases = torch.zeros(batch_size, self.in_features, self.grid_size + self.spline_order, device=device, dtype=x.dtype)
        
        # æ‰¾åˆ°æ¯ä¸ªç‚¹æ‰€åœ¨çš„åŒºé—´
        for i in range(self.grid_size):
            if i + 1 < grid_expanded.size(-1):
                mask = (x_expanded >= grid_expanded[:, :, i:i+1]) & (x_expanded < grid_expanded[:, :, i+1:i+2])
                bases[:, :, i] = mask.squeeze(-1).float()
        
        # é€’å½’è®¡ç®—é«˜é˜¶B-spline
        for k in range(1, self.spline_order + 1):
            new_bases = torch.zeros_like(bases, device=device, dtype=x.dtype)
            for i in range(bases.size(-1) - k):
                if i + k < grid.size(0):
                    # å·¦ä¾§é¡¹
                    denom1 = grid[i + k] - grid[i] + 1e-8
                    alpha1 = (x - grid[i].to(device)) / denom1.to(device)
                    new_bases[:, :, i] += alpha1 * bases[:, :, i]
                    
                    # å³ä¾§é¡¹
                    if i + k + 1 < grid.size(0):
                        denom2 = grid[i + k + 1] - grid[i + 1] + 1e-8
                        alpha2 = (grid[i + k + 1].to(device) - x) / denom2.to(device)
                        new_bases[:, :, i] += alpha2 * bases[:, :, i + 1]
            bases = new_bases
        
        # è¿”å›æ­£ç¡®ç»´åº¦çš„åŸºå‡½æ•°ï¼Œç¡®ä¿ä¸spline_weightå…¼å®¹
        return bases[:, :, :self.grid_size + self.spline_order]
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        assert x.dim() == 2 and x.size(1) == self.in_features
        
        # ç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ä¸Š
        device = x.device
        
        # ç¡®ä¿æ‰€æœ‰æ¨¡å‹å‚æ•°éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if self.base_weight.device != device:
            self.base_weight = self.base_weight.to(device)
        if self.spline_weight.device != device:
            self.spline_weight = self.spline_weight.to(device)
        if hasattr(self, 'spline_scaler') and self.spline_scaler.device != device:
            self.spline_scaler = self.spline_scaler.to(device)
        
        # åŸºç¡€å˜æ¢
        base_output = F.linear(self.base_activation_fn(x), self.base_weight)
        
        # B-splineå˜æ¢
        grid = self.grid.to(device)
        spline_basis = self.b_splines(x, grid)
        spline_output = torch.einsum('bik,oik->bo', spline_basis, self.spline_weight)
        
        if self.enable_standalone_scale_spline:
            spline_output = spline_output * self.spline_scaler.sum(dim=1, keepdim=True).T
        
        return base_output + spline_output

class TKANCell(nn.Module):
    """TKAN å•å…ƒæ ¼å®ç°"""
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        kan_degree: int = 3,
        kan_grid_size: int = 5,
        dropout: float = 0.0,
        recurrent_dropout: float = 0.0,
        use_bias: bool = True
    ):
        """TKAN å•å…ƒæ ¼åˆå§‹åŒ–"""
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.dropout = dropout
        self.recurrent_dropout = recurrent_dropout
        
        # KANå±‚ç”¨äºè¾“å…¥å˜æ¢
        self.input_kan = KANLinear(
            input_size, 
            hidden_size * 4,  # 4ä¸ªé—¨ï¼šforget, input, candidate, output
            grid_size=kan_grid_size,
            spline_order=kan_degree
        )
        
        # KANå±‚ç”¨äºéšè—çŠ¶æ€å˜æ¢
        self.hidden_kan = KANLinear(
            hidden_size,
            hidden_size * 4,
            grid_size=kan_grid_size,
            spline_order=kan_degree
        )
        
        # åç½®é¡¹
        if use_bias:
            self.bias = nn.Parameter(torch.zeros(hidden_size * 4))
        else:
            self.register_parameter('bias', None)
        
        # Dropoutå±‚
        self.dropout_layer = nn.Dropout(dropout)
        self.recurrent_dropout_layer = nn.Dropout(recurrent_dropout)
    
    def forward(self, input: torch.Tensor, hidden: Optional[torch.Tensor] = None) -> torch.Tensor:
        """å•æ­¥å‰å‘ä¼ æ’­"""
        batch_size = input.size(0)
        device = input.device
        
        if hidden is None:
            hidden = torch.zeros(batch_size, self.hidden_size, device=device, dtype=input.dtype)
        else:
            # ç¡®ä¿hiddenåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if hidden.device != device:
                hidden = hidden.to(device)
        
        # åº”ç”¨dropout
        input = self.dropout_layer(input)
        hidden = self.recurrent_dropout_layer(hidden)
        
        # KANå˜æ¢
        gi = self.input_kan(input)
        gh = self.hidden_kan(hidden)
        
        # æ·»åŠ åç½®
        if self.bias is not None:
            if self.bias.device != device:
                self.bias = self.bias.to(device)
            gi = gi + self.bias
        
        # è®¡ç®—é—¨æ§å€¼
        gates = gi + gh
        forget_gate, input_gate, candidate_gate, output_gate = gates.chunk(4, dim=1)
        
        # åº”ç”¨æ¿€æ´»å‡½æ•°
        forget_gate = torch.sigmoid(forget_gate)
        input_gate = torch.sigmoid(input_gate)
        candidate_gate = torch.tanh(candidate_gate)
        output_gate = torch.sigmoid(output_gate)
        
        # æ›´æ–°ç»†èƒçŠ¶æ€å’Œéšè—çŠ¶æ€
        new_hidden = forget_gate * hidden + input_gate * candidate_gate
        output = output_gate * torch.tanh(new_hidden)
        
        return output

class TKANLayer(nn.Module):
    """TKAN å±‚å®ç°"""
    
    def __init__(
        self,
        input_size: int,
        hidden_size: int,
        return_sequences: bool = False,
        stateful: bool = False,
        **kwargs
    ):
        """TKAN å±‚åˆå§‹åŒ–"""
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.return_sequences = return_sequences
        self.stateful = stateful
        
        self.cell = TKANCell(input_size, hidden_size, **kwargs)
        
        # çŠ¶æ€å­˜å‚¨ï¼ˆç”¨äºstatefulæ¨¡å¼ï¼‰
        self.register_buffer('state', None)
    
    def forward(self, x: torch.Tensor, initial_state: Optional[torch.Tensor] = None) -> torch.Tensor:
        """åºåˆ—å‰å‘ä¼ æ’­"""
        batch_size, seq_len, input_size = x.size()
        device = x.device
        
        # åˆå§‹åŒ–çŠ¶æ€
        if initial_state is not None:
            hidden = initial_state.to(device) if initial_state.device != device else initial_state
        elif self.stateful and self.state is not None:
            hidden = self.state.to(device) if self.state.device != device else self.state
        else:
            hidden = torch.zeros(batch_size, self.hidden_size, device=device, dtype=x.dtype)
        
        outputs = []
        for t in range(seq_len):
            hidden = self.cell(x[:, t], hidden)
            if self.return_sequences:
                outputs.append(hidden)
        
        # ä¿å­˜çŠ¶æ€ï¼ˆç”¨äºstatefulæ¨¡å¼ï¼‰
        if self.stateful:
            self.state = hidden.detach()
        
        if self.return_sequences:
            return torch.stack(outputs, dim=1)
        else:
            return hidden
    
    def reset_states(self):
        """é‡ç½®çŠ¶æ€ï¼ˆç”¨äº stateful æ¨¡å¼ï¼‰"""
        self.state = None

# =============================================================================
# æ¨¡å‹åŸºç±»
# =============================================================================

class BaseModel(nn.Module, ABC):
    """æ¨¡å‹åŸºç±»"""
    
    def __init__(self, config: TKANConfig):
        super().__init__()
        self.config = config
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        try:
            total_params = sum(p.numel() for p in self.parameters())
            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        except Exception:
            # å¦‚æœæ¨¡å‹è¿˜æœªæ„å»ºï¼Œè¿”å›é»˜è®¤å€¼
            total_params = 0
            trainable_params = 0
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024) if total_params > 0 else 0.0  # å‡è®¾float32
        }

class TKANModel(BaseModel):
    """TKAN ä¸»æ¨¡å‹ç±»"""
    
    def __init__(self, config: TKANConfig):
        super().__init__(config)
        
        # è¾“å…¥ç»´åº¦ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.input_dim = getattr(config, 'input_dim', None)
        
        # æ¨¡å‹ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.tkan_layers = None
        self.output_layer = None
        self.dropout = nn.Dropout(config.dropout)
        self.batch_norm = None
        
        # å¦‚æœé…ç½®ä¸­æ²¡æœ‰input_dimï¼Œå¿…é¡»åœ¨è®­ç»ƒæ—¶åŠ¨æ€è®¾ç½®
        if self.input_dim is None:
            print("âš ï¸ é…ç½®ä¸­æœªæŒ‡å®šinput_dimï¼Œå°†åœ¨æ•°æ®åŠ è½½æ—¶è‡ªåŠ¨æ£€æµ‹")
    
    def _build_layers(self, input_dim: int):
        """ğŸ”¥ ä½¿ç”¨åŠ¨æ€è¾“å…¥ç»´åº¦æ„å»ºç½‘ç»œï¼Œç»å¯¹ä¸èƒ½ç¡¬ç¼–ç """
        if input_dim <= 0:
            raise ValueError(f"âŒ è¾“å…¥ç»´åº¦å¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {input_dim}")
        
        print(f"ğŸ”§ æ„å»ºTKANç½‘ç»œå±‚ï¼Œè¾“å…¥ç»´åº¦: {input_dim}")
        
        # æ›´æ–°é…ç½®
        self.config.input_dim = input_dim
        self.input_dim = input_dim
        
        # éªŒè¯éšè—å±‚ç»´åº¦é…ç½®
        if not self.config.hidden_dims or len(self.config.hidden_dims) == 0:
            raise ValueError("âŒ hidden_dimsé…ç½®ä¸èƒ½ä¸ºç©º")
        
        # æ„å»ºTKANå±‚
        self.tkan_layers = nn.ModuleList()
        layer_input_size = input_dim
        
        for i, hidden_dim in enumerate(self.config.hidden_dims):
            if hidden_dim <= 0:
                raise ValueError(f"âŒ éšè—å±‚ç»´åº¦å¿…é¡»å¤§äº0ï¼Œç¬¬{i}å±‚ç»´åº¦: {hidden_dim}")
            
            return_sequences = (i < len(self.config.hidden_dims) - 1) or self.config.return_sequences
            
            print(f"  ğŸ”§ æ„å»ºç¬¬{i+1}å±‚TKAN: {layer_input_size} -> {hidden_dim}, return_sequences={return_sequences}")
            
            tkan_layer = TKANLayer(
                input_size=layer_input_size,
                hidden_size=hidden_dim,
                return_sequences=return_sequences,
                stateful=self.config.stateful,
                kan_degree=self.config.spline_order,
                kan_grid_size=self.config.grid_size,
                dropout=self.config.dropout,
                recurrent_dropout=self.config.recurrent_dropout,
                use_bias=self.config.use_bias
            )
            self.tkan_layers.append(tkan_layer)
            layer_input_size = hidden_dim
        
        # è¾“å‡ºå±‚
        final_hidden_size = self.config.hidden_dims[-1] if self.config.hidden_dims else self.config.units
        print(f"  ğŸ”§ æ„å»ºè¾“å‡ºå±‚: {final_hidden_size} -> {self.config.output_dim}")
        self.output_layer = nn.Linear(final_hidden_size, self.config.output_dim)
        
        # æ‰¹é‡å½’ä¸€åŒ–
        self.batch_norm = nn.BatchNorm1d(final_hidden_size)
        
        print(f"âœ… TKANç½‘ç»œå±‚æ„å»ºå®Œæˆï¼Œå…±{len(self.tkan_layers)}å±‚TKAN + 1å±‚è¾“å‡º")
        
        # éªŒè¯æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in self.parameters())
        if total_params == 0:
            raise RuntimeError("âŒ æ¨¡å‹æ„å»ºåå‚æ•°æ•°é‡ä¸º0ï¼Œè¯·æ£€æŸ¥ç½‘ç»œç»“æ„")
        print(f"âœ… æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        device = x.device
        
        # é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºç½‘ç»œ
        if self.tkan_layers is None:
            if self.input_dim is None:
                if x.dim() == 2:
                    self.input_dim = x.shape[-1]  # ä»è¾“å…¥æ•°æ®è‡ªåŠ¨æ£€æµ‹
                elif x.dim() == 3:
                    self.input_dim = x.shape[-1]
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç»´åº¦: {x.shape}")
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç»´åº¦: {self.input_dim}")
            self._build_layers(self.input_dim)
            # ç¡®ä¿æ–°æ„å»ºçš„å±‚åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.to(device)
        
        # ç¡®ä¿æ‰€æœ‰æ¨¡å‹ç»„ä»¶åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if self.tkan_layers is not None:
            for layer in self.tkan_layers:
                if next(layer.parameters()).device != device:
                    layer.to(device)
        
        if self.output_layer is not None and next(self.output_layer.parameters()).device != device:
            self.output_layer.to(device)
        
        if self.batch_norm is not None and next(self.batch_norm.parameters()).device != device:
            self.batch_norm.to(device)
        
        # ç¡®ä¿è¾“å…¥æ˜¯3D (batch_size, seq_len, features)
        if x.dim() == 2:
            x = x.unsqueeze(1)
        
        # é€šè¿‡TKANå±‚
        for tkan_layer in self.tkan_layers:
            x = tkan_layer(x)
            if x.dim() == 3:
                # å¦‚æœè¿”å›åºåˆ—ï¼Œåªå¯¹æœ€åä¸€ä¸ªæ—¶é—´æ­¥åº”ç”¨dropout
                x = self.dropout(x)
            else:
                x = self.dropout(x)
        
        # å¦‚æœæ˜¯3Dè¾“å‡ºï¼Œå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        if x.dim() == 3:
            x = x[:, -1, :]
        
        # æ‰¹é‡å½’ä¸€åŒ–
        x = self.batch_norm(x)
        
        # è¾“å‡ºå±‚
        x = self.output_layer(x)
        
        # è¾“å‡ºæ¿€æ´» - åˆ†ç±»ä»»åŠ¡ä½¿ç”¨logitsè¾“å‡ºï¼Œæ¨ç†æ—¶å†åº”ç”¨sigmoid
        if self.config.model_type == "classification":
            # è®­ç»ƒæ—¶è¿”å›logitsï¼Œæ¨ç†æ—¶éœ€è¦æ‰‹åŠ¨åº”ç”¨sigmoid
            return x
        elif self.config.output_activation == 'softmax':
            x = F.softmax(x, dim=-1)
            return x
        else:
            return x

# =============================================================================
# æ•°æ®å¤„ç†ç±»
# =============================================================================

class TKANDataLoaderManager:
    """TKANæ•°æ®åŠ è½½å™¨ç®¡ç†ç±» - åªæ”¯æŒçœŸå®æ•°æ®"""
    
    def __init__(self, data_config_path: str, config: TKANConfig):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
        
        Args:
            data_config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«data_splitä¿¡æ¯
            config: TKANé…ç½®å¯¹è±¡
        """
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.data_split_info = self.data_config.get('data_split', {})
        self.scaler = StandardScaler()
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ï¼Œé»˜è®¤ä½¿ç”¨Phase 1é…ç½®"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸­æœ‰phase_1é…ç½®ï¼Œä½¿ç”¨å®ƒä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²ä¿¡æ¯
            if 'phase_1' in config:
                phase_1_config = config['phase_1']
                # æ„å»ºæ ‡å‡†çš„data_splitæ ¼å¼
                config['data_split'] = {
                    "workflow_type": "time_series",
                    "train": {
                        "start_date": phase_1_config['train_data']['start_date'],
                        "end_date": phase_1_config['train_data']['end_date'],
                        "samples": phase_1_config['train_data']['samples'],
                        "duration_years": phase_1_config['train_data']['duration_years'],
                        "file": phase_1_config['train_data']['file']
                    },
                    "validation": {
                        "start_date": phase_1_config['valid_data']['start_date'],
                        "end_date": phase_1_config['valid_data']['end_date'],
                        "samples": phase_1_config['valid_data']['samples'],
                        "duration_years": phase_1_config['valid_data']['duration_years'],
                        "file": phase_1_config['valid_data']['file']
                    },
                    "test": {
                        "start_date": phase_1_config['test_data']['start_date'],
                        "end_date": phase_1_config['test_data']['end_date'],
                        "samples": phase_1_config['test_data']['samples'],
                        "duration_years": phase_1_config['test_data']['duration_years'],
                        "file": phase_1_config['test_data']['file']
                    }
                }
                print(f"âœ… ä½¿ç”¨Phase 1é…ç½®ä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²")
                print(f"  è®­ç»ƒæœŸ: {config['data_split']['train']['start_date']} åˆ° {config['data_split']['train']['end_date']}")
                print(f"  éªŒè¯æœŸ: {config['data_split']['validation']['start_date']} åˆ° {config['data_split']['validation']['end_date']}")
                print(f"  æµ‹è¯•æœŸ: {config['data_split']['test']['start_date']} åˆ° {config['data_split']['test']['end_date']}")
            
            return config
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path}: {e}")
    
    def get_input_dim_from_dataframe(self, df):
        """ä»DataFrameè·å–ç‰¹å¾ç»´åº¦ï¼ˆæ’é™¤æ ‡ç­¾åˆ—ï¼‰"""
        # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
        feature_identifier = self.data_config.get('data_format', {}).get('feature_identifier', '@')
        feature_cols = [col for col in df.columns if feature_identifier in col]
        
        if not feature_cols:
            # å¦‚æœæ²¡æœ‰@ç¬¦å·çš„åˆ—ï¼Œå‡è®¾é™¤äº†'class'åˆ—å¤–éƒ½æ˜¯ç‰¹å¾
            feature_cols = [col for col in df.columns if col != 'class']
        
        features = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {features}")
        return features
    
    def validate_input_dimensions(self, config, actual_input_dim):
        """éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„è¾“å…¥ç»´åº¦ä¸å®é™…æ•°æ®æ˜¯å¦ä¸€è‡´"""
        config_input_dim = getattr(config, 'input_dim', None)
        
        if config_input_dim is None:
            print(f"ğŸ“‹ é…ç½®æ–‡ä»¶ä¸­æœªæŒ‡å®šinput_dimï¼Œä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        if config_input_dim != actual_input_dim:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…ï¼é…ç½®: {config_input_dim}, å®é™…: {actual_input_dim}")
            print(f"ğŸ”§ ä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        print(f"âœ… è¾“å…¥ç»´åº¦éªŒè¯é€šè¿‡: {actual_input_dim}")
        return actual_input_dim
    
    def _load_real_dataframes(self):
        """ğŸš« ä¸¥æ ¼ç¦æ­¢ï¼šä¸å¾—åŒ…å«ä»»ä½•æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆä»£ç """
        try:
            # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
            data_folder = self.data_config['data_paths']['data_folder']
            data_phase = self.data_config['data_paths']['data_phase']
            data_file_pattern = self.data_config['data_paths']['data_file']
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®æ–‡ä»¶
            import glob
            file_pattern = os.path.join(data_folder, data_file_pattern)
            data_files = glob.glob(file_pattern)
            
            if not data_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æ–‡ä»¶: {file_pattern}")
            
            # åŠ è½½ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
            data_file = data_files[0]
            print(f"ğŸ“Š åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
            
            # è¯»å–Parquetæ–‡ä»¶
            df = pd.read_parquet(data_file)
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            
            return self._split_dataframes(df)
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _split_dataframes(self, df):
        """æ—¶é—´åºåˆ—æ•°æ®åˆ†å‰²"""
        # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
        feature_identifier = self.data_config.get('data_format', {}).get('feature_identifier', '@')
        feature_cols = [col for col in df.columns if feature_identifier in col]
        
        if not feature_cols:
            raise ValueError(f"æœªæ‰¾åˆ°åŒ…å« '{feature_identifier}' çš„ç‰¹å¾åˆ—")
        
        print(f"ğŸ” è¯†åˆ«åˆ° {len(feature_cols)} ä¸ªç‰¹å¾åˆ—")
        
        # è·å–ç‰¹å¾å’Œæ ‡ç­¾
        X = df[feature_cols].values.astype(np.float32)
        y = df['class'].values.astype(np.float32)
        
        # å¤„ç†ç¼ºå¤±å€¼
        X = np.nan_to_num(X, nan=0.0)
        
        # æ—¶é—´åºåˆ—åˆ†å‰²
        if 'date' in df.columns:
            dates = pd.to_datetime(df['date'])
            # æŒ‰æ—¶é—´æ’åº
            sort_idx = dates.argsort()
            X = X[sort_idx]
            y = y[sort_idx]
            dates = dates.iloc[sort_idx]
            
            # å‰1å¹´ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä½™ä½œä¸ºæµ‹è¯•é›†
            train_end_date = dates.min() + pd.DateOffset(years=1)
            train_mask = dates <= train_end_date
            
            X_train = X[train_mask]
            y_train = y[train_mask]
            X_test = X[~train_mask]
            y_test = y[~train_mask]
            
            # ç¡®ä¿æµ‹è¯•é›†ä¸ä¸ºç©º
            if len(X_test) == 0:
                # å¦‚æœæŒ‰å¹´ä»½åˆ†å‰²å¯¼è‡´æµ‹è¯•é›†ä¸ºç©ºï¼Œä½¿ç”¨æ¯”ä¾‹åˆ†å‰²
                train_size = int(len(X) * 0.7)
                val_size = int(len(X) * 0.2)
                test_size = len(X) - train_size - val_size
                
                X_train = X[:train_size]
                y_train = y[:train_size]
                X_val = X[train_size:train_size+val_size]
                y_val = y[train_size:train_size+val_size]
                X_test = X[train_size+val_size:]
                y_test = y[train_size+val_size:]
            else:
                # ä»è®­ç»ƒé›†ä¸­åˆ†å‡ºéªŒè¯é›†ï¼ˆ20%ï¼‰
                val_size = int(len(X_train) * 0.2)
                X_val = X_train[-val_size:]
                y_val = y_train[-val_size:]
                X_train = X_train[:-val_size]
                y_train = y_train[:-val_size]
        else:
            # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œä½¿ç”¨ç®€å•çš„æ—¶é—´åºåˆ—åˆ†å‰²
            train_size = int(len(X) * 0.6)
            val_size = int(len(X) * 0.2)
            test_size = len(X) - train_size - val_size
            
            # ç¡®ä¿æ¯ä¸ªé›†åˆè‡³å°‘æœ‰1ä¸ªæ ·æœ¬
            if test_size < 1:
                train_size = int(len(X) * 0.7)
                val_size = int(len(X) * 0.2)
                test_size = len(X) - train_size - val_size
            
            if test_size < 1:
                # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œè°ƒæ•´æ¯”ä¾‹
                train_size = max(1, int(len(X) * 0.8))
                val_size = max(1, int(len(X) * 0.1))
                test_size = max(1, len(X) - train_size - val_size)
            
            X_train = X[:train_size]
            y_train = y[:train_size]
            X_val = X[train_size:train_size+val_size]
            y_val = y[train_size:train_size+val_size]
            X_test = X[train_size+val_size:train_size+val_size+test_size]
            y_test = y[train_size+val_size:train_size+val_size+test_size]
        
        print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(X_train)}")
        print(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(X_val)}")
        print(f"ğŸ“Š æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        # åˆ›å»ºDataFrame
        train_df = pd.DataFrame(X_train, columns=feature_cols)
        train_df['class'] = y_train
        
        val_df = pd.DataFrame(X_val, columns=feature_cols)
        val_df['class'] = y_val
        
        test_df = pd.DataFrame(X_test, columns=feature_cols)
        test_df['class'] = y_test
        
        return train_df, val_df, test_df
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=4,        # æ•°æ®åŠ è½½ä¼˜åŒ–
            pin_memory=True,      # GPUå†…å­˜ä¼˜åŒ–
            persistent_workers=True,  # æ•°æ®åŠ è½½ä¼˜åŒ–
            prefetch_factor=2     # æ•°æ®åŠ è½½ä¼˜åŒ–
        )
    
    def load_data_loaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        ğŸ”¥ å¿…é¡»ä»çœŸå®æ•°æ®æ–‡ä»¶åŠ è½½ï¼Œä¸¥æ ¼ç¦æ­¢æ¨¡æ‹Ÿæ•°æ®
        """
        # åŠ è½½çœŸå®æ•°æ®
        train_df, val_df, test_df = self._load_real_dataframes()
        
        # ğŸ”¥ å…³é”®ï¼šåŠ¨æ€æ£€æµ‹ç‰¹å¾ç»´åº¦
        feature_cols = [col for col in train_df.columns if col != 'class']
        input_dim = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ° {input_dim} ä¸ªç‰¹å¾: {feature_cols[:5]}...")
        
        # æ›´æ–°é…ç½®
        self.config.input_dim = input_dim
        
        # å‡†å¤‡æ•°æ®
        X_train = train_df[feature_cols].values.astype(np.float32)
        y_train = train_df['class'].values.astype(np.float32)
        X_val = val_df[feature_cols].values.astype(np.float32)
        y_val = val_df['class'].values.astype(np.float32)
        X_test = test_df[feature_cols].values.astype(np.float32)
        y_test = test_df['class'].values.astype(np.float32)
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        X_train = self.scaler.fit_transform(X_train)
        X_val = self.scaler.transform(X_val)
        X_test = self.scaler.transform(X_test)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train = torch.FloatTensor(X_train)
        y_train = torch.FloatTensor(y_train)
        X_val = torch.FloatTensor(X_val)
        y_val = torch.FloatTensor(y_val)
        X_test = torch.FloatTensor(X_test)
        y_test = torch.FloatTensor(y_test)
        
        # ä¸ºTKANæ·»åŠ æ—¶é—´ç»´åº¦ï¼ˆå‡è®¾æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
        X_train = X_train.unsqueeze(1)  # (batch, 1, features)
        X_val = X_val.unsqueeze(1)
        X_test = X_test.unsqueeze(1)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        test_dataset = TensorDataset(X_test, y_test)
        
        # åˆ›å»ºDataLoader
        train_loader = self._create_dataloader(train_dataset, shuffle=True, batch_size=batch_size)
        val_loader = self._create_dataloader(val_dataset, shuffle=False, batch_size=batch_size)
        test_loader = self._create_dataloader(test_dataset, shuffle=False, batch_size=batch_size)
        
        return train_loader, val_loader, test_loader
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_split_info
    
    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç›¸å…³ä¿¡æ¯"""
        return {
            "workflow_type": self.data_split_info.get("workflow_type", "unknown"),
            "train_period": self.data_split_info.get("train", {}),
            "validation_period": self.data_split_info.get("validation", {}),
            "test_period": self.data_split_info.get("test", {}),
            "data_config": self.data_config
        }

# =============================================================================
# è®­ç»ƒå™¨ç±»
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config: TKANConfig, model: TKANModel, data_loader_manager: TKANDataLoaderManager,
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir  # Checkpointä¿å­˜ç›®å½•
        
        # è®¾å¤‡è®¾ç½®
        self.device = setup_device(getattr(config, 'device', 'auto'))
        self.rank = 0
        self.world_size = 1
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logging(config.logs_dir, "training")
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config.use_mixed_precision and torch.cuda.is_available() else None
        self.use_amp = config.use_mixed_precision and torch.cuda.is_available()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_score = 0.0
        self.train_history = []
        
        # Checkpointè·Ÿè¸ªå™¨ï¼ˆæ”¯æŒå¤šåŒºé—´æœ€ä½³ï¼‰
        self.checkpoint_tracker = {
            'global_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'early_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'mid_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'late_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None}
        }
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(config.seed)
        
        # åˆ›å»ºç›®å½•
        create_directories(config)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡å¹¶è®°å½•GPUä¿¡æ¯
        self.model.to(self.device)
        
        # ç¡®ä¿æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        for module in self.model.modules():
            if hasattr(module, 'to'):
                module.to(self.device)
        
        self._log_device_info()
        
        # è®°å½•æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if self.use_amp:
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_info = self.data_loader_manager.get_data_info()
        self.logger.info(f"æ•°æ®åˆ†å‰²ä¿¡æ¯: {data_info['workflow_type']}")
        self.logger.info(f"è®­ç»ƒæœŸé—´: {data_info['train_period']}")
        self.logger.info(f"éªŒè¯æœŸé—´: {data_info['validation_period']}")
        self.logger.info(f"æµ‹è¯•æœŸé—´: {data_info['test_period']}")
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡å’ŒGPUä½¿ç”¨ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            
            self.logger.info(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name}")
            self.logger.info(f"ğŸ“Š GPUæ€»å†…å­˜: {gpu_memory:.1f}GB")
            self.logger.info(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {current_memory:.2f}GB")
            self.logger.info(f"ğŸ“Š å¯ç”¨å†…å­˜: {gpu_memory - current_memory:.2f}GB")
        else:
            self.logger.info("ğŸ–¥ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            
        # è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f}MB")
    
    def setup_training_components(self, train_loader: DataLoader = None):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # å¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰æ„å»ºå±‚ï¼Œå…ˆè¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æ¥æ„å»º
        if self.model.tkan_layers is None and train_loader is not None:
            print("ğŸ”§ æ¨¡å‹å±‚æœªæ„å»ºï¼Œè¿›è¡Œåˆå§‹åŒ–å‰å‘ä¼ æ’­...")
            self.model.train()
            with torch.no_grad():
                # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ¥åˆå§‹åŒ–æ¨¡å‹
                for batch_data, _ in train_loader:
                    batch_data = batch_data.to(self.device)
                    _ = self.model(batch_data)  # è§¦å‘æ¨¡å‹å±‚æ„å»º
                    break
            print("âœ… æ¨¡å‹å±‚æ„å»ºå®Œæˆ")
        
        # éªŒè¯æ¨¡å‹å‚æ•°æ˜¯å¦å­˜åœ¨
        model_params = list(self.model.parameters())
        if len(model_params) == 0:
            raise RuntimeError("âŒ æ¨¡å‹å‚æ•°ä¸ºç©ºï¼Œæ— æ³•åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€‚è¯·æ£€æŸ¥æ¨¡å‹æ„å»ºé€»è¾‘ã€‚")
        
        print(f"âœ… æ¨¡å‹å‚æ•°æ•°é‡: {len(model_params)}")
        total_params = sum(p.numel() for p in model_params)
        print(f"âœ… æ€»å‚æ•°æ•°é‡: {total_params:,}")
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=self.config.learning_rate,
            weight_decay=1e-4
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', patience=5, factor=0.5
        )
        
        # æŸå¤±å‡½æ•° - ä½¿ç”¨BCEWithLogitsLossé¿å…æ··åˆç²¾åº¦è®­ç»ƒé—®é¢˜
        if self.config.model_type == "classification":
            self.criterion = nn.BCEWithLogitsLoss()
        else:
            self.criterion = nn.MSELoss()
    
    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with autocast():
                    output = self.model(data)
                    loss = self.criterion(output.squeeze(), target)
                
                # æ··åˆç²¾åº¦åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
                
                # æ¢¯åº¦è£å‰ª
                if hasattr(self.config, 'gradient_clip_value'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                # ä¼˜åŒ–å™¨æ­¥è¿›
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                # æ ‡å‡†è®­ç»ƒ
                output = self.model(data)
                loss = self.criterion(output.squeeze(), target)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                if hasattr(self.config, 'gradient_clip_value'):
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.optimizer.step()
            
            total_loss += loss.item()
            
            if self.config.model_type == "classification":
                # å¯¹logitsåº”ç”¨sigmoidåå†è¿›è¡Œé¢„æµ‹
                probs = torch.sigmoid(output.squeeze())
                preds = (probs > 0.5).float()
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        metrics = {"loss": total_loss / len(train_loader)}
        
        if self.config.model_type == "classification":
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        
        return metrics
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                # æ··åˆç²¾åº¦æ¨ç†
                if self.use_amp:
                    with autocast():
                        output = self.model(data)
                        loss = self.criterion(output.squeeze(), target)
                else:
                    output = self.model(data)
                    loss = self.criterion(output.squeeze(), target)
                
                total_loss += loss.item()
                
                if self.config.model_type == "classification":
                    # å¯¹logitsåº”ç”¨sigmoidå¾—åˆ°æ¦‚ç‡
                    probs = torch.sigmoid(output.squeeze())
                    preds = (probs > 0.5).float()
                    all_probs.extend(probs.cpu().numpy())
                    all_preds.extend(preds.cpu().numpy())
                    all_targets.extend(target.cpu().numpy())
        
        metrics = {"loss": total_loss / len(val_loader)}
        
        if self.config.model_type == "classification":
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
            metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
            
            # è®¡ç®—AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡è¾“å‡ºï¼‰
            if len(all_probs) > 0 and len(np.unique(all_targets)) == 2:
                try:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs)
                    # æ·»åŠ éªŒè¯é¢„æµ‹æ•°æ®ç”¨äºOptunaä¼˜åŒ–
                    metrics["y_true_val"] = all_targets
                    metrics["y_prob_val"] = all_probs
                except Exception:
                    pass  # å¦‚æœæ— æ³•è®¡ç®—AUCï¼Œè·³è¿‡
        
        return metrics
    
    def train(self, batch_size: int = None, no_save_model: bool = False) -> Dict[str, Any]:
        """ğŸ”„ å®Œæ•´è®­ç»ƒ: è®­ç»ƒå°†è¿è¡Œå®Œæ•´çš„epochsæ•°é‡ï¼ŒğŸš« ä¸¥æ ¼ç¦æ­¢å¿«é€Ÿæµ‹è¯•æ¨¡å¼"""
        # è·å–æ•°æ®åŠ è½½å™¨
        batch_size = batch_size or self.config.batch_size
        train_loader, val_loader, test_loader = self.data_loader_manager.load_data_loaders(
            batch_size=batch_size)
        
        # ä¼ å…¥è®­ç»ƒæ•°æ®åŠ è½½å™¨æ¥è®¾ç½®è®­ç»ƒç»„ä»¶
        self.setup_training_components(train_loader)
        
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {self.model.get_model_info()}")
        
        start_time = time.time()
        epochs = self.config.epochs  # ğŸš« ä¸¥æ ¼ç¦æ­¢å¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨å®Œæ•´epochs
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader)

            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # è®°å½•å†å²
            epoch_info = {
                "epoch": epoch + 1,
                "train_loss": train_metrics["loss"],
                "val_loss": val_metrics["loss"],
                "lr": self.optimizer.param_groups[0]['lr'],
                "time": time.time() - epoch_start
            }
            
            if self.config.model_type == "classification":
                epoch_info.update({
                    "train_acc": train_metrics.get("accuracy", 0),
                    "val_acc": val_metrics.get("accuracy", 0),
                    "val_f1": val_metrics.get("f1", 0),
                    "train_auc": train_metrics.get("auc", 0),
                    "val_auc": val_metrics.get("auc", 0)
                })
                current_score = val_metrics.get("accuracy", 0)
            else:
                current_score = -val_metrics["loss"]  # å›å½’ä»»åŠ¡ä½¿ç”¨è´ŸæŸå¤±
            
            self.train_history.append(epoch_info)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(current_score)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if current_score > self.best_val_score:
                self.best_val_score = current_score
                if not no_save_model:
                    self.save_checkpoint(epoch + 1, is_best=True)
            
            # GPUå†…å­˜ç›‘æ§ï¼ˆæ¯10ä¸ªepochè®°å½•ä¸€æ¬¡ï¼‰
            if self.device.type == 'cuda' and (epoch + 1) % 10 == 0:
                current_memory = torch.cuda.memory_allocated() / 1024**3
                max_memory = torch.cuda.max_memory_allocated() / 1024**3
                cached_memory = torch.cuda.memory_reserved() / 1024**3
                self.logger.info(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨ (Epoch {epoch+1}): å½“å‰ {current_memory:.2f}GB, å³°å€¼ {max_memory:.2f}GB, ç¼“å­˜ {cached_memory:.2f}GB")
            
            # æ—¥å¿—è¾“å‡º
            log_msg = (
                f"Epoch {epoch+1}/{epochs} - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}"
            )

            if self.config.model_type == "classification":
                log_msg += f", Val Acc: {val_metrics.get('accuracy', 0):.4f}"
                if 'auc' in val_metrics:
                    log_msg += f", Val AUC: {val_metrics['auc']:.4f}"

            log_msg += f", Time: {epoch_info['time']:.2f}s"
            self.logger.info(log_msg)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        return {
            "best_val_score": self.best_val_score,
            "total_epochs": len(self.train_history),
            "total_time": total_time,
            "train_history": self.train_history
        }
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_score': self.best_val_score,
            'config': asdict(self.config)
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        # ä½¿ç”¨ä¼ å…¥çš„checkpointç›®å½•ï¼ˆä¼˜å…ˆï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„
        checkpoint_dir = Path(self.checkpoint_dir) if self.checkpoint_dir else Path(self.config.checkpoint_dir)
        
        checkpoint_path = checkpoint_dir / "latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = Path(self.config.results_dir) / "training_history.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        self.logger.info(f"è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

# =============================================================================
# æ¨ç†å™¨ç±»
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, config: TKANConfig, model: TKANModel, data_loader_manager: TKANDataLoaderManager,
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir  # Checkpointä¿å­˜ç›®å½•
        self.device = setup_device()
        self.logger = setup_logging(config.logs_dir, "inference")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)
        self.model.eval()
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        return checkpoint
    
    def predict_batch(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """æ‰¹é‡æ¨ç†"""
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in data_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                if self.config.model_type == "classification":
                    # å¯¹logitsåº”ç”¨sigmoidå¾—åˆ°æ¦‚ç‡
                    probs = torch.sigmoid(outputs.squeeze())
                    preds = (probs > 0.5).float()
                    all_probs.extend(probs.cpu().numpy())
                    all_preds.extend(preds.cpu().numpy())
                else:
                    all_preds.extend(outputs.cpu().numpy())
                    all_probs.extend(outputs.cpu().numpy())  # å›å½’ä»»åŠ¡æ¦‚ç‡å³ä¸ºé¢„æµ‹å€¼
        
        return np.array(all_preds), np.array(all_probs)
    
    def predict_single(self, data: np.ndarray) -> Tuple[Union[int, float], np.ndarray]:
        """å•æ ·æœ¬æ¨ç†"""
        data_tensor = torch.FloatTensor(data).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            output = self.model(data_tensor)
            
            if self.config.model_type == "classification":
                # å¯¹logitsåº”ç”¨sigmoidå¾—åˆ°æ¦‚ç‡
                prob = torch.sigmoid(output.squeeze())
                pred = (prob > 0.5).float().item()
                return pred, np.array([prob.cpu().numpy()])
            else:
                pred = output.item()
                return pred, np.array([pred])
    
    def evaluate(self) -> Dict[str, float]:
        """ğŸ“Š å¿…é¡»åŒ…å«AUC: åœ¨æ¨¡å‹è¯„ä¼°ä¸­å¿…é¡»åŒ…å«AUCæŒ‡æ ‡è®¡ç®—ï¼Œç‰¹åˆ«æ˜¯äºŒåˆ†ç±»ä»»åŠ¡"""
        # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                if self.config.model_type == "classification":
                    # å¯¹logitsåº”ç”¨sigmoidå¾—åˆ°æ¦‚ç‡
                    probs = torch.sigmoid(outputs.squeeze())
                    preds = (probs > 0.5).float()
                    all_probs.extend(probs.cpu().numpy())
                    all_preds.extend(preds.cpu().numpy())
                else:
                    all_preds.extend(outputs.cpu().numpy())
                
                all_targets.extend(target.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {}
        
        if self.config.model_type == "classification":
            metrics["accuracy"] = accuracy_score(all_targets, all_preds)
            metrics["precision"] = precision_score(all_targets, all_preds, average='weighted')
            metrics["recall"] = recall_score(all_targets, all_preds, average='weighted')
            metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
            
            # ğŸ“Š å¿…é¡»åŒ…å«AUCæŒ‡æ ‡è®¡ç®—
            if len(np.unique(all_targets)) == 2:  # äºŒåˆ†ç±»
                metrics["auc"] = roc_auc_score(all_targets, all_probs)
            elif len(np.unique(all_targets)) > 2:  # å¤šåˆ†ç±»
                try:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs, multi_class='ovr', average='weighted')
                except ValueError:
                    # å¦‚æœæ— æ³•è®¡ç®—å¤šåˆ†ç±»AUCï¼Œè·³è¿‡
                    pass
        else:
            from sklearn.metrics import mean_squared_error, mean_absolute_error
            metrics["mse"] = mean_squared_error(all_targets, all_preds)
            metrics["mae"] = mean_absolute_error(all_targets, all_preds)
            metrics["rmse"] = np.sqrt(metrics["mse"])
        
        return metrics
    
    def save_checkpoint_if_best(self, epoch: int, val_loss: float):
        """æ™ºèƒ½checkpointä¿å­˜ï¼šå…¨å±€æœ€ä½³ + åŒºé—´æœ€ä½³"""
        # æ›´æ–°å…¨å±€æœ€ä½³
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        # æ›´æ–°åŒºé—´æœ€ä½³ï¼ˆå®æ—¶æ›´æ–°ï¼‰
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        # ğŸ’¡ å…³é”®ï¼šåœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³ä¸ºinterval_best
        if epoch == 29:  # epochä»0å¼€å§‹ï¼Œ29æ˜¯ç¬¬30ä¸ªepoch
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')
    
    def _save_checkpoint(self, epoch: int, val_loss: float, checkpoint_type: str):
        """ä¿å­˜å•ä¸ªcheckpoint"""
        import os
        
        # åˆ é™¤æ—§checkpoint
        old_path = self.checkpoint_tracker[checkpoint_type]['path']
        if old_path and os.path.exists(old_path):
            os.remove(old_path)
        
        # ä¿å­˜æ–°checkpoint
        checkpoint_name = f"checkpoint_{checkpoint_type}_epoch{epoch:03d}_val{val_loss:.4f}.pt"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'checkpoint_type': checkpoint_type,
            'config': asdict(self.config)
        }
        
        torch.save(checkpoint, checkpoint_path)
        
        self.checkpoint_tracker[checkpoint_type] = {
            'epoch': epoch,
            'val_loss': val_loss,
            'path': checkpoint_path
        }
        
        self.logger.info(f"ğŸ’¾ ä¿å­˜{checkpoint_type} checkpoint: epoch {epoch}, val_loss {val_loss:.4f}")
    
    def _save_interval_best(self, checkpoint_type: str, epoch_range: str):
        """åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³checkpointä¸ºinterval_best"""
        import shutil
        import os
        
        # è·å–å½“å‰åŒºé—´æœ€ä½³çš„checkpointè·¯å¾„
        source_path = self.checkpoint_tracker[checkpoint_type]['path']
        
        if source_path and os.path.exists(source_path):
            # åˆ›å»ºinterval_bestæ–‡ä»¶å
            interval_name = f"interval_best_{epoch_range}.pt"
            interval_path = os.path.join(self.checkpoint_dir, interval_name)
            
            # å¤åˆ¶checkpoint
            shutil.copy2(source_path, interval_path)
            
            epoch = self.checkpoint_tracker[checkpoint_type]['epoch']
            val_loss = self.checkpoint_tracker[checkpoint_type]['val_loss']
            
            self.logger.info(f"ğŸ“¦ å¤åˆ¶åŒºé—´æœ€ä½³ [{epoch_range}]: epoch {epoch}, val_loss {val_loss:.4f}")
            self.logger.info(f"   æºæ–‡ä»¶: {os.path.basename(source_path)}")
            self.logger.info(f"   ç›®æ ‡æ–‡ä»¶: {interval_name}")
        else:
            self.logger.warning(f"âš ï¸ åŒºé—´ [{epoch_range}] æ²¡æœ‰æ‰¾åˆ°æœ€ä½³checkpoint")
    
    def save_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, 
                        output_path: str):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        results = {
            "predictions": predictions.tolist(),
            "probabilities": probabilities.tolist(),
            "config": asdict(self.config),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: TKANConfig, model: TKANModel):
        self.config = config
        self.model = model
    
    def generate_model_md(self, output_path: str = "MODEL.md"):
        """ç”ŸæˆMODEL.mdæ–‡æ¡£"""
        model_info = self.model.get_model_info()
        
        content = f"""# {self.config.model_name} Model

## æ¨¡å‹æ¦‚è¿°

{self.config.model_name} æ˜¯ä¸€ä¸ªåŸºäºTime-series Kolmogorov-Arnold Networksçš„{self.config.model_type}æ¨¡å‹ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- **ä»»åŠ¡ç±»å‹**: {self.config.model_type}
- **æ¨¡å‹å‚æ•°**: {model_info['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°**: {model_info['trainable_parameters']:,}
- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.2f} MB

## æ¨¡å‹æ¶æ„

### æ ¸å¿ƒç»„ä»¶

TKANæ¨¡å‹ç»“åˆäº†Kolmogorov-Arnold Networkså’Œæ—¶é—´åºåˆ—å¤„ç†èƒ½åŠ›ï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- **KANçº¿æ€§å±‚**: ä½¿ç”¨B-splineåŸºå‡½æ•°è¿›è¡Œéçº¿æ€§å˜æ¢
- **TKANå•å…ƒæ ¼**: åŸºäºKANçš„å¾ªç¯ç¥ç»ç½‘ç»œå•å…ƒ
- **å¤šå±‚æ¶æ„**: æ”¯æŒå¤šå±‚TKANå±‚å †å 
- **åŠ¨æ€ç»´åº¦é€‚é…**: è‡ªåŠ¨é€‚åº”ä¸åŒçš„è¾“å…¥ç‰¹å¾ç»´åº¦

### ç½‘ç»œç»“æ„

```
è¾“å…¥å±‚ -> TKANå±‚1 -> TKANå±‚2 -> ... -> æ‰¹é‡å½’ä¸€åŒ– -> è¾“å‡ºå±‚ -> æ¿€æ´»å‡½æ•°
```

## æŠ€æœ¯åŸç†

TKANæ¨¡å‹åŸºäºKolmogorov-Arnoldè¡¨ç¤ºå®šç†ï¼Œä½¿ç”¨å¯å­¦ä¹ çš„å•å˜é‡å‡½æ•°æ›¿ä»£ä¼ ç»Ÿçš„çº¿æ€§å˜æ¢ï¼Œ
èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰æ•°æ®ä¸­çš„éçº¿æ€§å…³ç³»å’Œæ—¶é—´ä¾èµ–æ€§ã€‚

## é…ç½®å‚æ•°

### è®­ç»ƒé…ç½®
- å­¦ä¹ ç‡: {self.config.learning_rate}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- è®­ç»ƒè½®æ•°: {self.config.epochs}
- æ··åˆç²¾åº¦è®­ç»ƒ: {self.config.use_mixed_precision}

### æ¨¡å‹é…ç½®
- éšè—ç»´åº¦: {self.config.hidden_dims}
- Dropout: {self.config.dropout}
- ç½‘æ ¼å¤§å°: {self.config.grid_size}
- æ ·æ¡é˜¶æ•°: {self.config.spline_order}

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```python
python TKAN_unified.py train --config config.yaml --data-config data.yaml
```

### æ¨¡å‹æ¨ç†

```python
python TKAN_unified.py inference --config config.yaml --data-config data.yaml --checkpoint best_model.pth
```

### æ¨¡å‹è¯„ä¼°

```python
python TKAN_unified.py inference --config config.yaml --data-config data.yaml --checkpoint best_model.pth
```

## æ€§èƒ½æŒ‡æ ‡

æ¨¡å‹æ”¯æŒä»¥ä¸‹è¯„ä¼°æŒ‡æ ‡ï¼š
- å‡†ç¡®ç‡ (Accuracy)
- ç²¾ç¡®ç‡ (Precision)
- å¬å›ç‡ (Recall)
- F1åˆ†æ•° (F1-Score)
- AUC-ROC (äºŒåˆ†ç±»ä»»åŠ¡)

## æ³¨æ„äº‹é¡¹

1. æ¨¡å‹ä¼šè‡ªåŠ¨æ£€æµ‹è¾“å…¥ç‰¹å¾ç»´åº¦ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®
2. æ”¯æŒGPUåŠ é€Ÿå’Œæ··åˆç²¾åº¦è®­ç»ƒ
3. å»ºè®®ä½¿ç”¨æ—¶é—´åºåˆ—æ•°æ®è¿›è¡Œè®­ç»ƒ
4. æ¨¡å‹è¾“å‡ºç»è¿‡sigmoidæ¿€æ´»ï¼Œé€‚ç”¨äºäºŒåˆ†ç±»ä»»åŠ¡

## æ›´æ–°æ—¥å¿—

- åˆå§‹ç‰ˆæœ¬: {datetime.now().strftime('%Y-%m-%d')}
- æ”¯æŒåŠ¨æ€ç»´åº¦é€‚é…
- é›†æˆOptunaè¶…å‚æ•°ä¼˜åŒ–
- æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ

"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¨¡å‹æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")

# =============================================================================
# ä¸»è¦æ¥å£å‡½æ•°
# =============================================================================

def create_model_factory(config: TKANConfig) -> TKANModel:
    """æ¨¡å‹å·¥å‚å‡½æ•°"""
    return TKANModel(config)

def create_data_loader_manager(data_config_path: str, config: TKANConfig) -> TKANDataLoaderManager:
    """æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨å·¥å‚å‡½æ•°"""
    return TKANDataLoaderManager(data_config_path, config)

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               optuna_config_path: str = None, device: str = "auto",
               epochs_override: int = None, no_save_model: bool = False,
               seed: int = 42, checkpoint_dir: str = "checkpoints"):
    # è®¾ç½®éšæœºç§å­
    import random
    import numpy as np
    import torch
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    logger.info(f"ğŸ’¾ Checkpointä¿å­˜ç›®å½•: {checkpoint_dir}")
    
    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # 1. åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = TKANConfig(**config_dict.get('model', {}))
    
    # åº”ç”¨Optunaé…ç½®è¦†ç›–
    if optuna_config_path and os.path.exists(optuna_config_path):
        with open(optuna_config_path, 'r', encoding='utf-8') as f:
            optuna_config = yaml.safe_load(f)
        
        # åº”ç”¨è¶…å‚æ•°è¦†ç›–
        for key, value in optuna_config.items():
            if hasattr(config, key):
                setattr(config, key, value)
                print(f"ğŸ”§ Optunaè¦†ç›–å‚æ•°: {key} = {value}")
    
    # è¦†ç›–é…ç½®
    if device != "auto":
        config.device = device
    if epochs_override:
        config.epochs = epochs_override
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # 3. åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨
    trainer = UnifiedTrainer(config, model, data_loader_manager, checkpoint_dir=checkpoint_dir)
    
    # 5. æ‰§è¡Œè®­ç»ƒ
    results = trainer.train(no_save_model=no_save_model)
    
    # 6. è¾“å‡ºJSONæ ¼å¼çš„è®­ç»ƒç»“æœä¾›Optunaè§£æ
    final_results = {
        "final_results": {
            "best_val_score": results['best_val_score'],
            "total_epochs": results['total_epochs'],
            "total_time": results['total_time'],
            "final_train_acc": results['train_history'][-1].get('train_acc', 0),
            "final_val_acc": results['train_history'][-1].get('val_acc', 0),
            "train_auc": results['train_history'][-1].get('train_auc', 0),
            "val_auc": results['train_history'][-1].get('val_auc', 0),
            "train_losses": [epoch['train_loss'] for epoch in results['train_history']],
            "val_losses": [epoch['val_loss'] for epoch in results['train_history']],
            "train_accuracies": [epoch.get('train_acc', 0) for epoch in results['train_history']],
            "val_accuracies": [epoch.get('val_acc', 0) for epoch in results['train_history']]
        }
    }
    print(json.dumps(final_results, indent=2))

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "best_model.pth"):
    """ä¸»æ¨ç†å‡½æ•°"""
    # 1. åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    config = TKANConfig(**config_dict.get('model', {}))
    
    # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # 3. åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # 4. åˆ›å»ºæ¨ç†å™¨
    inferencer = UnifiedInferencer(config, model, data_loader_manager)
    
    # 5. åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = inferencer.load_checkpoint(checkpoint_path)
    
    # 6. æ‰§è¡Œæ¨ç†å’Œè¯„ä¼°
    metrics = inferencer.evaluate()
    
    # 7. è¾“å‡ºç»“æœ
    print("æ¨ç†è¯„ä¼°ç»“æœ:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value:.4f}")

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ä¸»æ–‡æ¡£ç”Ÿæˆå‡½æ•°"""
    # 1. åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    config = TKANConfig(**config_dict.get('model', {}))
    
    # 2. åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # 3. ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config, model)
    doc_generator.generate_model_md("MODEL.md")
    
    print("æ–‡æ¡£ç”Ÿæˆå®Œæˆ")

# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€TKANæ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·")
    parser.add_argument("mode", choices=["train", "inference", "docs"], help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="config.yaml", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-config", default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default="best_model.pth", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--data", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # Optunaä¼˜åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--optuna-config", help="Optunaè¯•éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--no-save-model", action="store_true", help="ä¸ä¿å­˜æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°æ€§ï¼‰")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    
    return parser.parse_args()

if __name__ == "__main__":
    # è®¾ç½®å›ºå®šçš„æ—¥å¿—è·¯å¾„
    log_dir = "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/training/logs/TKAN_9235352221"
    logger = setup_logging(log_dir=log_dir, log_filename="log.txt")
    
    args = parse_args()
    
    if args.mode == "train":
        main_train(
            config_path=args.config,
            data_config_path=args.data_config,
            optuna_config_path=args.optuna_config,
            device=args.device,
            epochs_override=args.epochs,
            no_save_model=args.no_save_model,
            seed=args.seed,
            checkpoint_dir=args.checkpoint_dir
        )
    elif args.mode == "inference":
        main_inference(args.config, args.data_config, args.checkpoint)
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data_config)