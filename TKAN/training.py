#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
training.py - TKANæ¨¡å‹è®­ç»ƒè„šæœ¬
å®ç°å®Œæ•´çš„TKANæ¨¡å‹è®­ç»ƒæµç¨‹ï¼Œæ”¯æŒäºŒå…ƒåˆ†ç±»ä»»åŠ¡
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
import numpy as np
import yaml
import argparse
import logging
import os
import json
import random
from typing import Dict, Any, Optional, Tuple, Union, List
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹
from model import TKAN, set_all_seeds, setup_device


class TKANTrainer:
    """TKANæ¨¡å‹è®­ç»ƒå™¨ç±»"""
    
    def __init__(self, config_path: str, data_config_path: str, quick_test: bool = False):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            data_config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
        """
        self.config_path = config_path
        self.data_config_path = data_config_path
        self.quick_test = quick_test
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        self.data_config = self._load_config(data_config_path)
        
        # è®¾ç½®è®¾å¤‡å’Œéšæœºç§å­
        self.device = setup_device()
        set_all_seeds(42)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = StandardScaler()
        self.train_loader = None
        self.val_loader = None
        self.test_loader = None
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'val_accuracy': [],
            'val_f1': []
        }
        
        # è®¾ç½®æ—¥å¿—
        self.logger = self._setup_logging()
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            raise FileNotFoundError(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}")
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•"""
        logger = logging.getLogger('TKANTrainer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def load_data(self) -> None:
        """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
        try:
            self.logger.info("å¼€å§‹åŠ è½½æ•°æ®...")
            
            # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
            data_folder = self.data_config['data_paths']['data_folder']
            data_phase = self.data_config['data_paths']['data_phase']
            data_file_pattern = self.data_config['data_paths']['data_file']
            
            # æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®æ–‡ä»¶
            import glob
            file_pattern = os.path.join(data_folder, data_file_pattern)
            data_files = glob.glob(file_pattern)
            
            if not data_files:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é…çš„æ•°æ®æ–‡ä»¶: {file_pattern}")
            
            # åŠ è½½ç¬¬ä¸€ä¸ªåŒ¹é…çš„æ–‡ä»¶
            data_file = data_files[0]
            self.logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {data_file}")
            
            # è¯»å–Parquetæ–‡ä»¶
            df = pd.read_parquet(data_file)
            self.logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªä½¿ç”¨å°‘é‡æ•°æ®
            if self.quick_test:
                df = df.head(100)
                self.logger.info(f"å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨ {len(df)} ä¸ªæ ·æœ¬")
            
            self.raw_data = df
            
        except Exception as e:
            self.logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            raise
    
    def prepare_data(self) -> None:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        try:
            self.logger.info("å¼€å§‹æ•°æ®é¢„å¤„ç†...")
            
            df = self.raw_data.copy()
            
            # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
            feature_identifier = self.data_config['data_format']['feature_identifier']
            feature_cols = [col for col in df.columns if feature_identifier in col]
            
            if not feature_cols:
                raise ValueError(f"æœªæ‰¾åˆ°åŒ…å« '{feature_identifier}' çš„ç‰¹å¾åˆ—")
            
            self.logger.info(f"è¯†åˆ«åˆ° {len(feature_cols)} ä¸ªç‰¹å¾åˆ—")
            
            # è·å–ç‰¹å¾å’Œæ ‡ç­¾
            X = df[feature_cols].values.astype(np.float32)
            y = df['class'].values.astype(np.float32)
            
            # å¤„ç†ç¼ºå¤±å€¼
            X = np.nan_to_num(X, nan=0.0)
            
            # æ—¶é—´åºåˆ—åˆ†å‰²
            if 'date' in df.columns and not self.quick_test:
                dates = pd.to_datetime(df['date'])
                # æŒ‰æ—¶é—´æ’åº
                sort_idx = dates.argsort()
                X = X[sort_idx]
                y = y[sort_idx]
                dates = dates.iloc[sort_idx]
                
                # å‰1å¹´ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä½™ä½œä¸ºæµ‹è¯•é›†
                train_end_date = dates.min() + pd.DateOffset(years=1)
                train_mask = dates <= train_end_date
                
                X_train = X[train_mask]
                y_train = y[train_mask]
                X_test = X[~train_mask]
                y_test = y[~train_mask]
                
                # ç¡®ä¿æµ‹è¯•é›†ä¸ä¸ºç©º
                if len(X_test) == 0:
                    # å¦‚æœæŒ‰å¹´ä»½åˆ†å‰²å¯¼è‡´æµ‹è¯•é›†ä¸ºç©ºï¼Œä½¿ç”¨æ¯”ä¾‹åˆ†å‰²
                    train_size = int(len(X) * 0.7)
                    val_size = int(len(X) * 0.2)
                    test_size = len(X) - train_size - val_size
                    
                    X_train = X[:train_size]
                    y_train = y[:train_size]
                    X_val = X[train_size:train_size+val_size]
                    y_val = y[train_size:train_size+val_size]
                    X_test = X[train_size+val_size:]
                    y_test = y[train_size+val_size:]
                else:
                    # ä»è®­ç»ƒé›†ä¸­åˆ†å‡ºéªŒè¯é›†ï¼ˆ20%ï¼‰
                    val_size = int(len(X_train) * 0.2)
                    X_val = X_train[-val_size:]
                    y_val = y_train[-val_size:]
                    X_train = X_train[:-val_size]
                    y_train = y_train[:-val_size]
                
            else:
                # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œä½¿ç”¨ç®€å•çš„æ—¶é—´åºåˆ—åˆ†å‰²
                train_size = int(len(X) * 0.6)
                val_size = int(len(X) * 0.2)
                test_size = len(X) - train_size - val_size
                
                # ç¡®ä¿æ¯ä¸ªé›†åˆè‡³å°‘æœ‰1ä¸ªæ ·æœ¬
                if test_size < 1:
                    train_size = int(len(X) * 0.7)
                    val_size = int(len(X) * 0.2)
                    test_size = len(X) - train_size - val_size
                
                if test_size < 1:
                    # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œè°ƒæ•´æ¯”ä¾‹
                    train_size = max(1, int(len(X) * 0.8))
                    val_size = max(1, int(len(X) * 0.1))
                    test_size = max(1, len(X) - train_size - val_size)
                
                X_train = X[:train_size]
                y_train = y[:train_size]
                X_val = X[train_size:train_size+val_size]
                y_val = y[train_size:train_size+val_size]
                X_test = X[train_size+val_size:train_size+val_size+test_size]
                y_test = y[train_size+val_size:train_size+val_size+test_size]
            
            self.logger.info(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
            self.logger.info(f"éªŒè¯é›†å¤§å°: {len(X_val)}")
            self.logger.info(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
            
            # ç‰¹å¾æ ‡å‡†åŒ–
            X_train = self.scaler.fit_transform(X_train)
            X_val = self.scaler.transform(X_val)
            X_test = self.scaler.transform(X_test)
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡
            X_train = torch.FloatTensor(X_train)
            y_train = torch.FloatTensor(y_train)
            X_val = torch.FloatTensor(X_val)
            y_val = torch.FloatTensor(y_val)
            X_test = torch.FloatTensor(X_test)
            y_test = torch.FloatTensor(y_test)
            
            # ä¸ºTKANæ·»åŠ æ—¶é—´ç»´åº¦ï¼ˆå‡è®¾æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
            X_train = X_train.unsqueeze(1)  # (batch, 1, features)
            X_val = X_val.unsqueeze(1)
            X_test = X_test.unsqueeze(1)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            batch_size = self.config['training']['batch_size']
            if self.quick_test:
                batch_size = min(batch_size, 16)
            
            train_dataset = TensorDataset(X_train, y_train)
            val_dataset = TensorDataset(X_val, y_val)
            test_dataset = TensorDataset(X_test, y_test)
            
            self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
            self.test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
            
            # ä¿å­˜æ•°æ®ä¿¡æ¯
            self.num_features = X_train.shape[-1]
            self.num_samples = len(X_train)
            
            self.logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def setup_model(self) -> None:
        """è®¾ç½®æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨"""
        try:
            self.logger.info("è®¾ç½®æ¨¡å‹...")
            
            # åˆ›å»ºæ¨¡å‹
            self.model = TKAN(self.config)
            self.model.to(self.device)
            
            # é€šè¿‡ä¸€ä¸ªæ ·æœ¬æ¥åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœæœ‰æ•°æ®åŠ è½½å™¨ï¼‰
            if hasattr(self, 'train_loader') and self.train_loader is not None:
                sample_batch = next(iter(self.train_loader))
                sample_input, _ = sample_batch
                sample_input = sample_input.to(self.device)
                _ = self.model(sample_input)  # è§¦å‘å»¶è¿Ÿåˆå§‹åŒ–
            
            # è®¾ç½®ä¼˜åŒ–å™¨
            optimizer_name = self.config['training']['optimizer'].lower()
            lr = float(self.config['training']['learning_rate'])
            weight_decay = float(self.config['training']['weight_decay'])
            
            if optimizer_name == 'adam':
                self.optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)
            elif optimizer_name == 'sgd':
                self.optimizer = optim.SGD(self.model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)
            elif optimizer_name == 'adamw':
                self.optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")
            
            # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler_config = self.config['training'].get('scheduler', {})
            if isinstance(scheduler_config, str):
                scheduler_type = scheduler_config
                scheduler_config = {}
            else:
                scheduler_type = scheduler_config.get('type', 'step')
            
            if scheduler_type == 'step':
                step_size = scheduler_config.get('step_size', 30)
                gamma = scheduler_config.get('gamma', 0.1)
                self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=step_size, gamma=gamma)
            elif scheduler_type == 'cosine':
                T_max = scheduler_config.get('T_max', 50)
                self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=T_max)
            elif scheduler_type == 'reduce':
                patience = scheduler_config.get('patience', 10)
                factor = scheduler_config.get('factor', 0.5)
                self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=patience, factor=factor)
            else:
                self.scheduler = None
            
            self.logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters())}")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def train_epoch(self) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            
            # è®¡ç®—æŸå¤±
            loss = nn.BCELoss()(output.squeeze(), target)
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            max_grad_norm = self.config['training'].get('max_grad_norm', 1.0)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_grad_norm)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªè®­ç»ƒå‡ ä¸ªæ‰¹æ¬¡
            if self.quick_test and batch_idx >= 2:
                break
        
        return total_loss / num_batches if num_batches > 0 else 0.0
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.val_loader):
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                
                loss = nn.BCELoss()(output.squeeze(), target)
                total_loss += loss.item()
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                probs = output.squeeze().cpu().numpy()
                preds = (probs > 0.5).astype(int)
                
                all_probs.extend(probs)
                all_preds.extend(preds)
                all_targets.extend(target.cpu().numpy())
                
                # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªéªŒè¯å‡ ä¸ªæ‰¹æ¬¡
                if self.quick_test and batch_idx >= 2:
                    break
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(all_targets, all_preds, all_probs)
        metrics['loss'] = total_loss / len(self.val_loader) if len(self.val_loader) > 0 else 0.0
        
        return metrics
    
    def test(self) -> Dict[str, float]:
        """æµ‹è¯•æ¨¡å‹"""
        self.model.eval()
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for batch_idx, (data, target) in enumerate(self.test_loader):
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                
                # æ”¶é›†é¢„æµ‹ç»“æœ
                probs = output.squeeze().cpu().numpy()
                preds = (probs > 0.5).astype(int)
                
                all_probs.extend(probs)
                all_preds.extend(preds)
                all_targets.extend(target.cpu().numpy())
                
                # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªæµ‹è¯•å‡ ä¸ªæ‰¹æ¬¡
                if self.quick_test and batch_idx >= 2:
                    break
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(all_targets, all_preds, all_probs)
        
        return metrics
    
    def calculate_metrics(self, y_true: List, y_pred: List, y_prob: List) -> Dict[str, float]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        try:
            y_true = np.array(y_true)
            y_pred = np.array(y_pred)
            y_prob = np.array(y_prob)
            
            metrics = {
                'accuracy': accuracy_score(y_true, y_pred),
                'precision': precision_score(y_true, y_pred, zero_division=0),
                'recall': recall_score(y_true, y_pred, zero_division=0),
                'f1': f1_score(y_true, y_pred, zero_division=0)
            }
            
            # è®¡ç®—AUCæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼‰
            if len(np.unique(y_true)) > 1:
                try:
                    metrics['auc_roc'] = roc_auc_score(y_true, y_prob)
                    metrics['auc_pr'] = average_precision_score(y_true, y_prob)
                except:
                    metrics['auc_roc'] = 0.0
                    metrics['auc_pr'] = 0.0
            else:
                metrics['auc_roc'] = 0.0
                metrics['auc_pr'] = 0.0
            
            return metrics
            
        except Exception as e:
            self.logger.warning(f"æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            return {
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'auc_roc': 0.0,
                'auc_pr': 0.0
            }
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]) -> None:
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        try:
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
                'metrics': metrics,
                'config': self.config,
                'scaler': self.scaler
            }
            
            # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
            checkpoint_dir = 'checkpoints'
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
            checkpoint_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
            torch.save(checkpoint, checkpoint_path)
            
            # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œé¢å¤–ä¿å­˜
            if metrics['loss'] < self.best_val_loss:
                best_path = os.path.join(checkpoint_dir, 'best_model.pth')
                torch.save(checkpoint, best_path)
                self.best_val_loss = metrics['loss']
                self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {metrics['loss']:.4f}")
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def load_checkpoint(self, checkpoint_path: str) -> None:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        try:
            # ä½¿ç”¨weights_only=Falseæ¥å…¼å®¹sklearnå¯¹è±¡
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if self.scheduler and checkpoint['scheduler_state_dict']:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            self.scaler = checkpoint['scaler']
            
            self.logger.info(f"æˆåŠŸåŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            raise
    
    def export_model(self, export_path: str) -> None:
        """å¯¼å‡ºæ¨¡å‹"""
        try:
            # åˆ›å»ºå¯¼å‡ºç›®å½•ï¼ˆå¦‚æœè·¯å¾„åŒ…å«ç›®å½•ï¼‰
            export_dir = os.path.dirname(export_path)
            if export_dir:  # åªæœ‰å½“ç›®å½•è·¯å¾„ä¸ä¸ºç©ºæ—¶æ‰åˆ›å»º
                os.makedirs(export_dir, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹
            export_data = {
                'model_state_dict': self.model.state_dict(),
                'config': self.config,
                'scaler': self.scaler,
                'num_features': self.num_features
            }
            
            torch.save(export_data, export_path)
            self.logger.info(f"æ¨¡å‹å·²å¯¼å‡ºåˆ°: {export_path}")
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹å¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            self.logger.info("å¼€å§‹è®­ç»ƒ...")
            
            epochs = self.config['training']['epochs']
            patience = self.config['training']['patience']
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªè®­ç»ƒå°‘é‡epoch
            if self.quick_test:
                epochs = min(epochs, 2)
                patience = min(patience, 2)
                self.logger.info(f"å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šè®­ç»ƒ {epochs} ä¸ªepoch")
            
            for epoch in range(epochs):
                # è®­ç»ƒ
                train_loss = self.train_epoch()
                
                # éªŒè¯
                val_metrics = self.validate()
                val_loss = val_metrics['loss']
                
                # æ›´æ–°å­¦ä¹ ç‡
                if self.scheduler:
                    if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                        self.scheduler.step(val_loss)
                    else:
                        self.scheduler.step()
                
                # è®°å½•è®­ç»ƒå†å²
                self.training_history['train_loss'].append(train_loss)
                self.training_history['val_loss'].append(val_loss)
                self.training_history['val_accuracy'].append(val_metrics['accuracy'])
                self.training_history['val_f1'].append(val_metrics['f1'])
                
                # æ‰“å°è¿›åº¦
                self.logger.info(
                    f"Epoch {epoch+1}/{epochs} - "
                    f"Train Loss: {train_loss:.4f}, "
                    f"Val Loss: {val_loss:.4f}, "
                    f"Val Acc: {val_metrics['accuracy']:.4f}, "
                    f"Val F1: {val_metrics['f1']:.4f}"
                )
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                self.save_checkpoint(epoch, val_metrics)
                
                # æ—©åœæ£€æŸ¥
                if val_loss < self.best_val_loss:
                    self.best_val_loss = val_loss
                    self.patience_counter = 0
                else:
                    self.patience_counter += 1
                    
                if self.patience_counter >= patience:
                    self.logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} ä¸ªepochåœæ­¢è®­ç»ƒ")
                    break
            
            # æµ‹è¯•æœ€ä½³æ¨¡å‹
            best_model_path = 'checkpoints/best_model.pth'
            if os.path.exists(best_model_path):
                self.load_checkpoint(best_model_path)
            
            test_metrics = self.test()
            
            # ä¿å­˜è®­ç»ƒå†å²
            history_path = 'training_history.json'
            with open(history_path, 'w') as f:
                json.dump(self.training_history, f, indent=2)
            
            # å¯¼å‡ºæœ€ç»ˆæ¨¡å‹
            self.export_model('final_model.pth')
            
            self.logger.info("è®­ç»ƒå®Œæˆï¼")
            self.logger.info(f"æœ€ç»ˆæµ‹è¯•ç»“æœ: {test_metrics}")
            
            return {
                'training_history': self.training_history,
                'test_metrics': test_metrics,
                'best_val_loss': self.best_val_loss
            }
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            raise


def quick_validation():
    """å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯ä»£ç æ­£ç¡®æ€§"""
    print("ğŸš€ å¼€å§‹ training.py å¿«é€ŸéªŒè¯...")
    
    try:
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        config_path = "config.yaml"
        data_config_path = "data.yaml"
        
        if not os.path.exists(config_path):
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
            
        if not os.path.exists(data_config_path):
            print(f"âŒ æ•°æ®é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {data_config_path}")
            return False
        
        print("âœ… é…ç½®æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
        
        # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹
        trainer = TKANTrainer(config_path, data_config_path, quick_test=True)
        print("âœ… è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # å°è¯•åŠ è½½æ•°æ®
        try:
            trainer.load_data()
            print("âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ æ•°æ®åŠ è½½å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼‰: {e}")
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒéªŒè¯
            print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡ŒéªŒè¯...")
            
            # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
            np.random.seed(42)
            n_samples = 100
            n_features = 50
            
            # æ¨¡æ‹Ÿç‰¹å¾æ•°æ®
            X = np.random.randn(n_samples, n_features).astype(np.float32)
            y = np.random.randint(0, 2, n_samples).astype(np.float32)
            
            # åˆ›å»ºæ¨¡æ‹ŸDataFrame
            feature_cols = [f"feature_{i}@test" for i in range(n_features)]
            data_dict = {col: X[:, i] for i, col in enumerate(feature_cols)}
            data_dict['class'] = y
            data_dict['date'] = pd.date_range('2020-01-01', periods=n_samples, freq='D')
            
            trainer.raw_data = pd.DataFrame(data_dict)
            print("âœ… æ¨¡æ‹Ÿæ•°æ®åˆ›å»ºæˆåŠŸ")
        
        # æ•°æ®é¢„å¤„ç†
        trainer.prepare_data()
        print("âœ… æ•°æ®é¢„å¤„ç†æˆåŠŸ")
        
        # è®¾ç½®æ¨¡å‹
        trainer.setup_model()
        print("âœ… æ¨¡å‹è®¾ç½®æˆåŠŸ")
        
        # å¿«é€Ÿè®­ç»ƒéªŒè¯
        print("ğŸ”„ å¼€å§‹å¿«é€Ÿè®­ç»ƒéªŒè¯...")
        results = trainer.train()
        print("âœ… è®­ç»ƒéªŒè¯æˆåŠŸ")
        
        print("ğŸ‰ training.py å¿«é€ŸéªŒè¯å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸è¿è¡Œ")
        return True
        
    except Exception as e:
        print(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='TKANæ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data-config', type=str, default='data.yaml',
                       help='æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--quick-test', action='store_true',
                       help='å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯è®­ç»ƒè„šæœ¬æ­£ç¡®æ€§')
    parser.add_argument('--resume', type=str, default=None,
                       help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    
    args = parser.parse_args()
    
    if args.quick_test:
        success = quick_validation()
        if success:
            print("âœ… å¿«é€ŸéªŒè¯æˆåŠŸï¼Œä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ")
        else:
            print("âŒ å¿«é€ŸéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
        return
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = TKANTrainer(args.config, args.data_config)
        
        # åŠ è½½æ•°æ®
        trainer.load_data()
        trainer.prepare_data()
        
        # è®¾ç½®æ¨¡å‹
        trainer.setup_model()
        
        # ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.resume:
            trainer.load_checkpoint(args.resume)
        
        # å¼€å§‹è®­ç»ƒ
        results = trainer.train()
        
        print("è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ç»ˆæµ‹è¯•æŒ‡æ ‡: {results['test_metrics']}")
        
    except Exception as e:
        print(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()