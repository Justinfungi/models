#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
training.py - BayesianCNN Training Script

è¯¥æ–‡ä»¶å®ç°äº†BayesianCNNæ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€
æ—©åœæœºåˆ¶å’Œæ¨¡å‹ä¿å­˜ã€‚æ”¯æŒé¡¹ç›®ç‰¹å®šçš„è¡¨æ ¼æ•°æ®æ ¼å¼å’Œè´å¶æ–¯ä¸ç¡®å®šæ€§é‡åŒ–ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import yaml
import json
import logging
import argparse
import os
import sys
import time
import random
from typing import Dict, Any, Tuple, Optional, List
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import warnings
from pathlib import Path

# è®¾ç½®è­¦å‘Šè¿‡æ»¤
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡å‹
try:
    from model import create_model, BayesianCNN
except ImportError:
    print("Warning: Could not import model.py. Make sure model.py is in the same directory.")


def set_all_seeds(seed: int = 42) -> None:
    """
    è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
    
    Args:
        seed: éšæœºç§å­å€¼
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def setup_device() -> torch.device:
    """
    è‡ªåŠ¨è®¾å¤‡é…ç½®å’Œä¼˜åŒ–è®¾ç½®
    
    Returns:
        é…ç½®å¥½çš„è®¾å¤‡å¯¹è±¡
    """
    if torch.cuda.is_available():
        device = torch.device('cuda')
        torch.cuda.empty_cache()
        print(f"Using GPU: {torch.cuda.get_device_name()}")
    else:
        device = torch.device('cpu')
        print("Using CPU")
    return device


class ELBOLoss(nn.Module):
    """
    Evidence Lower Bound (ELBO) æŸå¤±å‡½æ•°
    ç»“åˆè´Ÿå¯¹æ•°ä¼¼ç„¶å’ŒKLæ•£åº¦
    """
    
    def __init__(self, train_size: int):
        super().__init__()
        self.train_size = train_size
        self.nll_loss = nn.CrossEntropyLoss()
    
    def forward(self, outputs: torch.Tensor, targets: torch.Tensor, 
                kl_divergence: torch.Tensor, beta: float) -> torch.Tensor:
        """
        è®¡ç®—ELBOæŸå¤±
        
        Args:
            outputs: æ¨¡å‹è¾“å‡º
            targets: çœŸå®æ ‡ç­¾
            kl_divergence: KLæ•£åº¦
            beta: KLé¡¹æƒé‡
            
        Returns:
            ELBOæŸå¤±å€¼
        """
        nll = self.nll_loss(outputs, targets)
        kl_scaled = kl_divergence / self.train_size
        return nll + beta * kl_scaled


def logmeanexp(x: torch.Tensor, dim: int) -> torch.Tensor:
    """
    ç¨³å®šçš„log-mean-expè®¡ç®—ï¼Œç”¨äºé›†æˆé¢„æµ‹
    
    Args:
        x: è¾“å…¥å¼ é‡
        dim: è®¡ç®—ç»´åº¦
        
    Returns:
        log-mean-expç»“æœ
    """
    return torch.logsumexp(x, dim=dim) - np.log(x.shape[dim])


class BayesianCNNTrainer:
    """BayesianCNNè®­ç»ƒå™¨ - æ ¸å¿ƒè®­ç»ƒç®¡ç†ç±»"""
    
    def __init__(self, config_path: str = "config.yaml", 
                 data_config_path: str = "data.yaml"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ŒåŠ è½½é…ç½®ï¼Œè®¾ç½®ç¯å¢ƒ
        
        Args:
            config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            data_config_path: æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.data_config_path = data_config_path
        
        # åŠ è½½é…ç½®
        self.config = self.load_config(config_path)
        self.data_config = self.load_config(data_config_path)
        
        # è®¾ç½®ç¯å¢ƒ
        set_all_seeds(42)
        self.device = setup_device()
        self.setup_logging()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.optimizer = None
        self.criterion = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []
    
    def load_config(self, config_path: str) -> Dict[str, Any]:
        """
        åŠ è½½YAMLé…ç½®æ–‡ä»¶
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Returns:
            é…ç½®å­—å…¸
        """
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return config
        except FileNotFoundError:
            print(f"Warning: Config file {config_path} not found. Using default config.")
            return self.get_default_config()
        except Exception as e:
            print(f"Error loading config: {e}")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict[str, Any]:
        """
        è·å–é»˜è®¤é…ç½®
        
        Returns:
            é»˜è®¤é…ç½®å­—å…¸
        """
        return {
            'architecture': {
                'bayesian_config': {
                    'prior_mu': 0,
                    'prior_sigma': 0.1,
                    'posterior_mu_initial': [0, 0.1],
                    'posterior_rho_initial': [-3, 0.1]
                },
                'network': {
                    'input_dim': None,
                    'hidden_dims': [128, 64],
                    'output_dim': None,
                    'dropout': 0.2,
                    'activation': 'relu'
                }
            },
            'training': {
                'optimizer': {
                    'type': 'adam',
                    'learning_rate': 0.001,
                    'weight_decay': 1e-4
                },
                'epochs': 100,
                'batch_size': 32,
                'patience': 10,
                'loss': {
                    'kl_weight': 0.1
                },
                'ensemble': {
                    'train_samples': 1,
                    'val_samples': 5,
                    'test_samples': 10
                }
            }
        }
    
    def setup_logging(self):
        """è®¾ç½®ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ"""
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(os.path.join(log_dir, 'training.log')),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_data(self, data_path: Optional[str] = None, 
                  quick_test: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        åŠ è½½é¡¹ç›®ç‰¹å®šçš„è¡¨æ ¼æ•°æ®æ ¼å¼
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        try:
            # ç¡®å®šæ•°æ®è·¯å¾„
            if data_path is None:
                data_folder = self.data_config.get('data_paths', {}).get('data_folder', 'data/feature_set')
                data_file = self.data_config.get('data_paths', {}).get('data_file', 'mrmr_task_1_2013-01-01_2018-06-30.pq')
                data_path = os.path.join(data_folder, data_file)
            
            self.logger.info(f"Loading data from: {data_path}")
            
            # åŠ è½½parquetæ–‡ä»¶
            if not os.path.exists(data_path):
                self.logger.warning(f"Data file not found: {data_path}")
                return self.generate_synthetic_data(quick_test)
            
            df = pd.read_parquet(data_path)
            self.logger.info(f"Loaded data shape: {df.shape}")
            
            # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·ï¼‰
            feature_columns = [col for col in df.columns if '@' in col]
            if not feature_columns:
                self.logger.warning("No feature columns found with '@' symbol")
                # ä½¿ç”¨é™¤äº†æœ€åä¸€åˆ—å’Œæ—¥æœŸåˆ—ä¹‹å¤–çš„æ‰€æœ‰åˆ—ä½œä¸ºç‰¹å¾
                exclude_cols = ['date', 'time', 'symbol'] + [df.columns[-1]]
                feature_columns = [col for col in df.columns if col not in exclude_cols]
            
            label_column = df.columns[-1]
            self.logger.info(f"Found {len(feature_columns)} feature columns")
            self.logger.info(f"Label column: {label_column}")
            
            # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šä½¿ç”¨å°‘é‡æ•°æ®
            if quick_test:
                df = df.head(200)
                self.logger.info(f"Quick test mode: using {len(df)} samples")
            
            # æ—¶é—´åºåˆ—åˆ†å‰²
            if 'date' in df.columns:
                df_sorted = df.sort_values('date')
                start_date = pd.to_datetime(df_sorted['date'].iloc[0])
                train_end = start_date + pd.DateOffset(years=1)
                
                train_mask = pd.to_datetime(df_sorted['date']) < train_end
                test_mask = pd.to_datetime(df_sorted['date']) >= train_end
                
                X_train = df_sorted[train_mask][feature_columns].values
                y_train = df_sorted[train_mask][label_column].values
                X_test = df_sorted[test_mask][feature_columns].values
                y_test = df_sorted[test_mask][label_column].values
            else:
                # å¦‚æœæ²¡æœ‰æ—¥æœŸåˆ—ï¼Œä½¿ç”¨éšæœºåˆ†å‰²
                X = df[feature_columns].values
                y = df[label_column].values
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.2, random_state=42, stratify=y
                )
            
            self.logger.info(f"Train set: {X_train.shape}, Test set: {X_test.shape}")
            
            return X_train, y_train, X_test, y_test
            
        except Exception as e:
            self.logger.error(f"Error loading data: {e}")
            return self.generate_synthetic_data(quick_test)
    
    def generate_synthetic_data(self, quick_test: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        ç”Ÿæˆåˆæˆæ•°æ®ç”¨äºæµ‹è¯•
        
        Args:
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            åˆæˆçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        """
        self.logger.info("Generating synthetic data for testing")
        
        n_samples = 200 if quick_test else 1000
        n_features = 20
        n_classes = 2
        
        # ç”Ÿæˆéšæœºæ•°æ®
        X = np.random.randn(n_samples, n_features)
        y = np.random.randint(0, n_classes, n_samples)
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        return X_train, y_train, X_test, y_test
    
    def prepare_data(self, X_train: np.ndarray, y_train: np.ndarray, 
                     X_test: np.ndarray, y_test: np.ndarray,
                     quick_test: bool = False) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """
        å‡†å¤‡æ•°æ®åŠ è½½å™¨
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•æ ‡ç­¾
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
            
        Returns:
            è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®åŠ è½½å™¨
        """
        # æ•°æ®é¢„å¤„ç†
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # æ ‡ç­¾ç¼–ç 
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        y_test_encoded = self.label_encoder.transform(y_test)
        
        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        X_train_final, X_val, y_train_final, y_val = train_test_split(
            X_train_scaled, y_train_encoded, test_size=0.2, random_state=42,
            stratify=y_train_encoded
        )
        
        # æ›´æ–°é…ç½®ä¸­çš„æ•°æ®ç»´åº¦ä¿¡æ¯
        self.config['architecture']['network']['input_dim'] = X_train_final.shape[1]
        self.config['architecture']['network']['output_dim'] = len(np.unique(y_train_encoded))
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X_train_tensor = torch.FloatTensor(X_train_final)
        y_train_tensor = torch.LongTensor(y_train_final)
        X_val_tensor = torch.FloatTensor(X_val)
        y_val_tensor = torch.LongTensor(y_val)
        X_test_tensor = torch.FloatTensor(X_test_scaled)
        y_test_tensor = torch.LongTensor(y_test_encoded)
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
        
        # æ‰¹æ¬¡å¤§å°è°ƒæ•´
        batch_size = 16 if quick_test else self.config['training']['batch_size']
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        self.logger.info(f"Data prepared - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}")
        
        return train_loader, val_loader, test_loader
    
    def setup_model(self, train_size: int):
        """
        è®¾ç½®æ¨¡å‹ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        
        Args:
            train_size: è®­ç»ƒé›†å¤§å°
        """
        try:
            # åˆ›å»ºæ¨¡å‹
            self.model = create_model(self.config).to(self.device)
            self.logger.info(f"Model created with {sum(p.numel() for p in self.model.parameters())} parameters")
        except Exception as e:
            self.logger.warning(f"Could not create model from model.py: {e}")
            # åˆ›å»ºç®€å•çš„è´å¶æ–¯æ¨¡å‹
            self.model = self.create_simple_bayesian_model().to(self.device)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer_config = self.config['training']['optimizer']
        if optimizer_config['type'].lower() == 'adam':
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=optimizer_config['learning_rate'],
                weight_decay=optimizer_config.get('weight_decay', 0)
            )
        else:
            self.optimizer = optim.SGD(
                self.model.parameters(),
                lr=optimizer_config['learning_rate'],
                weight_decay=optimizer_config.get('weight_decay', 0),
                momentum=optimizer_config.get('momentum', 0.9)
            )
        
        # è®¾ç½®æŸå¤±å‡½æ•°
        self.criterion = ELBOLoss(train_size).to(self.device)
        
        self.logger.info("Model, optimizer, and loss function initialized")
    
    def create_simple_bayesian_model(self) -> nn.Module:
        """
        åˆ›å»ºç®€å•çš„è´å¶æ–¯æ¨¡å‹ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Returns:
            ç®€å•çš„è´å¶æ–¯æ¨¡å‹
        """
        input_dim = self.config['architecture']['network']['input_dim']
        output_dim = self.config['architecture']['network']['output_dim']
        hidden_dims = self.config['architecture']['network']['hidden_dims']
        dropout = self.config['architecture']['network']['dropout']
        
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        model = nn.Sequential(*layers)
        
        # æ·»åŠ ç®€å•çš„KLæ•£åº¦è®¡ç®—æ–¹æ³•
        def forward_with_kl(self, x):
            output = super(type(model), self).forward(x)
            # ç®€å•çš„KLæ•£åº¦ä¼°è®¡
            kl = torch.tensor(0.0, device=x.device)
            for param in self.parameters():
                kl += torch.sum(param ** 2) * 0.001
            return output, kl
        
        model.forward_with_kl = forward_with_kl.__get__(model, type(model))
        
        return model
    
    def get_beta(self, batch_idx: int, num_batches: int, epoch: int = 0) -> float:
        """
        è®¡ç®—KLæ•£åº¦æƒé‡beta
        
        Args:
            batch_idx: å½“å‰æ‰¹æ¬¡ç´¢å¼•
            num_batches: æ€»æ‰¹æ¬¡æ•°
            epoch: å½“å‰epoch
            
        Returns:
            betaæƒé‡å€¼
        """
        return self.config['training']['loss']['kl_weight']
    
    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Tuple[float, float, float]:
        """
        è®­ç»ƒä¸€ä¸ªepoch
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            
        Returns:
            å¹³å‡æŸå¤±ã€å‡†ç¡®ç‡ã€KLæ•£åº¦
        """
        self.model.train()
        total_loss = 0.0
        total_kl = 0.0
        all_preds = []
        all_labels = []
        
        num_ens = self.config['training']['ensemble']['train_samples']
        
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            # é›†æˆé‡‡æ ·
            if hasattr(self.model, 'forward_with_kl'):
                # ä½¿ç”¨è´å¶æ–¯å‰å‘ä¼ æ’­
                outputs_list = []
                kl_total = 0.0
                
                for j in range(num_ens):
                    net_out, kl = self.model.forward_with_kl(inputs)
                    kl_total += kl
                    outputs_list.append(F.log_softmax(net_out, dim=1))
                
                if len(outputs_list) > 1:
                    outputs = torch.stack(outputs_list, dim=2)
                    log_outputs = logmeanexp(outputs, dim=2)
                else:
                    log_outputs = outputs_list[0]
                
                kl_avg = kl_total / num_ens
            else:
                # æ ‡å‡†å‰å‘ä¼ æ’­
                outputs = self.model(inputs)
                log_outputs = F.log_softmax(outputs, dim=1)
                kl_avg = torch.tensor(0.0, device=self.device)
            
            # è®¡ç®—æŸå¤±
            beta = self.get_beta(batch_idx, len(train_loader), epoch)
            loss = self.criterion(torch.exp(log_outputs), labels, kl_avg, beta)
            
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            total_kl += kl_avg.item()
            
            # è®°å½•é¢„æµ‹
            preds = torch.argmax(log_outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
        
        avg_loss = total_loss / len(train_loader)
        avg_kl = total_kl / len(train_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        
        return avg_loss, accuracy, avg_kl
    
    def validate(self, val_loader: DataLoader, epoch: int) -> Tuple[float, float, Dict[str, float]]:
        """
        éªŒè¯æ¨¡å‹
        
        Args:
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            epoch: å½“å‰epoch
            
        Returns:
            éªŒè¯æŸå¤±ã€å‡†ç¡®ç‡ã€è¯¦ç»†æŒ‡æ ‡
        """
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        all_probs = []
        
        num_ens = self.config['training']['ensemble']['val_samples']
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                # é›†æˆé¢„æµ‹
                if hasattr(self.model, 'forward_with_kl'):
                    outputs_list = []
                    kl_total = 0.0
                    
                    for j in range(num_ens):
                        net_out, kl = self.model.forward_with_kl(inputs)
                        kl_total += kl
                        outputs_list.append(F.log_softmax(net_out, dim=1))
                    
                    if len(outputs_list) > 1:
                        outputs = torch.stack(outputs_list, dim=2)
                        log_outputs = logmeanexp(outputs, dim=2)
                    else:
                        log_outputs = outputs_list[0]
                    
                    kl_avg = kl_total / num_ens
                else:
                    outputs = self.model(inputs)
                    log_outputs = F.log_softmax(outputs, dim=1)
                    kl_avg = torch.tensor(0.0, device=self.device)
                
                # è®¡ç®—æŸå¤±
                beta = self.get_beta(0, 1, epoch)
                loss = self.criterion(torch.exp(log_outputs), labels, kl_avg, beta)
                total_loss += loss.item()
                
                # è®°å½•é¢„æµ‹
                probs = torch.exp(log_outputs)
                preds = torch.argmax(log_outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        
        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy,
            'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),
            'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),
            'f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0)
        }
        
        # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œè®¡ç®—AUC
        if len(np.unique(all_labels)) == 2:
            try:
                all_probs_array = np.array(all_probs)
                if all_probs_array.shape[1] == 2:
                    metrics['auc'] = roc_auc_score(all_labels, all_probs_array[:, 1])
            except:
                pass
        
        return avg_loss, accuracy, metrics
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, quick_test: bool = False):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        
        Args:
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            quick_test: æ˜¯å¦ä¸ºå¿«é€Ÿæµ‹è¯•æ¨¡å¼
        """
        epochs = 2 if quick_test else self.config['training']['epochs']
        patience = 3 if quick_test else self.config['training']['patience']
        
        self.logger.info(f"Starting training for {epochs} epochs")
        
        for epoch in range(epochs):
            start_time = time.time()
            
            # è®­ç»ƒ
            train_loss, train_acc, train_kl = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_loss, val_acc, val_metrics = self.validate(val_loader, epoch)
            
            # è®°å½•å†å²
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_accuracies.append(train_acc)
            self.val_accuracies.append(val_acc)
            
            epoch_time = time.time() - start_time
            
            # æ—¥å¿—è®°å½•
            self.logger.info(
                f"Epoch {epoch+1}/{epochs} - "
                f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
                f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, "
                f"KL: {train_kl:.4f}, Time: {epoch_time:.2f}s"
            )
            
            # æ—©åœæ£€æŸ¥
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                self.save_checkpoint(epoch, val_metrics)
                self.logger.info(f"New best model saved with val_loss: {val_loss:.4f}")
            else:
                self.patience_counter += 1
                
            if self.patience_counter >= patience:
                self.logger.info(f"Early stopping triggered after {epoch+1} epochs")
                break
        
        self.logger.info("Training completed")
    
    def test(self, test_loader: DataLoader) -> Dict[str, float]:
        """
        æµ‹è¯•æ¨¡å‹æ€§èƒ½
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            
        Returns:
            æµ‹è¯•æŒ‡æ ‡å­—å…¸
        """
        self.logger.info("Starting model testing")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        checkpoint_path = "checkpoints/best_model.pth"
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.logger.info("Loaded best model for testing")
        
        self.model.eval()
        all_preds = []
        all_labels = []
        all_probs = []
        uncertainties = []
        
        num_ens = self.config['training']['ensemble']['test_samples']
        
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                # é›†æˆé¢„æµ‹å’Œä¸ç¡®å®šæ€§è®¡ç®—
                if hasattr(self.model, 'forward_with_kl'):
                    outputs_list = []
                    
                    for j in range(num_ens):
                        net_out, _ = self.model.forward_with_kl(inputs)
                        outputs_list.append(F.softmax(net_out, dim=1))
                    
                    if len(outputs_list) > 1:
                        outputs_stack = torch.stack(outputs_list, dim=0)  # (num_ens, batch_size, num_classes)
                        mean_probs = torch.mean(outputs_stack, dim=0)
                        
                        # è®¡ç®—ä¸ç¡®å®šæ€§
                        epistemic_uncertainty = torch.var(outputs_stack, dim=0).sum(dim=1)
                        aleatoric_uncertainty = -torch.sum(mean_probs * torch.log(mean_probs + 1e-8), dim=1)
                        total_uncertainty = epistemic_uncertainty + aleatoric_uncertainty
                        
                        uncertainties.extend(total_uncertainty.cpu().numpy())
                    else:
                        mean_probs = outputs_list[0]
                        uncertainties.extend([0.0] * inputs.size(0))
                else:
                    outputs = self.model(inputs)
                    mean_probs = F.softmax(outputs, dim=1)
                    uncertainties.extend([0.0] * inputs.size(0))
                
                preds = torch.argmax(mean_probs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_probs.extend(mean_probs.cpu().numpy())
        
        # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
        test_metrics = {
            'test_accuracy': accuracy_score(all_labels, all_preds),
            'test_precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),
            'test_recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),
            'test_f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0),
            'mean_uncertainty': np.mean(uncertainties),
            'std_uncertainty': np.std(uncertainties)
        }
        
        # å¦‚æœæ˜¯äºŒåˆ†ç±»ï¼Œè®¡ç®—AUC
        if len(np.unique(all_labels)) == 2:
            try:
                all_probs_array = np.array(all_probs)
                if all_probs_array.shape[1] == 2:
                    test_metrics['test_auc'] = roc_auc_score(all_labels, all_probs_array[:, 1])
            except:
                pass
        
        self.logger.info(f"Test Results: {test_metrics}")
        return test_metrics
    
    def save_checkpoint(self, epoch: int, metrics: Dict[str, float]):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            epoch: å½“å‰epoch
            metrics: éªŒè¯æŒ‡æ ‡
        """
        checkpoint_dir = "checkpoints"
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_loss': self.best_val_loss,
            'config': self.config,
            'metrics': metrics,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_accuracies': self.train_accuracies,
            'val_accuracies': self.val_accuracies
        }
        
        torch.save(checkpoint, os.path.join(checkpoint_dir, 'best_model.pth'))
    
    def save_results(self, test_metrics: Dict[str, float]):
        """
        ä¿å­˜å®éªŒç»“æœåˆ°JSONæ–‡ä»¶
        
        Args:
            test_metrics: æµ‹è¯•æŒ‡æ ‡
        """
        results_dir = "results"
        os.makedirs(results_dir, exist_ok=True)
        
        results = {
            'model_name': 'BayesianCNN',
            'config': self.config,
            'test_metrics': test_metrics,
            'training_history': {
                'train_losses': self.train_losses,
                'val_losses': self.val_losses,
                'train_accuracies': self.train_accuracies,
                'val_accuracies': self.val_accuracies
            },
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        }
        
        results_path = os.path.join(results_dir, 'training_results.json')
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        self.logger.info(f"Results saved to {results_path}")
    
    def calculate_uncertainty(self, test_loader: DataLoader) -> Dict[str, np.ndarray]:
        """
        è®¡ç®—è´å¶æ–¯ä¸ç¡®å®šæ€§
        
        Args:
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨
            
        Returns:
            ä¸ç¡®å®šæ€§å­—å…¸
        """
        self.model.eval()
        all_predictions = []
        all_labels = []
        
        num_samples = self.config['training']['ensemble']['test_samples']
        
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs = inputs.to(self.device)
                
                # å¤šæ¬¡é‡‡æ ·
                predictions = []
                
                for _ in range(num_samples):
                    if hasattr(self.model, 'forward_with_kl'):
                        outputs, _ = self.model.forward_with_kl(inputs)
                    else:
                        outputs = self.model(inputs)
                    
                    probs = F.softmax(outputs, dim=1)
                    predictions.append(probs.cpu().numpy())
                
                predictions = np.array(predictions)  # (num_samples, batch_size, num_classes)
                all_predictions.append(predictions)
                all_labels.extend(labels.numpy())
        
        # è®¡ç®—ä¸ç¡®å®šæ€§
        all_predictions = np.concatenate(all_predictions, axis=1)
        
        # è®¤è¯†ä¸ç¡®å®šæ€§ (é¢„æµ‹æ–¹å·®)
        mean_predictions = np.mean(all_predictions, axis=0)
        epistemic_uncertainty = np.var(all_predictions, axis=0)
        
        # å¶ç„¶ä¸ç¡®å®šæ€§ (é¢„æµ‹ç†µ)
        aleatoric_uncertainty = -np.sum(mean_predictions * np.log(mean_predictions + 1e-8), axis=1)
        
        # æ€»ä¸ç¡®å®šæ€§
        total_uncertainty = epistemic_uncertainty.sum(axis=1) + aleatoric_uncertainty
        
        return {
            'epistemic_uncertainty': epistemic_uncertainty,
            'aleatoric_uncertainty': aleatoric_uncertainty,
            'total_uncertainty': total_uncertainty,
            'mean_predictions': mean_predictions
        }


def train_model_ensemble(net, optimizer, criterion, trainloader, num_ens: int, 
                        beta_type, epoch: int, num_epochs: int):
    """
    è´å¶æ–¯é›†æˆè®­ç»ƒå‡½æ•° - æ”¯æŒå¤šæ¬¡é‡‡æ ·
    
    Args:
        net: ç¥ç»ç½‘ç»œæ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨
        criterion: æŸå¤±å‡½æ•°
        trainloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        num_ens: é›†æˆé‡‡æ ·æ¬¡æ•°
        beta_type: Betaæƒé‡ç±»å‹
        epoch: å½“å‰epoch
        num_epochs: æ€»epochæ•°
        
    Returns:
        è®­ç»ƒæŸå¤±å’Œå‡†ç¡®ç‡
    """
    net.train()
    train_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (inputs, targets) in enumerate(trainloader):
        if torch.cuda.is_available():
            inputs, targets = inputs.cuda(), targets.cuda()
        
        optimizer.zero_grad()
        
        # é›†æˆé‡‡æ ·
        outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens)
        if torch.cuda.is_available():
            outputs = outputs.cuda()
        
        kl = 0
        for j in range(num_ens):
            net_out, _kl = net(inputs)
            kl += _kl
            outputs[:, :, j] = F.log_softmax(net_out, dim=1)
        
        kl = kl / num_ens
        log_outputs = logmeanexp(outputs, dim=2)
        
        beta = beta_type
        if isinstance(beta_type, str):
            beta = 0.1  # é»˜è®¤å€¼
        
        loss = criterion(log_outputs, targets, kl, beta)
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        _, predicted = log_outputs.max(1)
        total += targets.size(0)
        correct += predicted.eq(targets).sum().item()
    
    return train_loss / len(trainloader), 100. * correct / total


def validate_model_ensemble(net, criterion, validloader, num_ens: int, 
                           beta_type, epoch: int, num_epochs: int):
    """
    è´å¶æ–¯é›†æˆéªŒè¯å‡½æ•°
    
    Args:
        net: ç¥ç»ç½‘ç»œæ¨¡å‹
        criterion: æŸå¤±å‡½æ•°
        validloader: éªŒè¯æ•°æ®åŠ è½½å™¨
        num_ens: é›†æˆé‡‡æ ·æ¬¡æ•°
        beta_type: Betaæƒé‡ç±»å‹
        epoch: å½“å‰epoch
        num_epochs: æ€»epochæ•°
        
    Returns:
        éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
    """
    net.eval()
    valid_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_idx, (inputs, targets) in enumerate(validloader):
            if torch.cuda.is_available():
                inputs, targets = inputs.cuda(), targets.cuda()
            
            # é›†æˆé‡‡æ ·
            outputs = torch.zeros(inputs.shape[0], net.num_classes, num_ens)
            if torch.cuda.is_available():
                outputs = outputs.cuda()
            
            kl = 0
            for j in range(num_ens):
                net_out, _kl = net(inputs)
                kl += _kl
                outputs[:, :, j] = F.log_softmax(net_out, dim=1)
            
            kl = kl / num_ens
            log_outputs = logmeanexp(outputs, dim=2)
            
            beta = beta_type
            if isinstance(beta_type, str):
                beta = 0.1  # é»˜è®¤å€¼
            
            loss = criterion(log_outputs, targets, kl, beta)
            
            valid_loss += loss.item()
            _, predicted = log_outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    return valid_loss / len(validloader), 100. * correct / total


def calculate_uncertainty(outputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    è®¡ç®—é¢„æµ‹ä¸ç¡®å®šæ€§ - è´å¶æ–¯CNNç‰¹æœ‰åŠŸèƒ½
    
    Args:
        outputs: æ¨¡å‹è¾“å‡ºå¼ é‡ (num_samples, batch_size, num_classes)
        
    Returns:
        è®¤è¯†ä¸ç¡®å®šæ€§å’Œå¶ç„¶ä¸ç¡®å®šæ€§
    """
    # è®¡ç®—å¹³å‡é¢„æµ‹
    mean_outputs = torch.mean(outputs, dim=0)
    
    # è®¤è¯†ä¸ç¡®å®šæ€§ (é¢„æµ‹æ–¹å·®)
    epistemic_uncertainty = torch.var(outputs, dim=0)
    
    # å¶ç„¶ä¸ç¡®å®šæ€§ (é¢„æµ‹ç†µ)
    aleatoric_uncertainty = -torch.sum(mean_outputs * torch.log(mean_outputs + 1e-8), dim=1)
    
    return epistemic_uncertainty.sum(dim=1), aleatoric_uncertainty


def quick_validation():
    """å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯ä»£ç æ­£ç¡®æ€§"""
    print("ğŸš€ å¼€å§‹ training.py å¿«é€ŸéªŒè¯...")
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = BayesianCNNTrainer()
        
        # åŠ è½½æ•°æ®ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
        X_train, y_train, X_test, y_test = trainer.load_data(quick_test=True)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: Train {X_train.shape}, Test {X_test.shape}")
        
        # å‡†å¤‡æ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = trainer.prepare_data(
            X_train, y_train, X_test, y_test, quick_test=True
        )
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        
        # è®¾ç½®æ¨¡å‹
        trainer.setup_model(len(train_loader.dataset))
        print(f"âœ… æ¨¡å‹è®¾ç½®æˆåŠŸ")
        
        # å¿«é€Ÿè®­ç»ƒï¼ˆ1ä¸ªepochï¼‰
        print("ğŸƒ å¼€å§‹å¿«é€Ÿè®­ç»ƒéªŒè¯...")
        trainer.train(train_loader, val_loader, quick_test=True)
        print(f"âœ… è®­ç»ƒæµç¨‹éªŒè¯æˆåŠŸ")
        
        # å¿«é€Ÿæµ‹è¯•
        test_metrics = trainer.test(test_loader)
        print(f"âœ… æµ‹è¯•æµç¨‹éªŒè¯æˆåŠŸ: {test_metrics}")
        
        # ä¿å­˜ç»“æœ
        trainer.save_results(test_metrics)
        print(f"âœ… ç»“æœä¿å­˜æˆåŠŸ")
        
        print("ğŸ‰ training.py å¿«é€ŸéªŒè¯å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸è¿è¡Œã€‚")
        
    except Exception as e:
        print(f"âŒ å¿«é€ŸéªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='BayesianCNN Training Script')
    parser.add_argument('--config', type=str, default='config.yaml',
                       help='Path to config file')
    parser.add_argument('--data-config', type=str, default='data.yaml',
                       help='Path to data config file')
    parser.add_argument('--data-path', type=str, default=None,
                       help='Path to data file')
    parser.add_argument('--quick-test', action='store_true',
                       help='å¿«é€ŸéªŒè¯æ¨¡å¼ï¼šä½¿ç”¨æœ€å°é¢„ç®—éªŒè¯è®­ç»ƒè„šæœ¬æ­£ç¡®æ€§')
    parser.add_argument('--epochs', type=int, default=None,
                       help='Number of training epochs')
    parser.add_argument('--batch-size', type=int, default=None,
                       help='Batch size for training')
    parser.add_argument('--lr', type=float, default=None,
                       help='Learning rate')
    
    args = parser.parse_args()
    
    if args.quick_test:
        quick_validation()
        return
    
    try:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = BayesianCNNTrainer(args.config, args.data_config)
        
        # è¦†ç›–é…ç½®å‚æ•°
        if args.epochs is not None:
            trainer.config['training']['epochs'] = args.epochs
        if args.batch_size is not None:
            trainer.config['training']['batch_size'] = args.batch_size
        if args.lr is not None:
            trainer.config['training']['optimizer']['learning_rate'] = args.lr
        
        # åŠ è½½æ•°æ®
        X_train, y_train, X_test, y_test = trainer.load_data(args.data_path)
        
        # å‡†å¤‡æ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = trainer.prepare_data(
            X_train, y_train, X_test, y_test
        )
        
        # è®¾ç½®æ¨¡å‹
        trainer.setup_model(len(train_loader.dataset))
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train(train_loader, val_loader)
        
        # æµ‹è¯•æ¨¡å‹
        test_metrics = trainer.test(test_loader)
        
        # ä¿å­˜ç»“æœ
        trainer.save_results(test_metrics)
        
        # è®¡ç®—ä¸ç¡®å®šæ€§
        if trainer.config.get('inference', {}).get('uncertainty_estimation', True):
            uncertainty_results = trainer.calculate_uncertainty(test_loader)
            trainer.logger.info(f"Uncertainty analysis completed")
        
        trainer.logger.info("Training pipeline completed successfully!")
        
    except Exception as e:
        print(f"Error in training pipeline: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()