#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BayesianCNN_unified.py - ç»Ÿä¸€æ¨¡å‹å®ç°
åŸºäºè´å¶æ–¯å·ç§¯ç¥ç»ç½‘ç»œçš„ç»Ÿä¸€æ¨¡å‹å®ç°ï¼Œæ”¯æŒä¸ç¡®å®šæ€§é‡åŒ–å’ŒOptunaè¶…å‚æ•°ä¼˜åŒ–
"""

import os
import sys
import time
import yaml
import json
import pickle
import logging
import argparse
import warnings
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Union, List, Tuple, Callable
from dataclasses import dataclass, asdict
from abc import ABC, abstractmethod

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
import torch.nn.functional as F
from torch.nn import Parameter
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset, Dataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

warnings.filterwarnings('ignore')

# =============================================================================
# åŸºç¡€é…ç½®å’Œå·¥å…·å‡½æ•°
# =============================================================================

@dataclass
class BayesianCNNConfig:
    """BayesianCNNé…ç½®ç±»"""
    # æ¨¡å‹åŸºç¡€é…ç½®
    model_name: str = "BayesianCNN"
    model_type: str = "classification"
    
    # è®­ç»ƒé…ç½®
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 100
    
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    use_mixed_precision: bool = True
    dataloader_num_workers: int = 4
    pin_memory: bool = True
    gradient_clip_value: float = 1.0
    
    # è´å¶æ–¯é…ç½®
    prior_mu: float = 0.0
    prior_sigma: float = 0.1
    posterior_mu_initial: List[float] = None
    posterior_rho_initial: List[float] = None
    kl_weight: float = 0.1
    
    # ç½‘ç»œé…ç½®
    input_dim: Optional[int] = None
    hidden_dims: List[int] = None
    output_dim: Optional[int] = None
    dropout: float = 0.2
    activation: str = 'relu'
    
    # é›†æˆé…ç½®
    train_samples: int = 1
    val_samples: int = 5
    test_samples: int = 10
    
    # è®¾å¤‡é…ç½®
    device: str = "auto"
    seed: int = 42
    
    # è·¯å¾„é…ç½®
    data_path: str = "./data"
    checkpoint_dir: str = "./checkpoints"
    results_dir: str = "./results"
    logs_dir: str = "./logs"
    
    def __post_init__(self):
        if self.posterior_mu_initial is None:
            self.posterior_mu_initial = [0.0, 0.1]
        if self.posterior_rho_initial is None:
            self.posterior_rho_initial = [-3.0, 0.1]
        if self.hidden_dims is None:
            self.hidden_dims = [128, 64]

def set_all_seeds(seed: int = 42) -> None:
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§"""
    import random
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def setup_device(device_choice: str = "auto") -> torch.device:
    """è‡ªåŠ¨è®¾å¤‡é…ç½®å¹¶è®°å½•è¯¦ç»†çš„GPUä¿¡æ¯"""
    import logging
    logger = logging.getLogger(__name__)

    if device_choice == "cpu":
        device = torch.device('cpu')
        print("ğŸ–¥ï¸  å¼ºåˆ¶ä½¿ç”¨CPU")
        logger.info("è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨CPU")
    elif device_choice == "cuda":
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸš€ å¼ºåˆ¶ä½¿ç”¨GPU: {gpu_name} ({gpu_memory:.1f}GB)")
            logger.info(f"è®¾å¤‡é…ç½®: å¼ºåˆ¶ä½¿ç”¨GPU - {gpu_name} ({gpu_memory:.1f}GB)")
            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            logger.warning("CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
    else:  # auto
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            cached_memory = torch.cuda.memory_reserved() / 1024**3

            print(f"ğŸš€ è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU: {gpu_name}")
            print(f"   ğŸ“Š GPUå†…å­˜: {gpu_memory:.1f}GB æ€»é‡")
            print(f"   ğŸ“Š GPUæ•°é‡: {gpu_count}")
            print(f"   ğŸ“Š å½“å‰ä½¿ç”¨: {current_memory:.2f}GB")
            print(f"   ğŸ“Š ç¼“å­˜ä½¿ç”¨: {cached_memory:.2f}GB")

            logger.info(f"è®¾å¤‡é…ç½®: è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨GPU")
            logger.info(f"GPUä¿¡æ¯: {gpu_name}, {gpu_memory:.1f}GBæ€»å†…å­˜, {gpu_count}ä¸ªGPU")
            logger.info(f"GPUå†…å­˜ä½¿ç”¨: {current_memory:.2f}GBå·²ç”¨, {cached_memory:.2f}GBç¼“å­˜")

            torch.cuda.empty_cache()
        else:
            device = torch.device('cpu')
            print("ğŸ–¥ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
            logger.info("è®¾å¤‡é…ç½®: GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")

    return device

def setup_logging(log_dir: str = "./logs", prefix: str = "unified", log_filename: str = None) -> logging.Logger:
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Path(log_dir)
    log_dir.mkdir(parents=True, exist_ok=True)
    
    if log_filename:
        log_file = log_dir / log_filename
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = log_dir / f"{prefix}_{timestamp}.log"
    
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    return logger

def create_directories(config: BayesianCNNConfig) -> None:
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    dirs = [config.checkpoint_dir, config.results_dir, config.logs_dir]
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

# =============================================================================
# è´å¶æ–¯å±‚å®ç°
# =============================================================================

class ModuleWrapper(nn.Module):
    """è´å¶æ–¯å±‚çš„åŸºç¡€åŒ…è£…ç±»"""
    
    def __init__(self):
        super(ModuleWrapper, self).__init__()
    
    def kl_divergence(self) -> torch.Tensor:
        """è®¡ç®—KLæ•£åº¦ï¼Œå­ç±»éœ€è¦å®ç°æ­¤æ–¹æ³•"""
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç°kl_divergenceæ–¹æ³•")

class BBBLinear(ModuleWrapper):
    """è´å¶æ–¯çº¿æ€§å±‚å®ç°"""
    
    def __init__(self, in_features: int, out_features: int, bias: bool = True, 
                 prior_mu: float = 0.0, prior_sigma: float = 0.1,
                 posterior_mu_initial: List[float] = None, 
                 posterior_rho_initial: List[float] = None):
        super(BBBLinear, self).__init__()
        
        self.in_features = in_features
        self.out_features = out_features
        self.use_bias = bias
        
        # è®¾ç½®å…ˆéªŒåˆ†å¸ƒå‚æ•°
        self.prior_mu = prior_mu
        self.prior_sigma = prior_sigma
        self.posterior_mu_initial = posterior_mu_initial or [0.0, 0.1]
        self.posterior_rho_initial = posterior_rho_initial or [-3.0, 0.1]
        
        # æƒé‡å‚æ•°
        self.weight_mu = Parameter(torch.Tensor(out_features, in_features))
        self.weight_rho = Parameter(torch.Tensor(out_features, in_features))
        
        # åç½®å‚æ•°
        if self.use_bias:
            self.bias_mu = Parameter(torch.Tensor(out_features))
            self.bias_rho = Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias_mu', None)
            self.register_parameter('bias_rho', None)
        
        self.reset_parameters()
    
    def reset_parameters(self) -> None:
        """é‡ç½®å‚æ•°åˆ°åˆå§‹å€¼"""
        self.weight_mu.data.normal_(
            self.posterior_mu_initial[0], 
            self.posterior_mu_initial[1]
        )
        self.weight_rho.data.normal_(
            self.posterior_rho_initial[0], 
            self.posterior_rho_initial[1]
        )
        
        if self.use_bias:
            self.bias_mu.data.normal_(
                self.posterior_mu_initial[0], 
                self.posterior_mu_initial[1]
            )
            self.bias_rho.data.normal_(
                self.posterior_rho_initial[0], 
                self.posterior_rho_initial[1]
            )
    
    def forward(self, x: torch.Tensor, sample: bool = True) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        if sample:
            # é‡‡æ ·æƒé‡
            weight_sigma = torch.log1p(torch.exp(self.weight_rho))
            weight_eps = torch.randn_like(self.weight_mu).to(self.weight_mu.device)
            weight = self.weight_mu + weight_sigma * weight_eps
            
            # é‡‡æ ·åç½®
            if self.use_bias:
                bias_sigma = torch.log1p(torch.exp(self.bias_rho))
                bias_eps = torch.randn_like(self.bias_mu).to(self.bias_mu.device)
                bias = self.bias_mu + bias_sigma * bias_eps
            else:
                bias = None
        else:
            # ä½¿ç”¨å‡å€¼
            weight = self.weight_mu
            bias = self.bias_mu if self.use_bias else None
        
        return F.linear(x, weight, bias)
    
    def kl_divergence(self) -> torch.Tensor:
        """è®¡ç®—KLæ•£åº¦"""
        weight_sigma = torch.log1p(torch.exp(self.weight_rho))
        weight_kl = self._kl_divergence_gaussian(
            self.weight_mu, weight_sigma, self.prior_mu, self.prior_sigma
        )
        
        if self.use_bias:
            bias_sigma = torch.log1p(torch.exp(self.bias_rho))
            bias_kl = self._kl_divergence_gaussian(
                self.bias_mu, bias_sigma, self.prior_mu, self.prior_sigma
            )
            return weight_kl + bias_kl
        else:
            return weight_kl
    
    def _kl_divergence_gaussian(self, mu_q: torch.Tensor, sigma_q: torch.Tensor, 
                               mu_p: float, sigma_p: float) -> torch.Tensor:
        """è®¡ç®—ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒä¹‹é—´çš„KLæ•£åº¦"""
        kl = torch.log(sigma_p / sigma_q) + \
             (sigma_q.pow(2) + (mu_q - mu_p).pow(2)) / (2 * sigma_p**2) - 0.5
        return kl.sum()

# =============================================================================
# æ•°æ®å¤„ç†ç±»
# =============================================================================

class BayesianCNNDataLoaderManager:
    """BayesianCNNæ•°æ®åŠ è½½å™¨ç®¡ç†ç±»"""
    
    def __init__(self, data_config_path: str, config: BayesianCNNConfig):
        self.data_config_path = data_config_path
        self.config = config
        self.data_config = self._load_data_config()
        self.data_split_info = self.data_config.get('data_split', {})
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        
    def _load_data_config(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶"""
        try:
            with open(self.data_config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            if 'phase_1' in config:
                phase_1_config = config['phase_1']
                config['data_split'] = {
                    "workflow_type": "time_series",
                    "train": {
                        "start_date": phase_1_config['train_data']['start_date'],
                        "end_date": phase_1_config['train_data']['end_date'],
                        "samples": phase_1_config['train_data']['samples'],
                        "duration_years": phase_1_config['train_data']['duration_years'],
                        "file": phase_1_config['train_data']['file']
                    },
                    "validation": {
                        "start_date": phase_1_config['valid_data']['start_date'],
                        "end_date": phase_1_config['valid_data']['end_date'],
                        "samples": phase_1_config['valid_data']['samples'],
                        "duration_years": phase_1_config['valid_data']['duration_years'],
                        "file": phase_1_config['valid_data']['file']
                    },
                    "test": {
                        "start_date": phase_1_config['test_data']['start_date'],
                        "end_date": phase_1_config['test_data']['end_date'],
                        "samples": phase_1_config['test_data']['samples'],
                        "duration_years": phase_1_config['test_data']['duration_years'],
                        "file": phase_1_config['test_data']['file']
                    }
                }
                print(f"âœ… ä½¿ç”¨Phase 1é…ç½®ä½œä¸ºé»˜è®¤æ•°æ®åˆ†å‰²")
            
            return config
        except Exception as e:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®é…ç½®æ–‡ä»¶ {self.data_config_path}: {e}")
    
    def get_data_split_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯"""
        return self.data_split_info
    
    def _create_dataloader(self, dataset, shuffle: bool = False, batch_size: int = 32) -> DataLoader:
        """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=4,
            pin_memory=True,
            persistent_workers=True,
            prefetch_factor=2
        )
    
    def _load_real_dataframes(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ä»çœŸå®parquetæ–‡ä»¶åŠ è½½æ•°æ®"""
        try:
            # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
            data_split = self.data_split_info
            
            train_file = data_split['train']['file']
            valid_file = data_split['validation']['file']
            test_file = data_split['test']['file']
            
            # åŠ è½½æ•°æ®æ–‡ä»¶
            train_df = pd.read_parquet(train_file)
            valid_df = pd.read_parquet(valid_file)
            test_df = pd.read_parquet(test_file)
            
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
            print(f"  è®­ç»ƒé›†: {train_df.shape}")
            print(f"  éªŒè¯é›†: {valid_df.shape}")
            print(f"  æµ‹è¯•é›†: {test_df.shape}")
            
            return train_df, valid_df, test_df
            
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½çœŸå®æ•°æ®æ–‡ä»¶: {e}")
            print("ğŸ”„ ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•...")
            return self._generate_synthetic_data()
    
    def _generate_synthetic_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        n_samples = 1000
        n_features = 69
        n_classes = 2
        
        # ç”Ÿæˆç‰¹å¾æ•°æ®
        X = np.random.randn(n_samples, n_features)
        y = np.random.randint(0, n_classes, n_samples)
        
        # åˆ›å»ºç‰¹å¾åˆ—å
        feature_columns = [f"feature_{i}@test" for i in range(n_features)]
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(X, columns=feature_columns)
        df['label'] = y
        
        # åˆ†å‰²æ•°æ®
        train_size = int(0.6 * n_samples)
        valid_size = int(0.2 * n_samples)
        
        train_df = df[:train_size].copy()
        valid_df = df[train_size:train_size+valid_size].copy()
        test_df = df[train_size+valid_size:].copy()
        
        print(f"âœ… æ¨¡æ‹Ÿæ•°æ®ç”ŸæˆæˆåŠŸ:")
        print(f"  è®­ç»ƒé›†: {train_df.shape}")
        print(f"  éªŒè¯é›†: {valid_df.shape}")
        print(f"  æµ‹è¯•é›†: {test_df.shape}")
        
        return train_df, valid_df, test_df
    
    def get_input_dim_from_dataframe(self, df: pd.DataFrame) -> int:
        """ä»DataFrameè·å–ç‰¹å¾ç»´åº¦"""
        # è¯†åˆ«ç‰¹å¾åˆ—ï¼ˆåŒ…å«@ç¬¦å·çš„åˆ—ï¼‰
        feature_cols = [col for col in df.columns if '@' in col]
        if not feature_cols:
            # å¦‚æœæ²¡æœ‰@ç¬¦å·ï¼Œå‡è®¾æœ€åä¸€åˆ—æ˜¯æ ‡ç­¾ï¼Œå…¶ä½™éƒ½æ˜¯ç‰¹å¾
            feature_cols = [col for col in df.columns if col != 'label']
        
        features = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {features}")
        return features
    
    def validate_input_dimensions(self, config: BayesianCNNConfig, actual_input_dim: int) -> int:
        """éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„è¾“å…¥ç»´åº¦ä¸å®é™…æ•°æ®æ˜¯å¦ä¸€è‡´"""
        config_input_dim = getattr(config, 'input_dim', None)
        
        if config_input_dim is None:
            print(f"ğŸ“‹ é…ç½®æ–‡ä»¶ä¸­æœªæŒ‡å®šinput_dimï¼Œä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        if config_input_dim != actual_input_dim:
            print(f"âš ï¸ ç»´åº¦ä¸åŒ¹é…ï¼é…ç½®: {config_input_dim}, å®é™…: {actual_input_dim}")
            print(f"ğŸ”§ ä½¿ç”¨å®é™…æ•°æ®ç»´åº¦: {actual_input_dim}")
            config.input_dim = actual_input_dim
            return actual_input_dim
        
        print(f"âœ… è¾“å…¥ç»´åº¦éªŒè¯é€šè¿‡: {actual_input_dim}")
        return actual_input_dim
    
    def load_data_loaders(self, batch_size: int = 32) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """åŠ è½½æ•°æ®åŠ è½½å™¨"""
        # åŠ è½½çœŸå®æ•°æ®
        train_df, valid_df, test_df = self._load_real_dataframes()
        
        # åŠ¨æ€æ£€æµ‹ç‰¹å¾ç»´åº¦
        feature_cols = [col for col in train_df.columns if '@' in col]
        if not feature_cols:
            feature_cols = [col for col in train_df.columns if col != 'label']
        
        input_dim = len(feature_cols)
        print(f"ğŸ” æ£€æµ‹åˆ° {input_dim} ä¸ªç‰¹å¾")
        
        # éªŒè¯å¹¶æ›´æ–°é…ç½®
        self.validate_input_dimensions(self.config, input_dim)
        
        # å‡†å¤‡æ•°æ®
        X_train = train_df[feature_cols].values
        y_train = train_df['label'].values if 'label' in train_df.columns else train_df.iloc[:, -1].values
        
        X_valid = valid_df[feature_cols].values
        y_valid = valid_df['label'].values if 'label' in valid_df.columns else valid_df.iloc[:, -1].values
        
        X_test = test_df[feature_cols].values
        y_test = test_df['label'].values if 'label' in test_df.columns else test_df.iloc[:, -1].values
        
        # æ•°æ®é¢„å¤„ç†
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_valid_scaled = self.scaler.transform(X_valid)
        X_test_scaled = self.scaler.transform(X_test)
        
        # æ ‡ç­¾ç¼–ç 
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        y_valid_encoded = self.label_encoder.transform(y_valid)
        y_test_encoded = self.label_encoder.transform(y_test)
        
        # æ›´æ–°è¾“å‡ºç»´åº¦
        self.config.output_dim = len(np.unique(y_train_encoded))
        
        # è½¬æ¢ä¸ºå¼ é‡
        train_dataset = TensorDataset(
            torch.FloatTensor(X_train_scaled),
            torch.LongTensor(y_train_encoded)
        )
        valid_dataset = TensorDataset(
            torch.FloatTensor(X_valid_scaled),
            torch.LongTensor(y_valid_encoded)
        )
        test_dataset = TensorDataset(
            torch.FloatTensor(X_test_scaled),
            torch.LongTensor(y_test_encoded)
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = self._create_dataloader(train_dataset, shuffle=True, batch_size=batch_size)
        valid_loader = self._create_dataloader(valid_dataset, shuffle=False, batch_size=batch_size)
        test_loader = self._create_dataloader(test_dataset, shuffle=False, batch_size=batch_size)
        
        return train_loader, valid_loader, test_loader
    
    def get_data_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®ç›¸å…³ä¿¡æ¯"""
        return {
            "workflow_type": self.data_split_info.get("workflow_type", "unknown"),
            "train_period": self.data_split_info.get("train", {}),
            "validation_period": self.data_split_info.get("validation", {}),
            "test_period": self.data_split_info.get("test", {}),
            "data_config": self.data_config
        }

# =============================================================================
# æ¨¡å‹å®ç°
# =============================================================================

class BaseModel(nn.Module, ABC):
    """æ¨¡å‹åŸºç±»"""
    
    def __init__(self, config: BayesianCNNConfig):
        super().__init__()
        self.config = config
        
    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ - å­ç±»å¿…é¡»å®ç°"""
        pass
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            "model_name": self.config.model_name,
            "total_parameters": total_params,
            "trainable_parameters": trainable_params,
            "model_size_mb": total_params * 4 / (1024 * 1024)
        }

class BayesianCNNModel(BaseModel):
    """BayesianCNNæ¨¡å‹å®ç°"""
    
    def __init__(self, config: BayesianCNNConfig):
        super().__init__(config)
        
        # è·å–ç½‘ç»œå‚æ•°
        self.input_dim = getattr(config, 'input_dim', None)
        self.hidden_dims = config.hidden_dims
        self.output_dim = getattr(config, 'output_dim', None)
        self.dropout = config.dropout
        self.activation = config.activation
        
        # è´å¶æ–¯é…ç½®
        self.prior_mu = config.prior_mu
        self.prior_sigma = config.prior_sigma
        self.posterior_mu_initial = config.posterior_mu_initial
        self.posterior_rho_initial = config.posterior_rho_initial
        
        # å»¶è¿Ÿåˆå§‹åŒ–ç½‘ç»œå±‚
        self.layers = None
        self.dropout_layers = None
        
        # æ¿€æ´»å‡½æ•°
        self.activation_fn = self._get_activation_function(self.activation)
        
        # å¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦å·²çŸ¥ï¼Œç«‹å³æ„å»ºç½‘ç»œ
        if self.input_dim is not None and self.output_dim is not None:
            self._build_layers()
    
    def _get_activation_function(self, activation: str) -> nn.Module:
        """è·å–æ¿€æ´»å‡½æ•°"""
        activation_map = {
            'relu': nn.ReLU(),
            'tanh': nn.Tanh(),
            'sigmoid': nn.Sigmoid(),
            'leaky_relu': nn.LeakyReLU(),
            'elu': nn.ELU(),
            'gelu': nn.GELU()
        }
        
        if activation.lower() not in activation_map:
            print(f"âš ï¸ æœªçŸ¥çš„æ¿€æ´»å‡½æ•°: {activation}ï¼Œä½¿ç”¨ReLU")
            return nn.ReLU()
        
        return activation_map[activation.lower()]
    
    def _build_layers(self):
        """æ„å»ºç½‘ç»œå±‚"""
        if self.input_dim is None or self.output_dim is None:
            print("âš ï¸ è¾“å…¥æˆ–è¾“å‡ºç»´åº¦æœªæŒ‡å®šï¼Œæ— æ³•æ„å»ºç½‘ç»œ")
            return
        
        print(f"ğŸ”§ æ„å»ºç½‘ç»œå±‚ï¼Œè¾“å…¥ç»´åº¦: {self.input_dim}, è¾“å‡ºç»´åº¦: {self.output_dim}")
        
        self.layers = nn.ModuleList()
        self.dropout_layers = nn.ModuleList()
        
        # è¾“å…¥å±‚åˆ°ç¬¬ä¸€ä¸ªéšè—å±‚
        self.layers.append(
            BBBLinear(
                self.input_dim, 
                self.hidden_dims[0],
                prior_mu=self.prior_mu,
                prior_sigma=self.prior_sigma,
                posterior_mu_initial=self.posterior_mu_initial,
                posterior_rho_initial=self.posterior_rho_initial
            )
        )
        self.dropout_layers.append(nn.Dropout(self.dropout))
        
        # éšè—å±‚
        for i in range(len(self.hidden_dims) - 1):
            self.layers.append(
                BBBLinear(
                    self.hidden_dims[i], 
                    self.hidden_dims[i + 1],
                    prior_mu=self.prior_mu,
                    prior_sigma=self.prior_sigma,
                    posterior_mu_initial=self.posterior_mu_initial,
                    posterior_rho_initial=self.posterior_rho_initial
                )
            )
            self.dropout_layers.append(nn.Dropout(self.dropout))
        
        # è¾“å‡ºå±‚
        self.layers.append(
            BBBLinear(
                self.hidden_dims[-1], 
                self.output_dim,
                prior_mu=self.prior_mu,
                prior_sigma=self.prior_sigma,
                posterior_mu_initial=self.posterior_mu_initial,
                posterior_rho_initial=self.posterior_rho_initial
            )
        )
        
        # ç¡®ä¿æ–°æ„å»ºçš„å±‚åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self, '_device'):
            self.layers.to(self._device)
            self.dropout_layers.to(self._device)
            print(f"ğŸ”§ æ–°æ„å»ºçš„å±‚å·²ç§»åŠ¨åˆ°è®¾å¤‡: {self._device}")
    
    def forward(self, x: torch.Tensor, sample: bool = True) -> torch.Tensor:
        """å‰å‘ä¼ æ’­"""
        # é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶è‡ªåŠ¨æ£€æµ‹å¹¶æ„å»ºç½‘ç»œ
        if self.layers is None:
            if self.input_dim is None:
                self.input_dim = x.shape[-1]
                self.config.input_dim = self.input_dim
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹è¾“å…¥ç»´åº¦: {self.input_dim}")
            
            if self.output_dim is None:
                self.output_dim = 2  # é»˜è®¤äºŒåˆ†ç±»
                self.config.output_dim = self.output_dim
                print(f"ğŸ” é»˜è®¤è¾“å‡ºç»´åº¦: {self.output_dim}")
            
            self._build_layers()
        
        # å‰å‘ä¼ æ’­
        for i, layer in enumerate(self.layers[:-1]):
            x = layer(x, sample=sample)
            x = self.activation_fn(x)
            if i < len(self.dropout_layers):
                x = self.dropout_layers[i](x)
        
        # è¾“å‡ºå±‚ï¼ˆä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°å’Œdropoutï¼‰
        if len(self.layers) > 0:
            x = self.layers[-1](x, sample=sample)
        
        return x
    
    def kl_divergence(self) -> torch.Tensor:
        """è®¡ç®—æ‰€æœ‰è´å¶æ–¯å±‚çš„æ€»KLæ•£åº¦"""
        if self.layers is None:
            return torch.tensor(0.0)
        
        kl_sum = torch.tensor(0.0, device=next(self.parameters()).device)
        
        for layer in self.layers:
            if isinstance(layer, BBBLinear):
                kl_sum += layer.kl_divergence()
        
        return kl_sum
    
    def predict_with_uncertainty(self, x: torch.Tensor, num_samples: int = 100) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä½¿ç”¨è’™ç‰¹å¡æ´›é‡‡æ ·è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–é¢„æµ‹"""
        self.eval()
        
        predictions = []
        with torch.no_grad():
            for _ in range(num_samples):
                pred = self.forward(x, sample=True)
                predictions.append(pred)
        
        predictions = torch.stack(predictions)
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
        pred_mean = predictions.mean(dim=0)
        pred_std = predictions.std(dim=0)
        
        return pred_mean, pred_std

# =============================================================================
# æŸå¤±å‡½æ•°
# =============================================================================

class ELBOLoss(nn.Module):
    """Evidence Lower Bound (ELBO) æŸå¤±å‡½æ•°"""
    
    def __init__(self, train_size: int):
        super().__init__()
        self.train_size = train_size
        self.nll_loss = nn.CrossEntropyLoss()
    
    def forward(self, outputs: torch.Tensor, targets: torch.Tensor, 
                kl_divergence: torch.Tensor, beta: float) -> torch.Tensor:
        """è®¡ç®—ELBOæŸå¤±"""
        nll = self.nll_loss(outputs, targets)
        kl_scaled = kl_divergence / self.train_size
        return nll + beta * kl_scaled

def logmeanexp(x: torch.Tensor, dim: int) -> torch.Tensor:
    """ç¨³å®šçš„log-mean-expè®¡ç®—"""
    return torch.logsumexp(x, dim=dim) - np.log(x.shape[dim])

# =============================================================================
# è®­ç»ƒå™¨ç±»
# =============================================================================

class UnifiedTrainer:
    """ç»Ÿä¸€è®­ç»ƒå™¨"""
    
    def __init__(self, config: BayesianCNNConfig, model: BayesianCNNModel, data_loader_manager: BayesianCNNDataLoaderManager,
                 checkpoint_dir: str = "checkpoints"):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.checkpoint_dir = checkpoint_dir  # Checkpointä¿å­˜ç›®å½•
        
        # è®¾å¤‡è®¾ç½®
        self.device = setup_device(getattr(config, 'device', 'auto'))
        self.rank = 0
        self.world_size = 1
        
        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logging(config.logs_dir, "training")
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = None
        self.scheduler = None
        self.criterion = None
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        self.scaler = GradScaler() if config.use_mixed_precision and torch.cuda.is_available() else None
        self.use_amp = config.use_mixed_precision and torch.cuda.is_available()
        
        # è®­ç»ƒçŠ¶æ€
        self.best_val_score = 0.0
        self.train_history = []
        
        # Checkpointè·Ÿè¸ªå™¨ï¼ˆæ”¯æŒå¤šåŒºé—´æœ€ä½³ï¼‰
        self.checkpoint_tracker = {
            'global_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'early_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'mid_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None},
            'late_best': {'epoch': -1, 'val_loss': float('inf'), 'path': None}
        }
        
        # è®¾ç½®éšæœºç§å­
        set_all_seeds(config.seed)
        
        # åˆ›å»ºç›®å½•
        create_directories(config)
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡å¹¶è®°å½•GPUä¿¡æ¯
        self.model.to(self.device)
        self._log_device_info()
        
        # è®°å½•æ€§èƒ½ä¼˜åŒ–è®¾ç½®
        if self.use_amp:
            self.logger.info("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        
        # è®°å½•æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_info = self.data_loader_manager.get_data_info()
        self.logger.info(f"æ•°æ®åˆ†å‰²ä¿¡æ¯: {data_info['workflow_type']}")
    
    def _log_device_info(self):
        """è®°å½•è®¾å¤‡å’ŒGPUä½¿ç”¨ä¿¡æ¯"""
        if self.device.type == 'cuda':
            gpu_name = torch.cuda.get_device_name()
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            current_memory = torch.cuda.memory_allocated() / 1024**3
            
            self.logger.info(f"ğŸš€ ä½¿ç”¨GPUè®­ç»ƒ: {gpu_name}")
            self.logger.info(f"ğŸ“Š GPUæ€»å†…å­˜: {gpu_memory:.1f}GB")
            self.logger.info(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {current_memory:.2f}GB")
            self.logger.info(f"ğŸ“Š å¯ç”¨å†…å­˜: {gpu_memory - current_memory:.2f}GB")
        else:
            self.logger.info("ğŸ–¥ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            
        # è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        self.logger.info(f"ğŸ“Š æ¨¡å‹æ€»å‚æ•°: {total_params:,}")
        self.logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        self.logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f}MB")
    
    def _initialize_model_layers(self, train_loader: DataLoader):
        """é€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­åˆå§‹åŒ–æ¨¡å‹ç½‘ç»œå±‚"""
        if self.model.layers is None:
            # è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ¥åˆå§‹åŒ–æ¨¡å‹
            for data, target in train_loader:
                data = data.to(self.device, non_blocking=True)
                target = target.to(self.device, non_blocking=True)
                
                # è®¾ç½®æ¨¡å‹çš„è®¾å¤‡ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨æ„å»ºå±‚æ—¶ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
                self.model._device = self.device
                
                # æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æ¥æ„å»ºç½‘ç»œå±‚
                with torch.no_grad():
                    _ = self.model(data, sample=False)
                
                # ç¡®ä¿æ¨¡å‹å®Œå…¨ç§»åŠ¨åˆ°è®¾å¤‡
                self.model.to(self.device)
                print(f"ğŸ”§ æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
                
                # æ›´æ–°è¾“å‡ºç»´åº¦
                if self.model.output_dim is None or self.model.output_dim == 2:
                    unique_labels = torch.unique(target)
                    actual_output_dim = len(unique_labels)
                    if actual_output_dim != self.model.output_dim:
                        self.model.output_dim = actual_output_dim
                        self.config.output_dim = actual_output_dim
                        print(f"ğŸ” æ›´æ–°è¾“å‡ºç»´åº¦ä¸º: {actual_output_dim}")
                        # é‡æ–°æ„å»ºç½‘ç»œå±‚
                        self.model._build_layers()
                        # å†æ¬¡ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                        self.model.to(self.device)
                
                break  # åªéœ€è¦ä¸€ä¸ªæ‰¹æ¬¡æ¥åˆå§‹åŒ–
            
            # é‡æ–°è®°å½•æ¨¡å‹å‚æ•°ä¿¡æ¯
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            self.logger.info(f"ğŸ“Š æ¨¡å‹åˆå§‹åŒ–åæ€»å‚æ•°: {total_params:,}")
            self.logger.info(f"ğŸ“Š æ¨¡å‹åˆå§‹åŒ–åå¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
            self.logger.info(f"ğŸ“Š æ¨¡å‹åˆå§‹åŒ–åå¤§å°: {total_params * 4 / 1024**2:.1f}MB")
    
    def setup_training_components(self, train_size: int):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=self.config.learning_rate
        )
        
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', patience=5, factor=0.5
        )
        
        self.criterion = ELBOLoss(train_size).to(self.device)
    
    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        total_kl = 0.0
        all_preds = []
        all_targets = []
        
        num_ens = self.config.train_samples
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
            
            self.optimizer.zero_grad()
            
            # é›†æˆé‡‡æ ·
            if num_ens > 1:
                outputs_list = []
                kl_total = 0.0
                
                for j in range(num_ens):
                    net_out = self.model(data, sample=True)
                    kl = self.model.kl_divergence()
                    kl_total += kl
                    outputs_list.append(F.log_softmax(net_out, dim=1))
                
                outputs = torch.stack(outputs_list, dim=2)
                log_outputs = logmeanexp(outputs, dim=2)
                kl_avg = kl_total / num_ens
            else:
                # å•æ¬¡é‡‡æ ·
                outputs = self.model(data, sample=True)
                log_outputs = F.log_softmax(outputs, dim=1)
                kl_avg = self.model.kl_divergence()
            
            # æ··åˆç²¾åº¦è®­ç»ƒ
            if self.use_amp:
                with autocast():
                    loss = self.criterion(torch.exp(log_outputs), target, kl_avg, self.config.kl_weight)
                
                self.scaler.scale(loss).backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                loss = self.criterion(torch.exp(log_outputs), target, kl_avg, self.config.kl_weight)
                loss.backward()
                
                if hasattr(self.config, 'gradient_clip_value'):
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_value)
                
                self.optimizer.step()
            
            total_loss += loss.item()
            total_kl += kl_avg.item()
            
            # è®°å½•é¢„æµ‹
            preds = torch.argmax(log_outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
        
        avg_loss = total_loss / len(train_loader)
        avg_kl = total_kl / len(train_loader)
        accuracy = accuracy_score(all_targets, all_preds)
        
        return {"loss": avg_loss, "accuracy": accuracy, "kl_divergence": avg_kl}
    
    def validate_epoch(self, val_loader: DataLoader) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_targets = []
        all_probs = []
        
        num_ens = self.config.val_samples
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)
                
                # é›†æˆé¢„æµ‹
                if num_ens > 1:
                    outputs_list = []
                    kl_total = 0.0
                    
                    for j in range(num_ens):
                        net_out = self.model(data, sample=True)
                        kl = self.model.kl_divergence()
                        kl_total += kl
                        outputs_list.append(F.log_softmax(net_out, dim=1))
                    
                    outputs = torch.stack(outputs_list, dim=2)
                    log_outputs = logmeanexp(outputs, dim=2)
                    kl_avg = kl_total / num_ens
                else:
                    outputs = self.model(data, sample=True)
                    log_outputs = F.log_softmax(outputs, dim=1)
                    kl_avg = self.model.kl_divergence()
                
                # æ··åˆç²¾åº¦æ¨ç†
                if self.use_amp:
                    with autocast():
                        loss = self.criterion(torch.exp(log_outputs), target, kl_avg, self.config.kl_weight)
                else:
                    loss = self.criterion(torch.exp(log_outputs), target, kl_avg, self.config.kl_weight)
                
                total_loss += loss.item()
                
                # è®°å½•é¢„æµ‹
                probs = torch.exp(log_outputs)
                preds = torch.argmax(log_outputs, dim=1)
                
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        
        metrics = {"loss": total_loss / len(val_loader)}
        metrics["accuracy"] = accuracy_score(all_targets, all_preds)
        metrics["f1"] = f1_score(all_targets, all_preds, average='weighted')
        
        # è®¡ç®—AUC
        if len(np.unique(all_targets)) == 2:
            try:
                all_probs_array = np.array(all_probs)
                if all_probs_array.ndim == 2 and all_probs_array.shape[1] >= 2:
                    metrics["auc"] = roc_auc_score(all_targets, all_probs_array[:, 1])
            except Exception:
                pass
        
        return metrics
    
    def train(self, batch_size: int = None, no_save_model: bool = False) -> Dict[str, Any]:
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        # è·å–æ•°æ®åŠ è½½å™¨
        batch_size = batch_size or self.config.batch_size
        train_loader, val_loader, test_loader = self.data_loader_manager.load_data_loaders(batch_size=batch_size)
        
        # åˆå§‹åŒ–æ¨¡å‹ç½‘ç»œå±‚ï¼ˆé€šè¿‡ä¸€æ¬¡å‰å‘ä¼ æ’­ï¼‰
        self._initialize_model_layers(train_loader)
        
        self.setup_training_components(len(train_loader.dataset))
        
        self.logger.info("å¼€å§‹è®­ç»ƒ...")
        self.logger.info(f"æ¨¡å‹ä¿¡æ¯: {self.model.get_model_info()}")
        
        start_time = time.time()
        epochs = self.config.epochs
        
        for epoch in range(epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_metrics = self.validate_epoch(val_loader)
            
            # è®°å½•å†å²
            epoch_info = {
                "epoch": epoch + 1,
                "train_loss": train_metrics["loss"],
                "val_loss": val_metrics["loss"],
                "train_acc": train_metrics.get("accuracy", 0),
                "val_acc": val_metrics.get("accuracy", 0),
                "train_auc": train_metrics.get("auc", 0),
                "val_auc": val_metrics.get("auc", 0),
                "lr": self.optimizer.param_groups[0]['lr'],
                "time": time.time() - epoch_start
            }
            
            current_score = val_metrics.get("accuracy", 0)
            self.train_history.append(epoch_info)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(current_score)
            
            # ä¿å­˜checkpointï¼ˆä½¿ç”¨æ–°çš„æ™ºèƒ½ç­–ç•¥ï¼‰
            if not no_save_model:
                val_loss = epoch_info.get('val_loss', float('inf'))
                self.save_checkpoint_if_best(epoch, val_loss)
            
            # æ›´æ–°best_val_scoreï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
            if current_score > self.best_val_score:
                self.best_val_score = current_score
            
            # GPUå†…å­˜ç›‘æ§
            if self.device.type == 'cuda' and (epoch + 1) % 10 == 0:
                current_memory = torch.cuda.memory_allocated() / 1024**3
                max_memory = torch.cuda.max_memory_allocated() / 1024**3
                cached_memory = torch.cuda.memory_reserved() / 1024**3
                self.logger.info(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨ (Epoch {epoch+1}): å½“å‰ {current_memory:.2f}GB, å³°å€¼ {max_memory:.2f}GB, ç¼“å­˜ {cached_memory:.2f}GB")
            
            # æ—¥å¿—è¾“å‡º
            log_msg = (
                f"Epoch {epoch+1}/{epochs} - "
                f"Train Loss: {train_metrics['loss']:.4f}, "
                f"Val Loss: {val_metrics['loss']:.4f}, "
                f"Val Acc: {val_metrics.get('accuracy', 0):.4f}"
            )
            
            if 'auc' in val_metrics:
                log_msg += f", Val AUC: {val_metrics['auc']:.4f}"
            
            log_msg += f", Time: {epoch_info['time']:.2f}s"
            self.logger.info(log_msg)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        return {
            "best_val_score": self.best_val_score,
            "total_epochs": len(self.train_history),
            "total_time": total_time,
            "train_history": self.train_history
        }
    
    def save_checkpoint_if_best(self, epoch: int, val_loss: float):
        """æ™ºèƒ½checkpointä¿å­˜ï¼šå…¨å±€æœ€ä½³ + åŒºé—´æœ€ä½³"""
        # æ›´æ–°å…¨å±€æœ€ä½³
        if val_loss < self.checkpoint_tracker['global_best']['val_loss']:
            self._save_checkpoint(epoch, val_loss, 'global_best')
        
        # æ›´æ–°åŒºé—´æœ€ä½³ï¼ˆå®æ—¶æ›´æ–°ï¼‰
        if 0 <= epoch < 30:
            if val_loss < self.checkpoint_tracker['early_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'early_best')
        elif 30 <= epoch < 60:
            if val_loss < self.checkpoint_tracker['mid_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'mid_best')
        elif 60 <= epoch < 100:
            if val_loss < self.checkpoint_tracker['late_best']['val_loss']:
                self._save_checkpoint(epoch, val_loss, 'late_best')
        
        # ğŸ’¡ å…³é”®ï¼šåœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³ä¸ºinterval_best
        if epoch == 29:  # epochä»0å¼€å§‹ï¼Œ29æ˜¯ç¬¬30ä¸ªepoch
            self._save_interval_best('early_best', '00-30')
        elif epoch == 59:
            self._save_interval_best('mid_best', '30-60')
        elif epoch == 99:
            self._save_interval_best('late_best', '60-100')
    
    def _save_checkpoint(self, epoch: int, val_loss: float, checkpoint_type: str):
        """ä¿å­˜å•ä¸ªcheckpoint"""
        import os
        
        # åˆ é™¤æ—§checkpoint
        old_path = self.checkpoint_tracker[checkpoint_type]['path']
        if old_path and os.path.exists(old_path):
            os.remove(old_path)
        
        # ä¿å­˜æ–°checkpoint
        checkpoint_name = f"checkpoint_{checkpoint_type}_epoch{epoch:03d}_val{val_loss:.4f}.pt"
        checkpoint_path = os.path.join(self.checkpoint_dir, checkpoint_name)
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'checkpoint_type': checkpoint_type,
            'config': asdict(self.config)
        }
        
        torch.save(checkpoint, checkpoint_path)
        
        self.checkpoint_tracker[checkpoint_type] = {
            'epoch': epoch,
            'val_loss': val_loss,
            'path': checkpoint_path
        }
        
        self.logger.info(f"ğŸ’¾ ä¿å­˜{checkpoint_type} checkpoint: epoch {epoch}, val_loss {val_loss:.4f}")
    
    def _save_interval_best(self, checkpoint_type: str, epoch_range: str):
        """åœ¨åŒºé—´ç»“æŸæ—¶ï¼Œå¤åˆ¶åŒºé—´æœ€ä½³checkpointä¸ºinterval_best"""
        import shutil
        import os
        
        # è·å–å½“å‰åŒºé—´æœ€ä½³çš„checkpointè·¯å¾„
        source_path = self.checkpoint_tracker[checkpoint_type]['path']
        
        if source_path and os.path.exists(source_path):
            # åˆ›å»ºinterval_bestæ–‡ä»¶å
            interval_name = f"interval_best_{epoch_range}.pt"
            interval_path = os.path.join(self.checkpoint_dir, interval_name)
            
            # å¤åˆ¶checkpoint
            shutil.copy2(source_path, interval_path)
            
            epoch = self.checkpoint_tracker[checkpoint_type]['epoch']
            val_loss = self.checkpoint_tracker[checkpoint_type]['val_loss']
            
            self.logger.info(f"ğŸ“¦ å¤åˆ¶åŒºé—´æœ€ä½³ [{epoch_range}]: epoch {epoch}, val_loss {val_loss:.4f}")
            self.logger.info(f"   æºæ–‡ä»¶: {os.path.basename(source_path)}")
            self.logger.info(f"   ç›®æ ‡æ–‡ä»¶: {interval_name}")
        else:
            self.logger.warning(f"âš ï¸ åŒºé—´ [{epoch_range}] æ²¡æœ‰æ‰¾åˆ°æœ€ä½³checkpoint")
    
    def save_checkpoint(self, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆå‘åå…¼å®¹çš„æ—§æ–¹æ³•ï¼‰"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_score': self.best_val_score,
            'config': asdict(self.config)
        }
        
        # ä½¿ç”¨ä¼ å…¥çš„checkpointç›®å½•ï¼ˆä¼˜å…ˆï¼‰ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®ä¸­çš„
        checkpoint_dir = Path(self.checkpoint_dir) if self.checkpoint_dir else Path(self.config.checkpoint_dir)
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        checkpoint_path = checkpoint_dir / "latest_checkpoint.pth"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = checkpoint_dir / "best_model.pth"
            torch.save(checkpoint, best_path)
            self.logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°: {best_path}")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = Path(self.config.results_dir) / "training_history.json"
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        self.logger.info(f"è®­ç»ƒå†å²ä¿å­˜åˆ°: {history_path}")

# =============================================================================
# æ¨ç†å™¨ç±»
# =============================================================================

class UnifiedInferencer:
    """ç»Ÿä¸€æ¨ç†å™¨"""
    
    def __init__(self, config: BayesianCNNConfig, model: BayesianCNNModel, data_loader_manager: BayesianCNNDataLoaderManager):
        self.config = config
        self.model = model
        self.data_loader_manager = data_loader_manager
        self.device = setup_device()
        self.logger = setup_logging(config.logs_dir, "inference")
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model.to(self.device)
        self.model.eval()
    
    def load_checkpoint(self, checkpoint_path: str):
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»æ„å»ºç½‘ç»œå±‚
        if self.model.layers is None:
            # ä»checkpointä¸­è·å–é…ç½®ä¿¡æ¯æ¥æ„å»ºç½‘ç»œå±‚
            if 'config' in checkpoint:
                saved_config = checkpoint['config']
                if 'input_dim' in saved_config and 'output_dim' in saved_config:
                    self.model.input_dim = saved_config['input_dim']
                    self.model.output_dim = saved_config['output_dim']
                    self.model._device = self.device  # è®¾ç½®è®¾å¤‡ä¿¡æ¯
                    self.model._build_layers()
                    self.model.to(self.device)  # ç¡®ä¿æ–°æ„å»ºçš„å±‚åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                    self.logger.info(f"æ ¹æ®checkpointé…ç½®æ„å»ºç½‘ç»œå±‚: input_dim={self.model.input_dim}, output_dim={self.model.output_dim}")
                else:
                    self.logger.warning("checkpointä¸­ç¼ºå°‘input_dimæˆ–output_dimé…ç½®")
            else:
                # å°è¯•ä»state_dictæ¨æ–­ç½‘ç»œç»“æ„
                state_dict = checkpoint['model_state_dict']
                layer_keys = [k for k in state_dict.keys() if k.startswith('layers.')]
                if layer_keys:
                    # ä»ç¬¬ä¸€å±‚æ¨æ–­input_dim
                    first_layer_weight_key = 'layers.0.weight_mu'
                    if first_layer_weight_key in state_dict:
                        input_dim = state_dict[first_layer_weight_key].shape[1]
                        self.model.input_dim = input_dim
                        self.logger.info(f"ä»checkpointæ¨æ–­input_dim: {input_dim}")
                    
                    # ä»æœ€åä¸€å±‚æ¨æ–­output_dim
                    last_layer_idx = max([int(k.split('.')[1]) for k in layer_keys if '.' in k and k.split('.')[1].isdigit()])
                    last_layer_weight_key = f'layers.{last_layer_idx}.weight_mu'
                    if last_layer_weight_key in state_dict:
                        output_dim = state_dict[last_layer_weight_key].shape[0]
                        self.model.output_dim = output_dim
                        self.logger.info(f"ä»checkpointæ¨æ–­output_dim: {output_dim}")
                    
                    # æ„å»ºç½‘ç»œå±‚
                    if hasattr(self.model, 'input_dim') and hasattr(self.model, 'output_dim'):
                        self.model._device = self.device  # è®¾ç½®è®¾å¤‡ä¿¡æ¯
                        self.model._build_layers()
                        self.model.to(self.device)  # ç¡®ä¿æ–°æ„å»ºçš„å±‚åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                        self.logger.info("æ ¹æ®checkpoint state_dictæ„å»ºç½‘ç»œå±‚")
        
        # åŠ è½½çŠ¶æ€å­—å…¸
        try:
            self.model.load_state_dict(checkpoint['model_state_dict'])
            # ç¡®ä¿æ¨¡å‹å®Œå…¨åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            self.model.to(self.device)
            self.logger.info(f"æˆåŠŸåŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")
            self.logger.info(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
        except RuntimeError as e:
            self.logger.error(f"åŠ è½½checkpointå¤±è´¥: {e}")
            raise
        
        return checkpoint
    
    def predict_batch(self, data_loader: DataLoader) -> Tuple[np.ndarray, np.ndarray]:
        """æ‰¹é‡æ¨ç†"""
        all_preds = []
        all_probs = []
        
        with torch.no_grad():
            for data, _ in data_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
        
        return np.array(all_preds), np.array(all_probs)
    
    def predict_single(self, data: np.ndarray) -> Tuple[Union[int, float], np.ndarray]:
        """å•æ ·æœ¬æ¨ç†"""
        data_tensor = torch.FloatTensor(data).unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            output = self.model(data_tensor)
            probs = torch.softmax(output, dim=1)
            pred = torch.argmax(output, dim=1).item()
            return pred, probs.cpu().numpy()[0]
    
    def evaluate(self) -> Dict[str, float]:
        """æ¨¡å‹è¯„ä¼°"""
        # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
        _, _, test_loader = self.data_loader_manager.load_data_loaders()
        
        all_preds = []
        all_targets = []
        all_probs = []
        
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = self.model(data)
                
                probs = torch.softmax(outputs, dim=1)
                preds = torch.argmax(outputs, dim=1)
                all_probs.extend(probs.cpu().numpy())
                all_preds.extend(preds.cpu().numpy())
                all_targets.extend(target.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            "accuracy": accuracy_score(all_targets, all_preds),
            "precision": precision_score(all_targets, all_preds, average='weighted'),
            "recall": recall_score(all_targets, all_preds, average='weighted'),
            "f1": f1_score(all_targets, all_preds, average='weighted')
        }
        
        # AUCæŒ‡æ ‡è®¡ç®—
        if len(np.unique(all_targets)) == 2:
            all_probs_positive = np.array(all_probs)[:, 1]
            metrics["auc"] = roc_auc_score(all_targets, all_probs_positive)
        
        return metrics
    
    def generate_predictions(self, start_date: str = None, end_date: str = None, 
                           output_path: str = None, output_format: str = "parquet") -> pd.DataFrame:
        """ç”Ÿæˆé¢„æµ‹æ•°æ®æ–‡ä»¶ç”¨äºå›æµ‹"""
        import pandas as pd
        from datetime import datetime, timedelta
        
        # è·å–æ•°æ®åˆ†å‰²ä¿¡æ¯
        data_split_info = self.data_loader_manager.get_data_split_info()
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªæ•°æ®é›†
        if start_date and end_date:
            # ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®
            print(f"  ğŸ¯ ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
            
            # æ£€æŸ¥æ—¥æœŸèŒƒå›´å±äºå“ªä¸ªæ•°æ®é›†
            test_start = data_split_info.get('test', {}).get('start_date')
            test_end = data_split_info.get('test', {}).get('end_date')
            
            if test_start and test_end and start_date >= test_start and end_date <= test_end:
                print(f"  ğŸ“Š ä½¿ç”¨æµ‹è¯•é›†æ•°æ®")
                _, _, data_loader = self.data_loader_manager.load_data_loaders()
            else:
                print(f"  âš ï¸ æŒ‡å®šæ—¥æœŸèŒƒå›´ä¸åœ¨æµ‹è¯•é›†å†…ï¼Œä½¿ç”¨æµ‹è¯•é›†æ•°æ®")
                _, _, data_loader = self.data_loader_manager.load_data_loaders()
        else:
            # é»˜è®¤ä½¿ç”¨æµ‹è¯•é›†
            print(f"  ğŸ“Š ä½¿ç”¨æµ‹è¯•é›†æ•°æ®")
            _, _, data_loader = self.data_loader_manager.load_data_loaders()
            
            # ä½¿ç”¨æµ‹è¯•é›†çš„æ—¥æœŸèŒƒå›´
            test_info = data_split_info.get('test', {})
            start_date = test_info.get('start_date', '2020-07-01')
            end_date = test_info.get('end_date', '2020-12-31')
        
        # æ‰§è¡Œé¢„æµ‹
        predictions, probabilities = self.predict_batch(data_loader)
        
        # ç”Ÿæˆæ—¥æœŸåºåˆ—
        date_range = pd.date_range(start=start_date, end=end_date, freq='D')
        
        # å¦‚æœé¢„æµ‹æ•°é‡ä¸æ—¥æœŸæ•°é‡ä¸åŒ¹é…ï¼Œè°ƒæ•´æ—¥æœŸèŒƒå›´
        if len(predictions) != len(date_range):
            print(f"  ğŸ”§ è°ƒæ•´æ—¥æœŸèŒƒå›´ï¼šé¢„æµ‹æ•°é‡={len(predictions)}, æ—¥æœŸæ•°é‡={len(date_range)}")
            if len(predictions) < len(date_range):
                date_range = date_range[:len(predictions)]
            else:
                # æ‰©å±•æ—¥æœŸèŒƒå›´
                additional_days = len(predictions) - len(date_range)
                end_date_dt = pd.to_datetime(end_date)
                extended_dates = pd.date_range(
                    start=end_date_dt + timedelta(days=1),
                    periods=additional_days,
                    freq='D'
                )
                date_range = date_range.append(extended_dates)
        
        # åˆ›å»ºé¢„æµ‹æ•°æ®DataFrame
        predictions_df = pd.DataFrame({
            'date': date_range[:len(predictions)],
            'prediction': predictions,
            'confidence': probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities[:, 0],
            'probability_class_0': probabilities[:, 0] if probabilities.shape[1] > 1 else 1 - probabilities[:, 0],
            'probability_class_1': probabilities[:, 1] if probabilities.shape[1] > 1 else probabilities[:, 0]
        })
        
        # æ·»åŠ å…ƒæ•°æ®
        predictions_df['model_name'] = self.config.model_name
        predictions_df['timestamp'] = datetime.now().isoformat()
        
        # ä¿å­˜æ–‡ä»¶
        if output_path:
            if output_format == "parquet":
                predictions_df.to_parquet(output_path, index=False)
            elif output_format == "csv":
                predictions_df.to_csv(output_path, index=False)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {output_format}")
            
            print(f"  ğŸ’¾ é¢„æµ‹æ•°æ®å·²ä¿å­˜: {output_path}")
            print(f"  ğŸ“Š æ•°æ®å½¢çŠ¶: {predictions_df.shape}")
            print(f"  ğŸ“… æ—¥æœŸèŒƒå›´: {predictions_df['date'].min()} åˆ° {predictions_df['date'].max()}")
        
        return predictions_df
    
    def save_predictions(self, predictions: np.ndarray, probabilities: np.ndarray, output_path: str):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        results = {
            "predictions": predictions.tolist(),
            "probabilities": probabilities.tolist(),
            "config": asdict(self.config),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"é¢„æµ‹ç»“æœä¿å­˜åˆ°: {output_path}")

# =============================================================================
# æ–‡æ¡£ç”Ÿæˆå™¨
# =============================================================================

class ModelDocumentationGenerator:
    """æ¨¡å‹æ–‡æ¡£ç”Ÿæˆå™¨"""
    
    def __init__(self, config: BayesianCNNConfig, model: BayesianCNNModel):
        self.config = config
        self.model = model
    
    def generate_model_md(self, output_path: str = "MODEL.md"):
        """ç”ŸæˆMODEL.mdæ–‡æ¡£"""
        model_info = self.model.get_model_info()
        
        content = f"""# {self.config.model_name} Model

## æ¨¡å‹æ¦‚è¿°

{self.config.model_name} æ˜¯ä¸€ä¸ªåŸºäºè´å¶æ–¯æ·±åº¦å­¦ä¹ çš„{self.config.model_type}æ¨¡å‹ï¼Œæ”¯æŒä¸ç¡®å®šæ€§é‡åŒ–ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

- **ä»»åŠ¡ç±»å‹**: {self.config.model_type}
- **æ¨¡å‹å‚æ•°**: {model_info['total_parameters']:,}
- **å¯è®­ç»ƒå‚æ•°**: {model_info['trainable_parameters']:,}
- **æ¨¡å‹å¤§å°**: {model_info['model_size_mb']:.2f} MB
- **ä¸ç¡®å®šæ€§é‡åŒ–**: æ”¯æŒè´å¶æ–¯æ¨ç†

## æ¨¡å‹æ¶æ„

### è´å¶æ–¯ç½‘ç»œç»“æ„

- è¾“å…¥ç»´åº¦: {self.config.input_dim}
- éšè—å±‚: {self.config.hidden_dims}
- è¾“å‡ºç»´åº¦: {self.config.output_dim}
- æ¿€æ´»å‡½æ•°: {self.config.activation}
- Dropoutç‡: {self.config.dropout}

### è´å¶æ–¯é…ç½®

- å…ˆéªŒå‡å€¼: {self.config.prior_mu}
- å…ˆéªŒæ ‡å‡†å·®: {self.config.prior_sigma}
- KLæƒé‡: {self.config.kl_weight}

## æŠ€æœ¯åŸç†

è¯¥æ¨¡å‹ä½¿ç”¨å˜åˆ†è´å¶æ–¯æ¨ç†ï¼Œé€šè¿‡BBBLinearå±‚å®ç°æƒé‡çš„æ¦‚ç‡åˆ†å¸ƒå»ºæ¨¡ï¼Œ
æ”¯æŒä¸ç¡®å®šæ€§é‡åŒ–å’Œé²æ£’æ€§é¢„æµ‹ã€‚

## é…ç½®å‚æ•°

### è®­ç»ƒé…ç½®
- å­¦ä¹ ç‡: {self.config.learning_rate}
- æ‰¹æ¬¡å¤§å°: {self.config.batch_size}
- è®­ç»ƒè½®æ•°: {self.config.epochs}
- æ··åˆç²¾åº¦: {self.config.use_mixed_precision}

### é›†æˆé…ç½®
- è®­ç»ƒé‡‡æ ·: {self.config.train_samples}
- éªŒè¯é‡‡æ ·: {self.config.val_samples}
- æµ‹è¯•é‡‡æ ·: {self.config.test_samples}

## ä½¿ç”¨æ–¹æ³•

### è®­ç»ƒæ¨¡å‹

```python
python BayesianCNN_unified.py train --config config.yaml --data-config data.yaml
```

### æ¨¡å‹æ¨ç†

```python
python BayesianCNN_unified.py inference --checkpoint best_model.pth
```

### ç”Ÿæˆæ–‡æ¡£

```python
python BayesianCNN_unified.py docs
```

## æ€§èƒ½ç‰¹ç‚¹

- æ”¯æŒGPUåŠ é€Ÿè®­ç»ƒ
- æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–
- ä¸ç¡®å®šæ€§é‡åŒ–èƒ½åŠ›
- è´å¶æ–¯é›†æˆé¢„æµ‹

## æ›´æ–°æ—¥å¿—

- åˆå§‹ç‰ˆæœ¬: {datetime.now().strftime('%Y-%m-%d')}

"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"æ¨¡å‹æ–‡æ¡£å·²ç”Ÿæˆ: {output_path}")

# =============================================================================
# é…ç½®å¤„ç†å‡½æ•°
# =============================================================================

def _flatten_config(config_dict: Dict[str, Any]) -> Dict[str, Any]:
    """å°†åµŒå¥—çš„é…ç½®å­—å…¸æ‰å¹³åŒ–ä¸ºBayesianCNNConfigå¯æ¥å—çš„æ ¼å¼"""
    flattened = {}
    
    # å¤„ç†architectureéƒ¨åˆ†
    if 'architecture' in config_dict:
        arch = config_dict['architecture']
        
        # å¤„ç†è´å¶æ–¯é…ç½®
        if 'bayesian_config' in arch:
            bayesian = arch['bayesian_config']
            flattened.update({
                'prior_mu': bayesian.get('prior_mu', 0.0),
                'prior_sigma': bayesian.get('prior_sigma', 0.1),
                'posterior_mu_initial': bayesian.get('posterior_mu_initial', [0.0, 0.1]),
                'posterior_rho_initial': bayesian.get('posterior_rho_initial', [-3.0, 0.1])
            })
        
        # å¤„ç†ç½‘ç»œé…ç½®
        if 'network' in arch:
            network = arch['network']
            flattened.update({
                'input_dim': network.get('input_dim'),
                'hidden_dims': network.get('hidden_dims', [128, 64]),
                'output_dim': network.get('output_dim'),
                'dropout': network.get('dropout', 0.2),
                'activation': network.get('activation', 'relu')
            })
    
    # å¤„ç†trainingéƒ¨åˆ†
    if 'training' in config_dict:
        training = config_dict['training']
        
        # åŸºæœ¬è®­ç»ƒå‚æ•°
        flattened.update({
            'epochs': training.get('epochs', 100),
            'batch_size': training.get('batch_size', 32)
        })
        
        # ä¼˜åŒ–å™¨é…ç½®
        if 'optimizer' in training:
            optimizer = training['optimizer']
            flattened.update({
                'learning_rate': optimizer.get('learning_rate', 0.001)
            })
        
        # æŸå¤±å‡½æ•°é…ç½®
        if 'loss' in training:
            loss = training['loss']
            flattened.update({
                'kl_weight': loss.get('kl_weight', 0.1)
            })
    
    # å¤„ç†inferenceéƒ¨åˆ†
    if 'inference' in config_dict:
        inference = config_dict['inference']
        
        # é‡‡æ ·é…ç½®
        if 'sampling' in inference:
            sampling = inference['sampling']
            flattened.update({
                'train_samples': sampling.get('num_samples', 1),
                'val_samples': sampling.get('num_samples', 5),
                'test_samples': sampling.get('num_samples', 10)
            })
    
    # å¤„ç†é¡¶å±‚é…ç½®ï¼ˆç›´æ¥æ˜ å°„ï¼‰
    direct_mappings = [
        'model_name', 'model_type', 'device', 'seed', 'use_mixed_precision',
        'dataloader_num_workers', 'pin_memory', 'gradient_clip_value',
        'data_path', 'checkpoint_dir', 'results_dir', 'logs_dir'
    ]
    
    for key in direct_mappings:
        if key in config_dict:
            flattened[key] = config_dict[key]
    
    return flattened

# =============================================================================
# ä¸»è¦æ¥å£å‡½æ•°
# =============================================================================

def create_model_factory(config: BayesianCNNConfig) -> BayesianCNNModel:
    """æ¨¡å‹å·¥å‚å‡½æ•°"""
    return BayesianCNNModel(config)

def create_data_loader_manager(data_config_path: str, config: BayesianCNNConfig) -> BayesianCNNDataLoaderManager:
    """æ•°æ®åŠ è½½å™¨ç®¡ç†å™¨å·¥å‚å‡½æ•°"""
    return BayesianCNNDataLoaderManager(data_config_path, config)

def main_train(config_path: str = "config.yaml", data_config_path: str = "data.yaml",
               optuna_config_path: str = None, device: str = "auto",
               epochs_override: int = None, no_save_model: bool = False,
               seed: int = 42, checkpoint_dir: str = "checkpoints"):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    import random
    import numpy as np
    import torch
    import logging
    logger = logging.getLogger(__name__)
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    
    logger.info(f"ğŸ² è®¾ç½®éšæœºç§å­: {seed}")
    logger.info(f"ğŸ’¾ Checkpointä¿å­˜ç›®å½•: {checkpoint_dir}")
    
    # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # åŠ è½½é…ç½®
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_dict = yaml.safe_load(f)
    except FileNotFoundError:
        print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config_dict = {}
    
    # è§£æåµŒå¥—é…ç½®ç»“æ„
    flattened_config = _flatten_config(config_dict)
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = BayesianCNNConfig(**flattened_config)
    
    # åº”ç”¨Optunaé…ç½®è¦†ç›–
    if optuna_config_path and os.path.exists(optuna_config_path):
        try:
            with open(optuna_config_path, 'r', encoding='utf-8') as f:
                optuna_config = yaml.safe_load(f)
            
            # åº”ç”¨è¶…å‚æ•°è¦†ç›–
            for key, value in optuna_config.items():
                if hasattr(config, key):
                    setattr(config, key, value)
                    print(f"ğŸ”§ Optunaè¦†ç›–å‚æ•°: {key} = {value}")
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½Optunaé…ç½®: {e}")
    
    # è¦†ç›–é…ç½®
    if device != "auto":
        config.device = device
    if epochs_override:
        config.epochs = epochs_override
    
    # è®¾ç½®å›ºå®šçš„æ—¥å¿—è·¯å¾„
    log_dir = "/home/feng.hao.jie/deployment/model_explorer/c_training_evaluation_agent/training/logs/BayesianCNN_3356644015"
    logger = setup_logging(log_dir=log_dir, log_filename="log.txt")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä¼ é€’checkpointç›®å½•ï¼‰
    trainer = UnifiedTrainer(config, model, data_loader_manager, checkpoint_dir=checkpoint_dir)
    
    # æ‰§è¡Œè®­ç»ƒ
    results = trainer.train(no_save_model=no_save_model)
    
    # è¾“å‡ºJSONæ ¼å¼çš„è®­ç»ƒç»“æœ
    final_results = {
        "final_results": {
            "best_val_score": results['best_val_score'],
            "total_epochs": results['total_epochs'],
            "total_time": results['total_time'],
            "final_train_acc": results['train_history'][-1].get('train_acc', 0),
            "final_val_acc": results['train_history'][-1].get('val_acc', 0),
            "train_auc": results['train_history'][-1].get('train_auc', 0),
            "val_auc": results['train_history'][-1].get('val_auc', 0),
            "train_losses": [epoch['train_loss'] for epoch in results['train_history']],
            "val_losses": [epoch['val_loss'] for epoch in results['train_history']],
            "train_accuracies": [epoch.get('train_acc', 0) for epoch in results['train_history']],
            "val_accuracies": [epoch.get('val_acc', 0) for epoch in results['train_history']]
        }
    }
    print(json.dumps(final_results, indent=2))
    
    return results

def main_inference(config_path: str = "config.yaml", data_config_path: str = "data.yaml", 
                  checkpoint_path: str = "best_model.pth", mode: str = "eval",
                  start_date: str = None, end_date: str = None, 
                  output_path: str = None, output_format: str = "parquet"):
    """ä¸»æ¨ç†å‡½æ•° - æ”¯æŒè¯„ä¼°å’Œé¢„æµ‹æ–‡ä»¶ç”Ÿæˆ"""
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    # è§£æåµŒå¥—é…ç½®ç»“æ„
    flattened_config = _flatten_config(config_dict)
    config = BayesianCNNConfig(**flattened_config)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ç®¡ç†å™¨
    data_loader_manager = create_data_loader_manager(data_config_path, config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # åˆ›å»ºæ¨ç†å™¨
    inferencer = UnifiedInferencer(config, model, data_loader_manager)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    checkpoint = inferencer.load_checkpoint(checkpoint_path)
    
    if mode == "test":
        # ç”Ÿæˆé¢„æµ‹æ–‡ä»¶æ¨¡å¼
        if not output_path:
            output_path = f"predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{output_format}"
        
        print(f"ğŸ”® ç”Ÿæˆé¢„æµ‹æ–‡ä»¶æ¨¡å¼")
        print(f"  ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
        print(f"  ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"  ğŸ“Š è¾“å‡ºæ ¼å¼: {output_format}")
        
        # ç”Ÿæˆé¢„æµ‹æ•°æ®
        predictions_data = inferencer.generate_predictions(
            start_date=start_date,
            end_date=end_date,
            output_path=output_path,
            output_format=output_format
        )
        
        print(f"âœ… é¢„æµ‹æ–‡ä»¶ç”Ÿæˆå®Œæˆ: {output_path}")
        return {"predictions_file": output_path, "predictions_count": len(predictions_data)}
        
    else:
        # è¯„ä¼°æ¨¡å¼
        print(f"ğŸ“Š æ¨¡å‹è¯„ä¼°æ¨¡å¼")
        metrics = inferencer.evaluate()
        print(f"æ¨ç†ç»“æœ: {metrics}")
        return metrics

def main_generate_docs(config_path: str = "config.yaml", data_config_path: str = "data.yaml"):
    """ä¸»æ–‡æ¡£ç”Ÿæˆå‡½æ•°"""
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)
    
    # è§£æåµŒå¥—é…ç½®ç»“æ„
    flattened_config = _flatten_config(config_dict)
    config = BayesianCNNConfig(**flattened_config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model_factory(config)
    
    # ç”Ÿæˆæ–‡æ¡£
    doc_generator = ModelDocumentationGenerator(config, model)
    doc_generator.generate_model_md("MODEL.md")
    
    print("æ–‡æ¡£ç”Ÿæˆå®Œæˆ")

# =============================================================================
# å‘½ä»¤è¡Œæ¥å£
# =============================================================================

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="BayesianCNNç»Ÿä¸€æ¨¡å‹è®­ç»ƒã€æ¨ç†å’Œæ–‡æ¡£ç”Ÿæˆå·¥å…·")
    parser.add_argument("mode", choices=["train", "inference", "docs"], help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--config", default="config.yaml", help="æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-config", default="data.yaml", help="æ•°æ®é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--checkpoint", default="best_model.pth", help="æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--data", help="æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    
    # æ¨ç†ç›¸å…³å‚æ•°
    parser.add_argument("--inference-mode", choices=["test", "eval"], default="eval", help="æ¨ç†æ¨¡å¼ï¼štest(ç”Ÿæˆé¢„æµ‹æ–‡ä»¶) æˆ– eval(è¯„ä¼°)")
    parser.add_argument("--start-date", help="æ¨ç†å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end-date", help="æ¨ç†ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--format", choices=["parquet", "csv"], default="parquet", help="è¾“å‡ºæ ¼å¼")
    
    # Optunaä¼˜åŒ–ç›¸å…³å‚æ•°
    parser.add_argument("--optuna-config", help="Optunaè¯•éªŒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--device", choices=["cuda", "cpu", "auto"], default="auto", help="è®­ç»ƒè®¾å¤‡")
    parser.add_argument("--epochs", type=int, help="è®­ç»ƒè½®æ•°ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰")
    parser.add_argument("--no-save-model", action="store_true", help="ä¸ä¿å­˜æ¨¡å‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­ï¼ˆç¡®ä¿å¯å¤ç°æ€§ï¼‰")
    parser.add_argument("--checkpoint-dir", type=str, default="checkpoints", help="Checkpointä¿å­˜ç›®å½•")
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    
    if args.mode == "train":
        main_train(
            config_path=args.config,
            data_config_path=args.data_config,
            optuna_config_path=args.optuna_config,
            device=args.device,
            epochs_override=args.epochs,
            no_save_model=args.no_save_model,
            seed=args.seed,
            checkpoint_dir=args.checkpoint_dir
        )
    elif args.mode == "inference":
        main_inference(
            config_path=args.config,
            data_config_path=args.data_config,
            checkpoint_path=args.checkpoint,
            mode=args.inference_mode,
            start_date=args.start_date,
            end_date=args.end_date,
            output_path=args.output,
            output_format=args.format
        )
    elif args.mode == "docs":
        main_generate_docs(args.config, args.data_config)